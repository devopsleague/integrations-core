{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Agent Integrations","text":"<p>Welcome to the wonderful world of developing Agent Integrations for Datadog. Here we document how we do things, the processes for various tasks, coding conventions &amp; best practices, the internals of our testing infrastructure, and so much more.</p> <p>If you are intrigued, continue reading. If not, continue all the same </p>"},{"location":"#getting-started","title":"Getting started","text":"<p>To work on any integration (a.k.a. Check), you must setup your development environment.</p> <p>After that you may immediately begin testing or read through the best practices we strive to follow.</p> <p>Also, feel free to check out how ddev works and browse the API reference of the base package.</p>"},{"location":"#navigation","title":"Navigation","text":"<p>Desktop readers can use keyboard shortcuts to navigate.</p> Keys Action <ul><li>, (comma)</li><li>p</li></ul> Navigate to the \"previous\" page <ul><li>. (period)</li><li>n</li></ul> Navigate to the \"next\" page <ul><li>/</li><li>s</li></ul> Display the search modal"},{"location":"e2e/","title":"E2E","text":"<p>Any integration that makes use of our pytest plugin in its test suite supports end-to-end testing on a live Datadog Agent.</p> <p>The entrypoint for E2E management is the command group <code>ddev env</code>.</p>"},{"location":"e2e/#discovery","title":"Discovery","text":"<p>Use the <code>ls</code> command to see what environments are available, for example:</p> <pre><code>$ ddev env ls envoy\nenvoy:\n    py27\n    py38\n</code></pre> <p>You'll notice that only environments that actually run tests are available.</p> <p>Running simply <code>ddev env ls</code> with no arguments will display the active environments.</p>"},{"location":"e2e/#creation","title":"Creation","text":"<p>To start an environment run <code>ddev env start &lt;INTEGRATION&gt; &lt;ENVIRONMENT&gt;</code>, for example:</p> <pre><code>$ ddev env start envoy py38\nSetting up environment `py38`... success!\nUpdating `datadog/agent-dev:master`... success!\nDetecting the major version... Agent 7 detected\nWriting configuration for `py38`... success!\nStarting the Agent... success!\n\nConfig file (copied to your clipboard): C:\\Users\\ofek\\AppData\\Local\\dd-checks-dev\\envs\\envoy\\py38\\config\\envoy.yaml\nTo run this check, do: ddev env check envoy py38\nTo stop this check, do: ddev env stop envoy py38\n</code></pre> <p>This sets up the selected environment and an instance of the Agent running in a Docker container. The default configuration is defined by each environment's test suite and is saved to a file, which is then mounted to the Agent container so you may freely modify it.</p> <p>Let's see what we have running:</p> <pre><code>$ docker ps --format \"table {{.Image}}\\t{{.Status}}\\t{{.Ports}}\\t{{.Names}}\"\nIMAGE                          STATUS                            PORTS                                                     NAMES\ndatadog/agent-dev:master-py3   Up 4 seconds (health: starting)                                                             dd_envoy_py38\ndefault_service2               Up 5 seconds                      80/tcp, 10000/tcp                                         default_service2_1\nenvoyproxy/envoy:latest        Up 5 seconds                      0.0.0.0:8001-&gt;8001/tcp, 10000/tcp, 0.0.0.0:8000-&gt;80/tcp   default_front-envoy_1\ndefault_xds                    Up 5 seconds                      8080/tcp                                                  default_xds_1\ndefault_service1               Up 5 seconds                      80/tcp, 10000/tcp                                         default_service1_1\n</code></pre>"},{"location":"e2e/#agent-version","title":"Agent version","text":"<p>You can select a particular build of the Agent to use with the <code>--agent</code>/<code>-a</code> option. Any Docker image is valid e.g. <code>datadog/agent:7.17.0</code>.</p> <p>A custom nightly build will be used by default, which is re-built on every commit to the Datadog Agent repository.</p>"},{"location":"e2e/#integration-version","title":"Integration version","text":"<p>By default the version of the integration used will be the one shipped with the chosen Agent version, as if you had passed in the <code>--prod</code> flag. If you wish to modify an integration and test changes in real time, use the <code>--dev</code> flag.</p> <p>Doing so will mount and install the integration in the Agent container. All modifications to the integration's directory will be propagated to the Agent, whether it be a code change or switching to a different Git branch.</p> <p>If you modify the base package then you will need to mount that with the <code>--base</code> flag, which implicitly activates <code>--dev</code>.</p>"},{"location":"e2e/#testing","title":"Testing","text":"<p>To run tests against the live Agent, use the <code>ddev env test</code> command. It is similar to the test command except it is capable of running tests marked as E2E, and only runs such tests.</p>"},{"location":"e2e/#automation","title":"Automation","text":"<p>You can use the <code>--new-env</code>/<code>-ne</code> flag to automate environment management. For example running:</p> <pre><code>ddev env test apache:py38 vault:py38 -ne\n</code></pre> <p>will start the <code>py38</code> environment for Apache, run E2E tests, tear down the environment, and then do the same for Vault.</p> <p>Tip</p> <p>Since running tests implies code changes are being introduced, <code>--new-env</code> enables <code>--dev</code> by default.</p>"},{"location":"e2e/#execution","title":"Execution","text":"<p>Similar to the Agent's <code>check</code> command, you can perform manual check runs using <code>ddev env check &lt;INTEGRATION&gt; &lt;ENVIRONMENT&gt;</code>, for example:</p> <pre><code>$ ddev env check envoy py38 --log-level debug\n...\n=========\nCollector\n=========\n\n  Running Checks\n  ==============\n\n    envoy (1.12.0)\n    --------------\n      Instance ID: envoy:c705bd922a3c275c [OK]\n      Configuration Source: file:/etc/datadog-agent/conf.d/envoy.d/envoy.yaml\n      Total Runs: 1\n      Metric Samples: Last Run: 546, Total: 546\n      Events: Last Run: 0, Total: 0\n      Service Checks: Last Run: 1, Total: 1\n      Average Execution Time : 25ms\n      Last Execution Date : 2020-02-17 00:58:05.000000 UTC\n      Last Successful Execution Date : 2020-02-17 00:58:05.000000 UTC\n</code></pre>"},{"location":"e2e/#debugging","title":"Debugging","text":"<p>You may start an interactive debugging session using the <code>--breakpoint</code>/<code>-b</code> option.</p> <p>The option accepts an integer representing the line number at which to break. For convenience, <code>0</code> and <code>-1</code> are shortcuts to the first and last line of the integration's <code>check</code> method, respectively.</p> <pre><code>$ ddev env check envoy py38 -b 0\n&gt; /opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/envoy/envoy.py(34)check()\n-&gt; custom_tags = instance.get('tags', [])\n(Pdb) list\n 29             self.blacklisted_metrics = set()\n 30\n 31             self.caching_metrics = None\n 32\n 33         def check(self, instance):\n 34 B-&gt;         custom_tags = instance.get('tags', [])\n 35\n 36             try:\n 37                 stats_url = instance['stats_url']\n 38             except KeyError:\n 39                 msg = 'Envoy configuration setting `stats_url` is required'\n(Pdb) print(instance)\n{'stats_url': 'http://localhost:8001/stats'}\n</code></pre> <p>Caveat</p> <p>The line number must be within the integration's <code>check</code> method.</p>"},{"location":"e2e/#refreshing-state","title":"Refreshing state","text":"<p>Testing and manual check runs always reflect the current state of code and configuration however, if you want to see the result of changes in-app, you will need to refresh the environment by running <code>ddev env reload &lt;INTEGRATION&gt; &lt;ENVIRONMENT&gt;</code>.</p>"},{"location":"e2e/#removal","title":"Removal","text":"<p>To stop an environment run <code>ddev env stop &lt;INTEGRATION&gt; &lt;ENVIRONMENT&gt;</code>.</p> <p>Any environments that haven't been explicitly stopped will show as active in the output of <code>ddev env ls</code>, even persisting through system restarts. If you are confident that environments are no longer active, you can run <code>ddev env prune</code> to remove all accumulated environment state.</p>"},{"location":"setup/","title":"Setup","text":"<p>This will be relatively painless, we promise!</p>"},{"location":"setup/#integrations","title":"Integrations","text":"<p>You will need to clone integrations-core and/or integrations-extras depending on which integrations you intend to work on.</p>"},{"location":"setup/#python","title":"Python","text":"<p>To work on any integration you must install Python 3.9.</p> <p>After installation, restart your terminal and ensure that your newly installed Python comes first in your <code>PATH</code>.</p> macOSWindowsLinux <p>First update the formulae and Homebrew itself:</p> <pre><code>brew update\n</code></pre> <p>then install Python:</p> <pre><code>brew install python@3.9\n</code></pre> <p>After it completes, check the output to see if it asked you to run any extra commands and if so, execute them.</p> <p>Verify successful <code>PATH</code> modification:</p> <pre><code>which -a python\n</code></pre> <p>Windows users have it the easiest.</p> <p>Download the Python 3.9 64-bit executable installer and run it. When prompted, be sure to select the option to add to your <code>PATH</code>. Also, it is recommended that you choose the per-user installation method.</p> <p>Verify successful <code>PATH</code> modification:</p> <pre><code>where python\n</code></pre> <p>Ah, you enjoy difficult things. Are you using Gentoo?</p> <p>We recommend using either Miniconda or pyenv to install Python 3.9. Whatever you do, never modify the system Python.</p> <p>Verify successful <code>PATH</code> modification:</p> <pre><code>which -a python\n</code></pre>"},{"location":"setup/#pipx","title":"pipx","text":"<p>To install certain command line tools, you'll need pipx.</p> macOSWindowsLinux <p>Run:</p> <pre><code>brew install pipx\n</code></pre> <p>After it completes, check the output to see if it asked you to run any extra commands and if so, execute them.</p> <p>Verify successful <code>PATH</code> modification:</p> <pre><code>which -a pipx\n</code></pre> <p>Run:</p> <pre><code>python -m pip install pipx\n</code></pre> <p>Verify successful <code>PATH</code> modification:</p> <pre><code>where pipx\n</code></pre> <p>Run:</p> <pre><code>python -m pip install --user pipx\n</code></pre> <p>Verify successful <code>PATH</code> modification:</p> <pre><code>which -a pipx\n</code></pre>"},{"location":"setup/#ddev","title":"ddev","text":""},{"location":"setup/#installation","title":"Installation","text":"<p>You have 4 options to install the CLI.</p>"},{"location":"setup/#installers","title":"Installers","text":"macOSWindows GUI installerCommand line installer <ol> <li>In your browser, download the <code>.pkg</code> file: ddev-0.0.0.pkg</li> <li>Run your downloaded file and follow the on-screen instructions.</li> <li>Restart your terminal.</li> <li> <p>To verify that the shell can find and run the <code>ddev</code> command in your <code>PATH</code>, use the following command.</p> <pre><code>$ ddev --version\n0.0.0\n</code></pre> </li> </ol> <ol> <li> <p>Download the file using the <code>curl</code> command. The <code>-o</code> option specifies the file name that the downloaded package is written to. In this example, the file is written to <code>ddev-0.0.0.pkg</code> in the current directory.</p> <pre><code>curl -o ddev-0.0.0.pkg https://github.com/DataDog/integrations-core/releases/download/ddev-v0.0.0/ddev-0.0.0.pkg\n</code></pre> </li> <li> <p>Run the standard macOS <code>installer</code> program, specifying the downloaded <code>.pkg</code> file as the source. Use the <code>-pkg</code> parameter to specify the name of the package to install, and the <code>-target /</code> parameter for the drive in which to install the package. The files are installed to <code>/usr/local/ddev</code>, and an entry is created at <code>/etc/paths.d/ddev</code> that instructs shells to add the <code>/usr/local/ddev</code> directory to. You must include sudo on the command to grant write permissions to those folders.</p> <pre><code>sudo installer -pkg ./ddev-0.0.0.pkg -target /\n</code></pre> </li> <li> <p>Restart your terminal.</p> </li> <li> <p>To verify that the shell can find and run the <code>ddev</code> command in your <code>PATH</code>, use the following command.</p> <pre><code>$ ddev --version\n0.0.0\n</code></pre> </li> </ol> GUI installerCommand line installer <ol> <li>In your browser, download one the <code>.msi</code> files:<ul> <li>ddev-0.0.0-x64.msi</li> <li>ddev-0.0.0-x86.msi</li> </ul> </li> <li>Run your downloaded file and follow the on-screen instructions.</li> <li>Restart your terminal.</li> <li> <p>To verify that the shell can find and run the <code>ddev</code> command in your <code>PATH</code>, use the following command.</p> <pre><code>$ ddev --version\n0.0.0\n</code></pre> </li> </ol> <ol> <li> <p>Download and run the installer using the standard Windows <code>msiexec</code> program, specifying one of the <code>.msi</code> files as the source. Use the <code>/passive</code> and <code>/i</code> parameters to request an unattended, normal installation.</p> x64x86 <pre><code>msiexec /passive /i https://github.com/DataDog/integrations-core/releases/download/ddev-v0.0.0/ddev-0.0.0-x64.msi\n</code></pre> <pre><code>msiexec /passive /i https://github.com/DataDog/integrations-core/releases/download/ddev-v0.0.0/ddev-0.0.0-x86.msi\n</code></pre> </li> <li> <p>Restart your terminal.</p> </li> <li> <p>To verify that the shell can find and run the <code>ddev</code> command in your <code>PATH</code>, use the following command.</p> <pre><code>$ ddev --version\n0.0.0\n</code></pre> </li> </ol>"},{"location":"setup/#standalone-binaries","title":"Standalone binaries","text":"<p>After downloading the archive corresponding to your platform and architecture, extract the binary to a directory that is on your PATH and rename to <code>ddev</code>.</p> macOSWindowsLinux <ul> <li>ddev-0.0.0-aarch64-apple-darwin.tar.gz</li> <li>ddev-0.0.0-x86_64-apple-darwin.tar.gz</li> </ul> <ul> <li>ddev-0.0.0-x86_64-pc-windows-msvc.zip</li> <li>ddev-0.0.0-i686-pc-windows-msvc.zip</li> </ul> <ul> <li>ddev-0.0.0-aarch64-unknown-linux-gnu.tar.gz</li> <li>ddev-0.0.0-x86_64-unknown-linux-gnu.tar.gz</li> <li>ddev-0.0.0-x86_64-unknown-linux-musl.tar.gz</li> <li>ddev-0.0.0-i686-unknown-linux-gnu.tar.gz</li> <li>ddev-0.0.0-powerpc64le-unknown-linux-gnu.tar.gz</li> </ul>"},{"location":"setup/#pypi","title":"PyPI","text":"macOSWindowsLinux <p>Remove any executables shown in the output of <code>which -a ddev</code> and make sure that there is no active virtual environment, then run:</p> ARMIntel <pre><code>pipx install ddev --python /opt/homebrew/bin/python3.9\n</code></pre> <pre><code>pipx install ddev --python /usr/local/bin/python3.9\n</code></pre> <p>Warning</p> <p>Do not use <code>sudo</code> as it may result in a broken installation!</p> <p>Run:</p> <pre><code>pipx install ddev\n</code></pre> <p>Run:</p> <pre><code>pipx install ddev\n</code></pre> <p>Warning</p> <p>Do not use <code>sudo</code> as it may result in a broken installation!</p> <p>Upgrade at any time by running:</p> <pre><code>pipx upgrade ddev\n</code></pre>"},{"location":"setup/#development","title":"Development","text":"<p>This is if you cloned integrations-core and want to always use the version based on the current branch.</p> macOSWindowsLinux <p>Remove any executables shown in the output of <code>which -a ddev</code> and make sure that there is no active virtual environment, then run:</p> ARMIntel <pre><code>pipx install -e /path/to/integrations-core/ddev --python /opt/homebrew/opt/python@3.9/bin/python3.9\n</code></pre> <pre><code>pipx install -e /path/to/integrations-core/ddev --python /usr/local/opt/python@3.9/bin/python3.9\n</code></pre> <p>Warning</p> <p>Do not use <code>sudo</code> as it may result in a broken installation!</p> <p>Run:</p> <pre><code>pipx install -e /path/to/integrations-core/ddev\n</code></pre> <p>Run:</p> <pre><code>pipx install -e /path/to/integrations-core/ddev\n</code></pre> <p>Warning</p> <p>Do not use <code>sudo</code> as it may result in a broken installation!</p> <p>Re-sync dependencies at any time by running:</p> <pre><code>pipx upgrade ddev\n</code></pre> <p>Note</p> <p>Be aware that this method does not keep track of dependencies so you will need to re-run the command if/when the required dependencies are changed.</p> <p>Note</p> <p>Also be aware that this method does not get any changes from <code>datadog_checks_dev</code>, so if you have unreleased changes from <code>datadog_checks_dev</code> that may affect <code>ddev</code>, you will need to run the following to get the most recent changes from <code>datadog_checks_dev</code> to your <code>ddev</code>:</p> <pre><code>pipx inject ddev -e \"/path/to/datadog_checks_dev\"\n</code></pre>"},{"location":"setup/#configuration","title":"Configuration","text":"<p>Upon the first invocation, <code>ddev</code> will create its config file if it does not yet exist.</p> <p>You will need to set the location of each cloned repository:</p> <pre><code>ddev config set &lt;REPO&gt; /path/to/integrations-&lt;REPO&gt;\n</code></pre> <p>The <code>&lt;REPO&gt;</code> may be either <code>core</code> or <code>extras</code>.</p> <p>By default, the repo <code>core</code> will be the target of all commands. If you want to switch to <code>integrations-extras</code>, run:</p> <pre><code>ddev config set repo extras\n</code></pre>"},{"location":"setup/#docker","title":"Docker","text":"<p>Docker is used in nearly every integration's test suite therefore we simply require it to avoid confusion.</p> macOSWindowsLinux <ol> <li>Install Docker Desktop for Mac.</li> <li>Right-click the Docker taskbar item and update Preferences &gt; File Sharing with any locations you need to open.</li> </ol> <ol> <li>Install Docker Desktop for Windows.</li> <li>Right-click the Docker taskbar item and update Settings &gt; Shared Drives with any locations you need to open e.g. <code>C:\\</code>.</li> </ol> <ol> <li> <p>Install Docker Engine for your distribution:</p> UbuntuDebianFedoraCentOS <p>Docker CE for Ubuntu</p> <p>Docker CE for Debian</p> <p>Docker CE for Fedora</p> <p>Docker CE for CentOS</p> </li> <li> <p>Add your user to the <code>docker</code> group:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> </li> <li> <p>Sign out and then back in again so your changes take effect.</p> </li> </ol> <p>After installation, restart your terminal one last time.</p>"},{"location":"testing/","title":"Testing","text":"<p>The entrypoint for testing any integration is the command <code>test</code>.</p> <p>Under the hood, we use hatch for environment management and pytest as our test framework.</p>"},{"location":"testing/#discovery","title":"Discovery","text":"<p>Use the <code>--list</code>/<code>-l</code> flag to see what environments are available, for example:</p> <pre><code>$ ddev test postgres -l\n                                      Standalone\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name   \u2503 Type    \u2503 Features \u2503 Dependencies    \u2503 Environment variables   \u2503 Scripts   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 lint   \u2502 virtual \u2502          \u2502 black==22.12.0  \u2502                         \u2502 all       \u2502\n\u2502        \u2502         \u2502          \u2502 pydantic==2.0.2 \u2502                         \u2502 fmt       \u2502\n\u2502        \u2502         \u2502          \u2502 ruff==0.0.257   \u2502                         \u2502 style     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 latest \u2502 virtual \u2502 deps     \u2502                 \u2502 POSTGRES_VERSION=latest \u2502 benchmark \u2502\n\u2502        \u2502         \u2502          \u2502                 \u2502                         \u2502 test      \u2502\n\u2502        \u2502         \u2502          \u2502                 \u2502                         \u2502 test-cov  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        Matrices\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name    \u2503 Type    \u2503 Envs       \u2503 Features \u2503 Scripts   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 default \u2502 virtual \u2502 py3.9-9.6  \u2502 deps     \u2502 benchmark \u2502\n\u2502         \u2502         \u2502 py3.9-10.0 \u2502          \u2502 test      \u2502\n\u2502         \u2502         \u2502 py3.9-11.0 \u2502          \u2502 test-cov  \u2502\n\u2502         \u2502         \u2502 py3.9-12.1 \u2502          \u2502           \u2502\n\u2502         \u2502         \u2502 py3.9-13.0 \u2502          \u2502           \u2502\n\u2502         \u2502         \u2502 py3.9-14.0 \u2502          \u2502           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You'll notice that all environments for running tests are prefixed with <code>pyX.Y</code>, indicating the Python version to use. If you don't have a particular version installed (for example Python 2.7), such environments will be skipped.</p> <p>The second part of a test environment's name corresponds to the version of the product. For example, the <code>14.0</code> in <code>py3.9-14.0</code> implies tests will run against version 14.x of PostgreSQL.</p> <p>If there is no version suffix, it means that either:</p> <ol> <li>the version is pinned, usually set to pull the latest release, or</li> <li>there is no concept of a product, such as the <code>disk</code> check</li> </ol>"},{"location":"testing/#usage","title":"Usage","text":""},{"location":"testing/#explicit","title":"Explicit","text":"<p>Passing just the integration name will run every test environment. You may select a subset of environments to run by appending a <code>:</code> followed by a comma-separated list of environments.</p> <p>For example, executing:</p> <pre><code>ddev test postgres:py3.9-13.0,py3.9-11.0\n</code></pre> <p>will run tests for the environment <code>py3.9-13.0</code> followed by the environment <code>py3.9-11.0</code>.</p>"},{"location":"testing/#detection","title":"Detection","text":"<p>If no integrations are specified then only integrations that were changed will be tested, based on a diff between the latest commit to the current and <code>master</code> branches.</p> <p>The criteria for an integration to be considered changed is based on the file extension of paths in the diff. So for example if only Markdown files were modified then nothing will be tested.</p> <p>The integrations will be tested in lexicographical order.</p>"},{"location":"testing/#coverage","title":"Coverage","text":"<p>To measure code coverage, use the <code>--cov</code>/<code>-c</code> flag. Doing so will display a summary of coverage statistics after successful execution of integrations' tests.</p> <pre><code>$ ddev test tls -c\n...\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Coverage report \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nName                              Stmts   Miss Branch BrPart  Cover   Missing\n-----------------------------------------------------------------------------\ndatadog_checks\\tls\\__about__.py       1      0      0      0   100%\ndatadog_checks\\tls\\__init__.py        3      0      0      0   100%\ndatadog_checks\\tls\\tls.py           185      4     50      2    97%   160-167, 288-&gt;275, 297-&gt;300, 300\ndatadog_checks\\tls\\utils.py          43      0     16      0   100%\ntests\\__init__.py                     0      0      0      0   100%\ntests\\conftest.py                   105      0      0      0   100%\ntests\\test_config.py                 47      0      0      0   100%\ntests\\test_local.py                 113      0      0      0   100%\ntests\\test_remote.py                189      0      2      0   100%\ntests\\test_utils.py                  15      0      0      0   100%\ntests\\utils.py                       36      0      2      0   100%\n-----------------------------------------------------------------------------\nTOTAL                               737      4     70      2    99%\n</code></pre>"},{"location":"testing/#linting","title":"Linting","text":"<p>To run only the lint checks, use the <code>--lint</code>/<code>-s</code> shortcut flag.</p> <p>You may also only run the formatter using the <code>--fmt</code>/<code>-fs</code> shortcut flag. The formatter will automatically resolve the most common errors caught by the lint checks.</p>"},{"location":"testing/#argument-forwarding","title":"Argument forwarding","text":"<p>You may pass arbitrary arguments directly to <code>pytest</code>, for example:</p> <pre><code>ddev test postgres -- -m unit --pdb -x\n</code></pre>"},{"location":"architecture/ibm_i/","title":"IBM i","text":"<p>Note</p> <p>This section is meant for developers that want to understand the working of the IBM i integration.</p>"},{"location":"architecture/ibm_i/#overview","title":"Overview","text":"<p>The IBM i integration uses ODBC to connect to IBM i hosts and  query system data through an SQL interface. To do so, it uses the ODBC Driver for IBM i Access Client Solutions, an IBM propietary ODBC driver that manages connections to IBM i hosts.</p> <p>Limitations in the IBM i ODBC driver make it necessary to structure the check in a more complex way than would be expected, to avoid the check from hanging or leaking threads.</p>"},{"location":"architecture/ibm_i/#ibm-i-odbc-driver-limitations","title":"IBM i ODBC driver limitations","text":"<p>ODBC drivers can optionally support custom configuration through connection attributes, which help configure how a connection works. One fundamental connection attribute is <code>SQL_ATTR_QUERY_TIMEOUT</code> (and related <code>_TIMEOUT</code> attributes), which set the timeout for SQL queries done through the driver (or the timeout for other connection steps for other <code>_TIMEOUT</code> attributes). If this connection attribute is not set there is no timeout, which means the driver gets stuck waiting for a reply when a network issue happens.</p> <p>As of the writing of this document, the IBM i ODBC driver behavior when setting the <code>SQL_ATTR_QUERY_TIMEOUT</code> connection attribute is similar to the one described in ODBC Query Timeout Property. For the IBM i DB2 driver: the driver estimates the running time of a query and preemptively aborts the query if the estimate is above the specified threshold, but it does not take into account the actual running time of the query (and thus, it's not useful for avoiding network issues).</p>"},{"location":"architecture/ibm_i/#ibm-i-check-workaround","title":"IBM i check workaround","text":"<p>To deal with the OBDC driver limitations, the IBM i check needs to have an alternative way to abort a query once a given timeout has passed. To do so, the IBM i check runs queries in a subprocess which it kills and restarts when timeouts pass. This subprocess runs <code>query_script.py</code> using the embedded Python interpreter.</p> <p>It is essential that the connection is kept across queries. For a given connection, <code>ELAPSED_</code> columns on IBM i views report statistics since the last time the table was queried on that connection, thus if using different connections these values are always zero.</p> <p>To communicate with the main Agent process, the subprocess and the IBM i check exchange JSON-encoded messages through pipes until the special <code>ENDOFQUERY</code> message is received. Special care is needed to avoid blocking on reads and writes of the pipes.</p> <p>For adding/modifying the queries, the check uses the standard <code>QueryManager</code> class used for SQL-based checks, except that each query needs to include a timeout value (since, empirically, some queries take much longer to complete on IBM i hosts).</p>"},{"location":"architecture/snmp/","title":"SNMP","text":"<p>Note</p> <p>This section is meant for developers that want to understand the working of the SNMP integration.</p> <p>Be sure you are familiar with SNMP concepts, and you have read through the official SNMP integration docs.</p>"},{"location":"architecture/snmp/#overview","title":"Overview","text":"<p>While most integrations are either Python, JMX, or implemented in the Agent in Go, the SNMP integration is a bit more complex.</p> <p>Here's an overview of what this integration involves:</p> <ul> <li>A Python check, responsible for:<ul> <li>Collecting metrics from a specific device IP. Metrics typically come from profiles, but they can also be specified explicitly.</li> <li>Auto-discovering devices over a network. (Pending deprecation in favor of Agent auto-discovery.)</li> </ul> </li> <li>An Agent service listener, responsible for auto-discovering devices over a network and forwarding discovered instances to the existing Agent check scheduling pipeline. Also known as \"Agent SNMP auto-discovery\".</li> </ul> <p>The diagram below shows how these components interact for a typical VM-based setup (single Agent on a host). For Datadog Cluster Agent (DCA) deployments, see Cluster Agent support.</p> <p></p>"},{"location":"architecture/snmp/#python-check","title":"Python Check","text":""},{"location":"architecture/snmp/#dependencies","title":"Dependencies","text":"<p>The Python check uses PySNMP to make SNMP queries and manipulate SNMP data (OIDs, variables, and MIBs).</p>"},{"location":"architecture/snmp/#device-monitoring","title":"Device Monitoring","text":"<p>The primary functionality of the Python check is to collect metrics from a given device given its IP address.</p> <p>As all Python checks, it supports multi-instances configuration, where each instance represents a device:</p> <pre><code>instances:\n- ip_address: \"192.168.0.12\"\n# &lt;Options...&gt;\n</code></pre>"},{"location":"architecture/snmp/#python-auto-discovery","title":"Python Auto-Discovery","text":""},{"location":"architecture/snmp/#approach","title":"Approach","text":"<p>The Python check includes a multithreaded implementation of device auto-discovery. It runs on instances that use <code>network_address</code> instead of <code>ip_address</code>:</p> <pre><code>instances:\n- network_address: \"192.168.0.0/28\"\n# &lt;Options...&gt;\n</code></pre> <p>The main tasks performed by device auto-discovery are:</p> <ul> <li>Find new devices: For each IP in the <code>network_address</code> CIDR range, the check queries the device <code>sysObjectID</code>. If the query succeeds and the <code>sysObjectID</code> matches one of the registered profiles, the device is added as a discovered instance. This logic is run at regular intervals in a separate thread.</li> <li>Cache devices: To improve performance, discovered instances are cached on disk based on a hash of the instance. Since options from the <code>network_address</code> instance are copied into discovered instances, the cache is invalidated if the <code>network_address</code> changes.</li> <li>Check devices: On each check run, the check runs a check on all discovered instances. This is done in parallel using a threadpool. The check waits for all sub-checks to finish.</li> <li>Handle failures: Discovered instances that fail after a configured number of times are dropped. They may be rediscovered later.</li> <li>Submit discovery-related metrics: the check submits the total number of discovered devices for a given <code>network_address</code> instance.</li> </ul>"},{"location":"architecture/snmp/#caveats","title":"Caveats","text":"<p>The approach described above is not ideal for several reasons:</p> <ul> <li>The check code is harder to understand since the two distinct paths (\"single device\" vs \"entire network\") live in a single integration.</li> <li>Each network instance manages several long-running threads that span well beyond the lifespan of a single check run.</li> <li>Each network check pseudo-schedules other instances, which is normally the responsibility of the Agent.</li> </ul> <p>For this reason, auto-discovery was eventually implemented in the Agent as a proper service listener (see below), and users should be discouraged from using Python auto-discovery. When the deprecation period expires, we will be able to remove auto-discovery logic from the Python check, making it exclusively focused on checking single devices.</p>"},{"location":"architecture/snmp/#agent-auto-discovery","title":"Agent Auto-Discovery","text":""},{"location":"architecture/snmp/#dependencies_1","title":"Dependencies","text":"<p>Agent auto-discovery uses GoSNMP to get the <code>sysObjectID</code> of devices in the network.</p>"},{"location":"architecture/snmp/#standalone-agent","title":"Standalone Agent","text":"<p>Agent auto-discovery implements the same logic than the Python auto-discovery, but as a service listener in the Agent Go package.</p> <p>This approach leverages the existing Agent scheduling logic, and makes it possible to scale device auto-discovery using the Datadog Cluster Agent (see Cluster Agent support).</p> <p>Pending official documentation, here is an example configuration:</p> <pre><code># datadog.yaml\n\nlisteners:\n- name: snmp\n\nsnmp_listener:\nconfigs:\n- network: 10.0.0.0/28\nversion: 2\ncommunity: public\n- network: 10.0.1.0/30\nversion: 3\nuser: my-snmp-user\nauthentication_protocol: SHA\nauthentication_key: \"*****\"\nprivacy_protocol: AES\nprivacy_key: \"*****\"\nignored_ip_addresses:\n- 10.0.1.0\n- 10.0.1.1\n</code></pre>"},{"location":"architecture/snmp/#cluster-agent-support","title":"Cluster Agent Support","text":"<p>For Kubernetes environments, the Cluster Agent can be configured to use the SNMP Agent auto-discovery (via snmp listener) logic as a source of Cluster checks.</p> <p></p> <p>The Datadog Cluster Agent (DCA) uses the <code>snmp_listener</code> config (Agent auto-discovery) to listen for IP ranges, then schedules snmp check instances to be run by one or more normal Datadog Agents.</p> <p>Agent auto-discovery combined with Cluster Agent is very scalable, it can be used to monitor a large number of snmp devices.</p>"},{"location":"architecture/snmp/#example-cluster-agent-setup-with-snmp-agent-auto-discovery-using-datadog-helm-chart","title":"Example Cluster Agent setup with SNMP Agent auto-discovery using Datadog helm-chart","text":"<p>First you need to add Datadog Helm repository.</p> <pre><code>helm repo add datadog https://helm.datadoghq.com\nhelm repo update\n</code></pre> <p>Then run:</p> <pre><code>helm install datadog-monitoring --set datadog.apiKey=&lt;YOUR_API_KEY&gt; -f cluster-agent-values.yaml datadog/datadog\n</code></pre> Example cluster-agent-values.yaml <pre><code>datadog:\n## @param apiKey - string - required\n## Set this to your Datadog API key before the Agent runs.\n## ref: https://app.datadoghq.com/account/settings/agent/latest?platform=kubernetes\n#\napiKey: &lt;DATADOG_API_KEY&gt;\n\n## @param clusterName - string - optional\n## Set a unique cluster name to allow scoping hosts and Cluster Checks easily\n## The name must be unique and must be dot-separated tokens where a token can be up to 40 characters with the following restrictions:\n## * Lowercase letters, numbers, and hyphens only.\n## * Must start with a letter.\n## * Must end with a number or a letter.\n## Compared to the rules of GKE, dots are allowed whereas they are not allowed on GKE:\n## https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters#Cluster.FIELDS.name\n#\nclusterName: my-snmp-cluster\n\n## @param clusterChecks - object - required\n## Enable the Cluster Checks feature on both the cluster-agents and the daemonset\n## ref: https://docs.datadoghq.com/agent/autodiscovery/clusterchecks/\n## Autodiscovery via Kube Service annotations is automatically enabled\n#\nclusterChecks:\nenabled: true\n\n## @param tags  - list of key:value elements - optional\n## List of tags to attach to every metric, event and service check collected by this Agent.\n##\n## Learn more about tagging: https://docs.datadoghq.com/tagging/\n#\ntags:\n- 'env:test-snmp-cluster-agent'\n\n## @param clusterAgent - object - required\n## This is the Datadog Cluster Agent implementation that handles cluster-wide\n## metrics more cleanly, separates concerns for better rbac, and implements\n## the external metrics API so you can autoscale HPAs based on datadog metrics\n## ref: https://docs.datadoghq.com/agent/kubernetes/cluster/\n#\nclusterAgent:\n## @param enabled - boolean - required\n## Set this to true to enable Datadog Cluster Agent\n#\nenabled: true\n\n## @param confd - list of objects - optional\n## Provide additional cluster check configurations\n## Each key will become a file in /conf.d\n## ref: https://docs.datadoghq.com/agent/autodiscovery/\n#\nconfd:\n# Static checks\nhttp_check.yaml: |-\ncluster_check: true\ninstances:\n- name: 'Check Example Site1'\nurl: http://example.net\n- name: 'Check Example Site2'\nurl: http://example.net\n- name: 'Check Example Site3'\nurl: http://example.net\n# Autodiscovery template needed for `snmp_listener` to create instance configs\nsnmp.yaml: |-\ncluster_check: true\n\n# AD config below is copied from: https://github.com/DataDog/datadog-agent/blob/master/cmd/agent/dist/conf.d/snmp.d/auto_conf.yaml\nad_identifiers:\n- snmp\ninit_config:\ninstances:\n-\n## @param ip_address - string - optional\n## The IP address of the device to monitor.\n#\nip_address: \"%%host%%\"\n\n## @param port - integer - optional - default: 161\n## Default SNMP port.\n#\nport: \"%%port%%\"\n\n## @param snmp_version - integer - optional - default: 2\n## If you are using SNMP v1 set snmp_version to 1 (required)\n## If you are using SNMP v3 set snmp_version to 3 (required)\n#\nsnmp_version: \"%%extra_version%%\"\n\n## @param timeout - integer - optional - default: 5\n## Amount of second before timing out.\n#\ntimeout: \"%%extra_timeout%%\"\n\n## @param retries - integer - optional - default: 5\n## Amount of retries before failure.\n#\nretries: \"%%extra_retries%%\"\n\n## @param community_string - string - optional\n## Only useful for SNMP v1 &amp; v2.\n#\ncommunity_string: \"%%extra_community%%\"\n\n## @param user - string - optional\n## USERNAME to connect to your SNMP devices.\n#\nuser: \"%%extra_user%%\"\n\n## @param authKey - string - optional\n## Authentication key to use with your Authentication type.\n#\nauthKey: \"%%extra_auth_key%%\"\n\n## @param authProtocol - string - optional\n## Authentication type to use when connecting to your SNMP devices.\n## It can be one of: MD5, SHA, SHA224, SHA256, SHA384, SHA512.\n## Default to MD5 when `authKey` is specified.\n#\nauthProtocol: \"%%extra_auth_protocol%%\"\n\n## @param privKey - string - optional\n## Privacy type key to use with your Privacy type.\n#\nprivKey: \"%%extra_priv_key%%\"\n\n## @param privProtocol - string - optional\n## Privacy type to use when connecting to your SNMP devices.\n## It can be one of: DES, 3DES, AES, AES192, AES256, AES192C, AES256C.\n## Default to DES when `privKey` is specified.\n#\nprivProtocol: \"%%extra_priv_protocol%%\"\n\n## @param context_engine_id - string - optional\n## ID of your context engine; typically unneeded.\n## (optional SNMP v3-only parameter)\n#\ncontext_engine_id: \"%%extra_context_engine_id%%\"\n\n## @param context_name - string - optional\n## Name of your context (optional SNMP v3-only parameter).\n#\ncontext_name: \"%%extra_context_name%%\"\n\n## @param tags - list of key:value element - optional\n## List of tags to attach to every metric, event and service check emitted by this integration.\n##\n## Learn more about tagging: https://docs.datadoghq.com/tagging/\n#\ntags:\n# The autodiscovery subnet the device is part of.\n# Used by Agent autodiscovery to pass subnet name.\n- \"autodiscovery_subnet:%%extra_autodiscovery_subnet%%\"\n\n## @param extra_tags - string - optional\n## Comma separated tags to attach to every metric, event and service check emitted by this integration.\n## Example:\n##  extra_tags: \"tag1:val1,tag2:val2\"\n#\nextra_tags: \"%%extra_tags%%\"\n\n## @param oid_batch_size - integer - optional - default: 60\n## The number of OIDs handled by each batch. Increasing this number improves performance but\n## uses more resources.\n#\noid_batch_size: \"%%extra_oid_batch_size%%\"\n\n## @param datadog-cluster.yaml - object - optional\n## Specify custom contents for the datadog cluster agent config (datadog-cluster.yaml).\n#\ndatadog_cluster_yaml:\nlisteners:\n- name: snmp\n\n# See here for all `snmp_listener` configs: https://github.com/DataDog/datadog-agent/blob/master/pkg/config/config_template.yaml\nsnmp_listener:\nworkers: 2\ndiscovery_interval: 10\nconfigs:\n- network: 192.168.1.16/29\nversion: 2\nport: 1161\ncommunity: cisco_icm\n- network: 192.168.1.16/29\nversion: 2\nport: 1161\ncommunity: f5\n</code></pre> <p>TODO: architecture diagram, example setup, affected files and repos, local testing tools, etc.</p>"},{"location":"architecture/vsphere/","title":"vSphere","text":""},{"location":"architecture/vsphere/#high-level-information","title":"High-Level information","text":""},{"location":"architecture/vsphere/#product-overview","title":"Product overview","text":"<p>vSphere is a VMware product dedicated to managing a (usually) on-premise infrastructure. From physical machines running VMware ESXi that are called ESXi Hosts, users can spin up or migrate Virtual Machines from one host to another.</p> <p>vSphere is an integrated solution and provides an easy managing interface over concepts like data storage, or computing resource.</p>"},{"location":"architecture/vsphere/#terminology","title":"Terminology","text":"<p>This section details some of vSphere specific elements. This section does not intend to be an extensive list, but rather a place for those unfamiliar with the product to have the basics required to understand how the Datadog integration works.</p> <ul> <li>vSphere - The complete suite of tools and technologies detailed in this article.</li> <li>vCenter server - The main machine which controls ESXi hosts and provides both a web UI and an API to control the vSphere environment.</li> <li>vCSA (vCenter Server Appliance) - A specific kind of vCenter where the software runs in a dedicated Linux machine (more recent). By opposition, the legacy vCenter is typically installed on an existing Windows machine.</li> <li>ESXi host - The physical machine controlled by vCenter where the ESXi (bare-metal) virtualizer is installed. The host boots a minimal OS that can run Virtual Machines.</li> <li>VM - What anyone using vSphere really needs in the end, instances that can run applications and code. Note: Datadog monitors both ESXi hosts and VMs and it calls them both \"host\" (they are in the host map).</li> <li>Attributes/tags - It is possible to add attributes and tags to any vSphere resource, note that those two are now very similar with \"attributes\" being the deprecated thing to use.</li> <li>Datacenter - A set of resources grouped together. A single vCenter server can handle multiple datacenters.</li> <li>Datastore - A virtual vSphere concept to represent data storing capabilities. It can be an NFS server that ESXi hosts have read/write access to, it can be a mounted disk on the host and more. Datastores are often shared between multiple hosts. This allows Virtual Machines to be migrated from one host to another.</li> <li>Cluster - A logical grouping of computational resources, you can add multiple ESXi hosts in your cluster and then you can create VM in the cluster (and not on a specific host, vSphere will take care of placing your VM in one of the ESXi hosts and migrating it when needed).</li> <li>Photon OS - An open-source minimal Linux distribution and used by both ESXi and vCSA as a base.</li> </ul>"},{"location":"architecture/vsphere/#the-integration","title":"The integration","text":""},{"location":"architecture/vsphere/#setup","title":"Setup","text":"<p>The Datadog vSphere integration runs from a single agent and pulls all the information from a single vCenter endpoint. Because the agent cannot run directly on Photon OS, it is usually required that the agent runs within a dedicated VM inside the vSphere infrastructure.</p> <p>Once the agent is running, the minimal configuration (as of version 5.x) is as follows:</p> <pre><code>init_config:\ninstances:\n- host:\nusername:\npassword:\nuse_legacy_check_version: false\nempty_default_hostname: true\n</code></pre> <ul> <li> <p><code>host</code> is the endpoint used to access the vSphere Client from a web browser. The host is either a FQDN or an IP, not an http url.</p> </li> <li> <p><code>username</code> and <code>password</code> are the credentials to log in to vCenter.</p> </li> <li> <p><code>use_legacy_check_version</code> is a backward compatibility flag. It should always be set to false and this flag will be removed in a future version of the integration. Setting it to true tells the agent to use an older and deprecated version of the vSphere integration.</p> </li> <li> <p><code>empty_default_hostname</code> is a field used by the agent directly (and not the integration). By default, the agent does not allow submitting metrics without attaching an explicit host tag unless this flag is set to true. The vSphere integration uses that behavior for some metrics and service checks. For example, the <code>vsphere.vm.count</code> metric which gives a count of the VMs in the infra is not submitted with a host tag. This is particularly important if the agent runs inside a vSphere VM. If the <code>vsphere.vm.count</code> was submitted with a host tag, the Datadog backend would attach all the other host tags to the metric, for example <code>vsphere_type:vm</code> or <code>vsphere_host:&lt;NAME_OF_THE_ESX_HOST&gt;</code> which makes the metric almost impossible to use.</p> </li> </ul>"},{"location":"architecture/vsphere/#concepts","title":"Concepts","text":""},{"location":"architecture/vsphere/#collection-level","title":"Collection level","text":"<p>vSphere metrics are documented in their documentation page an each metric has a defined \"collection level\".</p> <p>That level determines the amount of data gathered by the integration and especially which metrics are available. More details here.</p> <p>By default, only the level 1 metrics are collected but this can be increased in the integration configuration file.</p>"},{"location":"architecture/vsphere/#realtime-vs-historical","title":"Realtime vs historical","text":"<ul> <li> <p>Each ESXi host collects and stores data for each metric on himself and every VM it hosts every 20 seconds. Those data points are stored for up to one hour and are called realtime. Note: Each metric concerns always either a VM or an ESXi hosts. Metrics that concern datastore for example are not collected in the ESXi hosts.</p> </li> <li> <p>Additionally, the vCenter server collects data from all the ESXi hosts and stores the datapoint with some aggregation rollup into its own database. Those data points are called \"historical\".</p> </li> <li> <p>Finally, the vCenter server also collects metrics for other kinds of resources (like Datastore, ClusterComputeResource, Datacenter...) Those data points are necessarily \"historical\".</p> </li> </ul> <p>The reason for such an important distinction is that historical metrics are much MUCH slower to collect than realtime metrics. The vSphere integration will always collect the \"realtime\" data for metrics that concern ESXi hosts and VMs. But the integration also collects metrics for Datastores, ClusterComputeResources, Datacenters, and maybe others in the future.</p> <p>That's why, in the context of the Datadog vSphere integration, we usually simplify by considering that:</p> <ul> <li> <p>VMs and ESXi hosts are \"realtime resources\". Metrics for such resources are quick and easy to get by querying vCenter that will in turn query all the ESXi hosts.</p> </li> <li> <p>Datastores, ClusterComputeResources, and Datacenters are \"historical resources\" and are much slower to collect.</p> </li> </ul> <p>To collect all metrics (realtime and historical), it is advised to use two \"check instances\". One with <code>collection_type: realtime</code> and one with <code>collection_type: historical</code> . This way all metrics will be collected but because both check instances are on different schedules, the slowness of collecting historical metrics won't affect the rate at which realtime metrics are collected.</p>"},{"location":"architecture/vsphere/#vsphere-tags-and-attributes","title":"vSphere tags and attributes","text":"<p>Similarly to how Datadog allows you to add tags to your different hosts (thins like the <code>os</code> or the <code>instance-type</code> of your machines), vSphere has \"tags\" and \"attributes\".</p> <p>A lot of details can be found here: https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-E8E854DD-AA97-4E0C-8419-CE84F93C4058.html#:~:text=Tags%20and%20attributes%20allow%20you,that%20tag%20to%20a%20category.</p> <p>But the overall idea is that both tags and attributes are additional information that you can attach to your vSphere resources and that \"tags\" are newer and more featureful than \"attributes\".</p>"},{"location":"architecture/vsphere/#filtering","title":"Filtering","text":"<p>A very flexible filtering system has been implemented with the vSphere integration.</p> <p>This allows fine-tuned configuration so that:</p> <ul> <li>You only pay for the host and VMs you really want to monitor.</li> <li>You reduce the load on your vCenter server by running just the queries that you need.</li> <li>You improve the check runtime which otherwise increases linearly with the size of their infrastructure and that was seen to take up to 10min in some large environments.</li> </ul> <p>We provide two types of filtering, one based on metrics, the other based on resources.</p> <p>The metric filter is fairly simple, for each resource type, you can provide some regexes. If a metric match any of the filter, it will be fetched and submitted. The configuration looks like this:</p> <pre><code>metric_filters:\nvm:\n- cpu\\..*\n- mem\\..*\nhost:\n- WHATEVER # Excludes everything\ndatacenter:\n- .*\n</code></pre> <p>The resource filter on the other hand, allows to exclude some vSphere resources (VM, ESXi host, etc.), based on an \"attribute\" of that resource. The possible attributes as of today are: - <code>name</code>, literally the name of the resource (as defined in vCenter) - <code>inventory_path</code>, a path-like string that represents the location of the resource in the inventory tree as each resource only ever has a single parent and recursively up to the root. For example: <code>/my.datacenter.local/vm/staging/myservice/vm_name</code> - <code>tag</code>, see the <code>tags and attributes</code> section. Used to filter resources based on the attached tags. - <code>attribute</code>, see the <code>tags and attributes</code> section. Used to filter resources based on the attached attributes. - <code>hostname</code> (only for VMs), the name of the ESXi host where the VM is running. - <code>guest_hostname</code> (only for VMs), the name of the OS as reported from within the machine. VMware tools have to be installed on the VM otherwise, vCenter is not able to fetch this information.</p> <p>A possible filtering configuration would look like this: <pre><code> resource_filters:\n- resource: vm\nproperty: name\npatterns:\n- &lt;VM_REGEX_1&gt;\n- &lt;VM_REGEX_2&gt;\n- resource: vm\nproperty: hostname\npatterns:\n- &lt;HOSTNAME_REGEX&gt;\n- resource: vm\nproperty: tag\ntype: blacklist\npatterns:\n- '^env:staging$'\n- resource: vm\nproperty: tag\ntype: whitelist  # type defaults to whitelist\npatterns:\n- '^env:.*$'\n- resource: vm\nproperty: guest_hostname\npatterns:\n- &lt;GUEST_HOSTNAME_REGEX&gt;\n- resource: host\nproperty: inventory_path\npatterns:\n- &lt;INVENTORY_PATH_REGEX&gt;\n</code></pre></p>"},{"location":"architecture/vsphere/#instance-tag","title":"Instance tag","text":"<p>In vSphere each metric is defined by three \"dimensions\".</p> <ul> <li>The resource on which the metric applies (for example the VM called \"abc1\")</li> <li>The name of the metric (for example cpu.usage).</li> <li>An additional available dimension that varies between metrics. (for example the cpu core id)</li> </ul> <p>This is similar to how Datadog represent metrics, except that the context cardinality is limited to two \"keys\", the name of the resource (usually the \"host\" tag), and there is space for one additional tag key.</p> <p>This available tag key is defined as the \"instance\" property, or \"instance tag\" in vSphere, and this dimension is not collected by default by the Datadog integration as it can have too big performance implications in large systems when compared to their added value from a monitoring perspective.</p> <p>Also when fetching metrics with the instance tag, vSphere only provides the value of the instance tag, it doesn't expose a human-readable \"key\" for that tag. In the <code>cpu.usage</code> metric with the core_id as the instance tag, the integration has to \"know\" that the meaning of the instance tag and that's why we rely on a hardcoded list in the integration.</p> <p>Because this instance tag can provide additional visibility, it is possible to enable it for some metrics from the configuration. For example, if we're really interested in getting the usage of the cpu per core, the setup can look like this:</p> <pre><code>collect_per_instance_filters:\nvm:\n- cpu\\.usage\\..*\n</code></pre>"},{"location":"architecture/win32_event_log/","title":"Windows Event Log","text":""},{"location":"architecture/win32_event_log/#overview","title":"Overview","text":"<p>Users set a <code>path</code> with which to collect events from that is the name of a channel like <code>System</code>, <code>Application</code>, etc.</p> <p>There are 3 ways to select filter criteria rather than collecting all events:</p> <ul> <li><code>query</code> - A raw XPath or structured XML query used to filter events. This overrides any selected <code>filters</code>.</li> <li> <p><code>filters</code> - A mapping of properties to allowed values. Every filter (equivalent to the <code>and</code> operator) must match   any value (equivalent to the <code>or</code> operator). This option is a convenience for a <code>query</code> that is relatively basic.</p> <p>Rather than collect all events and perform filtering within the check, the filters are converted to an XPath expression. This approach offloads all filtering to the kernel (like <code>query</code>), which increases performance and reduces bandwidth usage when connecting to a remote machine.</p> </li> <li> <p><code>included_messages</code>/<code>excluded_messages</code> - These are regular expression patterns used to filter by events' messages   specifically (if a message is found), with the exclude list taking precedence. These may be used in place of or   with <code>query</code>/<code>filters</code>, as there exists no query construct by which to select a message attribute.</p> </li> </ul> <p>A pull subscription model is used. At every check run, the cached event log handle waits to be signaled for a configurable number of seconds. If signaled, the check then polls all available events in batches of a configurable size.</p> <p>At configurable intervals, the most recently encountered event is saved to the filesystem. This is useful for preventing duplicate events being sent as a consequence of Agent restarts, especially when the <code>start</code> option is set to <code>oldest</code>.</p>"},{"location":"architecture/win32_event_log/#logs","title":"Logs","text":"<p>Events may alternatively be configured to be submitted as logs. The code for that resides here.</p> <p>Only a subset of the check's functionality is available. Namely, each log configuration will collect all events of the given channel without filtering, tagging, nor remote connection options.</p> <p>This implementation uses the push subscription model. There is a bit of C in charge of rendering the relevant data and registering the Go tailer callback that ultimately sends the log to the backend.</p>"},{"location":"architecture/win32_event_log/#legacy-mode","title":"Legacy mode","text":"<p>Setting <code>legacy_mode</code> to <code>true</code> in the check will use WMI to collect events, which is significantly more resource intensive. This mode has entirely different configuration options and will be removed in a future release.</p> <p>Agent 6 can only use this mode as Python 2 does not support the new implementation.</p>"},{"location":"base/about/","title":"About","text":"<p>The Base package provides all the functionality and utilities necessary for writing Agent Integrations. Most importantly it provides the AgentCheck base class from which every Check must be inherited.</p> <p>You would use it like so:</p> <pre><code>from datadog_checks.base import AgentCheck\n\n\nclass AwesomeCheck(AgentCheck):\n    __NAMESPACE__ = 'awesome'\n\n    def check(self, instance):\n        self.gauge('test', 1.23, tags=['foo:bar'])\n</code></pre> <p>The <code>check</code> method is what the Datadog Agent will execute.</p> <p>In this example we created a Check and gave it a namespace of <code>awesome</code>. This means that by default, every submission's name will be prefixed with <code>awesome.</code>.</p> <p>We submitted a gauge metric named <code>awesome.test</code> with a value of <code>1.23</code> tagged by <code>foo:bar</code>.</p> <p>The magic hidden by the usability of the API is that this actually calls a C binding which communicates with the Agent (written in Go).</p> <p></p>"},{"location":"base/api/","title":"API","text":""},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck","title":"<code>datadog_checks.base.checks.base.AgentCheck</code>","text":"<p>The base class for any Agent based integration.</p> <p>In general, you don't need to and you should not override anything from the base class except the <code>check</code> method but sometimes it might be useful for a Check to have its own constructor.</p> <p>When overriding <code>__init__</code> you have to remember that, depending on the configuration, the Agent might create several different Check instances and the method would be called as many times.</p> <p>Agent 6,7 signature:</p> <pre><code>AgentCheck(name, init_config, instances)    # instances contain only 1 instance\nAgentCheck.check(instance)\n</code></pre> <p>Agent 8 signature:</p> <pre><code>AgentCheck(name, init_config, instance)     # one instance\nAgentCheck.check()                          # no more instance argument for check method\n</code></pre> <p>Note</p> <p>when loading a Custom check, the Agent will inspect the module searching for a subclass of <code>AgentCheck</code>. If such a class exists but has been derived in turn, it'll be ignored - you should never derive from an existing Check.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>@traced_class\nclass AgentCheck(object):\n\"\"\"\n    The base class for any Agent based integration.\n    In general, you don't need to and you should not override anything from the base\n    class except the `check` method but sometimes it might be useful for a Check to\n    have its own constructor.\n    When overriding `__init__` you have to remember that, depending on the configuration,\n    the Agent might create several different Check instances and the method would be\n    called as many times.\n    Agent 6,7 signature:\n        AgentCheck(name, init_config, instances)    # instances contain only 1 instance\n        AgentCheck.check(instance)\n    Agent 8 signature:\n        AgentCheck(name, init_config, instance)     # one instance\n        AgentCheck.check()                          # no more instance argument for check method\n    !!! note\n        when loading a Custom check, the Agent will inspect the module searching\n        for a subclass of `AgentCheck`. If such a class exists but has been derived in\n        turn, it'll be ignored - **you should never derive from an existing Check**.\n    \"\"\"\n# If defined, this will be the prefix of every metric/service check and the source type of events\n__NAMESPACE__ = ''\nOK, WARNING, CRITICAL, UNKNOWN = ServiceCheck\n# Used by `self.http` for an instance of RequestsWrapper\nHTTP_CONFIG_REMAPPER = None\n# Used by `create_tls_context` for an instance of RequestsWrapper\nTLS_CONFIG_REMAPPER = None\n# Used by `self.set_metadata` for an instance of MetadataManager\n#\n# This is a mapping of metadata names to functions. When you call `self.set_metadata(name, value, **options)`,\n# if `name` is in this mapping then the corresponding function will be called with the `value`, and the\n# return value(s) will be sent instead.\n#\n# Transformer functions must satisfy the following signature:\n#\n#    def transform_&lt;NAME&gt;(value: Any, options: dict) -&gt; Union[str, Dict[str, str]]:\n#\n# If the return type is a string, then it will be sent as the value for `name`. If the return type is\n# a mapping type, then each key will be considered a `name` and will be sent with its (str) value.\nMETADATA_TRANSFORMERS = None\nFIRST_CAP_RE = re.compile(br'(.)([A-Z][a-z]+)')\nALL_CAP_RE = re.compile(br'([a-z0-9])([A-Z])')\nMETRIC_REPLACEMENT = re.compile(br'([^a-zA-Z0-9_.]+)|(^[^a-zA-Z]+)')\nTAG_REPLACEMENT = re.compile(br'[,\\+\\*\\-/()\\[\\]{}\\s]')\nMULTIPLE_UNDERSCORE_CLEANUP = re.compile(br'__+')\nDOT_UNDERSCORE_CLEANUP = re.compile(br'_*\\._*')\n# allows to set a limit on the number of metric name and tags combination\n# this check can send per run. This is useful for checks that have an unbounded\n# number of tag values that depend on the input payload.\n# The logic counts one set of tags per gauge/rate/monotonic_count call, and de-duplicates\n# sets of tags for other metric types. The first N sets of tags in submission order will\n# be sent to the aggregator, the rest are dropped. The state is reset after each run.\n# See https://github.com/DataDog/integrations-core/pull/2093 for more information.\nDEFAULT_METRIC_LIMIT = 0\n# Allow tracing for classic integrations\ndef __init_subclass__(cls, *args, **kwargs):\ntry:\n# https://github.com/python/mypy/issues/4660\nsuper().__init_subclass__(*args, **kwargs)  # type: ignore\nreturn traced_class(cls)\nexcept Exception:\nreturn cls\ndef __init__(self, *args, **kwargs):\n# type: (*Any, **Any) -&gt; None\n\"\"\"\n        Parameters:\n            name (str):\n                the name of the check\n            init_config (dict):\n                the `init_config` section of the configuration.\n            instance (list[dict]):\n                a one-element list containing the instance options from the\n                configuration file (a list is used to keep backward compatibility with\n                older versions of the Agent).\n        \"\"\"\n# NOTE: these variable assignments exist to ease type checking when eventually assigned as attributes.\nname = kwargs.get('name', '')\ninit_config = kwargs.get('init_config', {})\nagentConfig = kwargs.get('agentConfig', {})\ninstances = kwargs.get('instances', [])\nif len(args) &gt; 0:\nname = args[0]\nif len(args) &gt; 1:\ninit_config = args[1]\nif len(args) &gt; 2:\n# agent pass instances as tuple but in test we are usually using list, so we are testing for both\nif len(args) &gt; 3 or not isinstance(args[2], (list, tuple)) or 'instances' in kwargs:\n# old-style init: the 3rd argument is `agentConfig`\nagentConfig = args[2]\nif len(args) &gt; 3:\ninstances = args[3]\nelse:\n# new-style init: the 3rd argument is `instances`\ninstances = args[2]\n# NOTE: Agent 6+ should pass exactly one instance... But we are not abiding by that rule on our side\n# everywhere just yet. It's complicated... See: https://github.com/DataDog/integrations-core/pull/5573\ninstance = instances[0] if instances else None\nself.check_id = ''\nself.name = name  # type: str\nself.init_config = init_config  # type: InitConfigType\nself.agentConfig = agentConfig  # type: AgentConfigType\nself.instance = instance  # type: InstanceType\nself.instances = instances  # type: List[InstanceType]\nself.warnings = []  # type: List[str]\nself.disable_generic_tags = (\nis_affirmative(self.instance.get('disable_generic_tags', False)) if instance else False\n)\nself.debug_metrics = {}\nif self.init_config is not None:\nself.debug_metrics.update(self.init_config.get('debug_metrics', {}))\nif self.instance is not None:\nself.debug_metrics.update(self.instance.get('debug_metrics', {}))\n# `self.hostname` is deprecated, use `datadog_agent.get_hostname()` instead\nself.hostname = datadog_agent.get_hostname()  # type: str\nlogger = logging.getLogger('{}.{}'.format(__name__, self.name))\nself.log = CheckLoggingAdapter(logger, self)\nmetric_patterns = self.instance.get('metric_patterns', {}) if instance else {}\nif not isinstance(metric_patterns, dict):\nraise ConfigurationError('Setting `metric_patterns` must be a mapping')\nself.exclude_metrics_pattern = self._create_metrics_pattern(metric_patterns, 'exclude')\nself.include_metrics_pattern = self._create_metrics_pattern(metric_patterns, 'include')\n# TODO: Remove with Agent 5\n# Set proxy settings\nself.proxies = self._get_requests_proxy()\nif not self.init_config:\nself._use_agent_proxy = True\nelse:\nself._use_agent_proxy = is_affirmative(self.init_config.get('use_agent_proxy', True))\n# TODO: Remove with Agent 5\nself.default_integration_http_timeout = float(self.agentConfig.get('default_integration_http_timeout', 9))\nself._deprecations = {\n'increment': (\nFalse,\n(\n'DEPRECATION NOTICE: `AgentCheck.increment`/`AgentCheck.decrement` are deprecated, please '\n'use `AgentCheck.gauge` or `AgentCheck.count` instead, with a different metric name'\n),\n),\n'device_name': (\nFalse,\n(\n'DEPRECATION NOTICE: `device_name` is deprecated, please use a `device:` '\n'tag in the `tags` list instead'\n),\n),\n'in_developer_mode': (\nFalse,\n'DEPRECATION NOTICE: `in_developer_mode` is deprecated, please stop using it.',\n),\n'no_proxy': (\nFalse,\n(\n'DEPRECATION NOTICE: The `no_proxy` config option has been renamed '\n'to `skip_proxy` and will be removed in a future release.'\n),\n),\n'service_tag': (\nFalse,\n(\n'DEPRECATION NOTICE: The `service` tag is deprecated and has been renamed to `%s`. '\n'Set `disable_legacy_service_tag` to `true` to disable this warning. '\n'The default will become `true` and cannot be changed in Agent version 8.'\n),\n),\n'_config_renamed': (\nFalse,\n(\n'DEPRECATION NOTICE: The `%s` config option has been renamed '\n'to `%s` and will be removed in a future release.'\n),\n),\n}  # type: Dict[str, Tuple[bool, str]]\n# Setup metric limits\nself.metric_limiter = self._get_metric_limiter(self.name, instance=self.instance)\n# Lazily load and validate config\nself._config_model_instance = None  # type: Any\nself._config_model_shared = None  # type: Any\n# Functions that will be called exactly once (if successful) before the first check run\nself.check_initializations = deque()  # type: Deque[Callable[[], None]]\nif not PY2:\nself.check_initializations.append(self.load_configuration_models)\ndef _create_metrics_pattern(self, metric_patterns, option_name):\nall_patterns = metric_patterns.get(option_name, [])\nif not isinstance(all_patterns, list):\nraise ConfigurationError('Setting `{}` of `metric_patterns` must be an array'.format(option_name))\nmetrics_patterns = []\nfor i, entry in enumerate(all_patterns, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(\n'Entry #{} of setting `{}` of `metric_patterns` must be a string'.format(i, option_name)\n)\nif not entry:\nself.log.debug(\n'Entry #%s of setting `%s` of `metric_patterns` must not be empty, ignoring', i, option_name\n)\ncontinue\nmetrics_patterns.append(entry)\nif metrics_patterns:\nreturn re.compile('|'.join(metrics_patterns))\nreturn None\ndef _get_metric_limiter(self, name, instance=None):\n# type: (str, InstanceType) -&gt; Optional[Limiter]\nlimit = self._get_metric_limit(instance=instance)\nif limit &gt; 0:\nreturn Limiter(name, 'metrics', limit, self.warning)\nreturn None\ndef _get_metric_limit(self, instance=None):\n# type: (InstanceType) -&gt; int\nif instance is None:\n# NOTE: Agent 6+ will now always pass an instance when calling into a check, but we still need to\n# account for this case due to some tests not always passing an instance on init.\nself.log.debug(\n\"No instance provided (this is deprecated!). Reverting to the default metric limit: %s\",\nself.DEFAULT_METRIC_LIMIT,\n)\nreturn self.DEFAULT_METRIC_LIMIT\nmax_returned_metrics = instance.get('max_returned_metrics', self.DEFAULT_METRIC_LIMIT)\ntry:\nlimit = int(max_returned_metrics)\nexcept (ValueError, TypeError):\nself.warning(\n\"Configured 'max_returned_metrics' cannot be interpreted as an integer: %s. \"\n\"Reverting to the default limit: %s\",\nmax_returned_metrics,\nself.DEFAULT_METRIC_LIMIT,\n)\nreturn self.DEFAULT_METRIC_LIMIT\n# Do not allow to disable limiting if the class has set a non-zero default value.\nif limit == 0 and self.DEFAULT_METRIC_LIMIT &gt; 0:\nself.warning(\n\"Setting 'max_returned_metrics' to zero is not allowed. Reverting to the default metric limit: %s\",\nself.DEFAULT_METRIC_LIMIT,\n)\nreturn self.DEFAULT_METRIC_LIMIT\nreturn limit\n@staticmethod\ndef load_config(yaml_str):\n# type: (str) -&gt; Any\n\"\"\"\n        Convenience wrapper to ease programmatic use of this class from the C API.\n        \"\"\"\nreturn yaml.safe_load(yaml_str)\n@property\ndef http(self):\n# type: () -&gt; RequestsWrapper\n\"\"\"\n        Provides logic to yield consistent network behavior based on user configuration.\n        Only new checks or checks on Agent 6.13+ can and should use this for HTTP requests.\n        \"\"\"\nif not hasattr(self, '_http'):\nself._http = RequestsWrapper(self.instance or {}, self.init_config, self.HTTP_CONFIG_REMAPPER, self.log)\nreturn self._http\n@property\ndef diagnosis(self):\n# type: () -&gt; Diagnosis\n\"\"\"\n        A Diagnosis object to register explicit diagnostics and record diagnoses.\n        \"\"\"\nif not hasattr(self, '_diagnosis'):\nself._diagnosis = Diagnosis(sanitize=self.sanitize)\nreturn self._diagnosis\ndef get_tls_context(self, refresh=False, overrides=None):\n# type: (bool, Dict[AnyStr, Any]) -&gt; ssl.SSLContext\n\"\"\"\n        Creates and cache an SSLContext instance based on user configuration.\n        Note that user configuration can be overridden by using `overrides`.\n        This should only be applied to older integration that manually set config values.\n        Since: Agent 7.24\n        \"\"\"\nif not hasattr(self, '_tls_context_wrapper'):\nself._tls_context_wrapper = TlsContextWrapper(\nself.instance or {}, self.TLS_CONFIG_REMAPPER, overrides=overrides\n)\nif refresh:\nself._tls_context_wrapper.refresh_tls_context()\nreturn self._tls_context_wrapper.tls_context\n@property\ndef metadata_manager(self):\n# type: () -&gt; MetadataManager\n\"\"\"\n        Used for sending metadata via Go bindings.\n        \"\"\"\nif not hasattr(self, '_metadata_manager'):\nif not self.check_id and not using_stub_aggregator:\nraise RuntimeError('Attribute `check_id` must be set')\nself._metadata_manager = MetadataManager(self.name, self.check_id, self.log, self.METADATA_TRANSFORMERS)\nreturn self._metadata_manager\n@property\ndef check_version(self):\n# type: () -&gt; str\n\"\"\"\n        Return the dynamically detected integration version.\n        \"\"\"\nif not hasattr(self, '_check_version'):\n# 'datadog_checks.&lt;PACKAGE&gt;.&lt;MODULE&gt;...'\nmodule_parts = self.__module__.split('.')\npackage_path = '.'.join(module_parts[:2])\npackage = importlib.import_module(package_path)\n# Provide a default just in case\nself._check_version = getattr(package, '__version__', '0.0.0')\nreturn self._check_version\n@property\ndef in_developer_mode(self):\n# type: () -&gt; bool\nself._log_deprecation('in_developer_mode')\nreturn False\ndef log_typos_in_options(self, user_config, models_config, level):\n# only import it when running in python 3\nfrom jellyfish import jaro_winkler_similarity\nuser_configs = user_config or {}  # type: Dict[str, Any]\nmodels_config = models_config or {}\ntypos = set()  # type: Set[str]\nknown_options = {k for k, _ in models_config}  # type: Set[str]\nif not PY2:\nif isinstance(models_config, BaseModel):\n# Also add aliases, if any\nknown_options.update(set(models_config.model_dump(by_alias=True)))\nunknown_options = [option for option in user_configs.keys() if option not in known_options]  # type: List[str]\nfor unknown_option in unknown_options:\nsimilar_known_options = []  # type: List[Tuple[str, int]]\nfor known_option in known_options:\nratio = jaro_winkler_similarity(unknown_option, known_option)\nif ratio &gt; TYPO_SIMILARITY_THRESHOLD:\nsimilar_known_options.append((known_option, ratio))\ntypos.add(unknown_option)\nif len(similar_known_options) &gt; 0:\nsimilar_known_options.sort(key=lambda option: option[1], reverse=True)\nsimilar_known_options_names = [option[0] for option in similar_known_options]  # type: List[str]\nmessage = (\n'Detected potential typo in configuration option in {}/{} section: `{}`. Did you mean {}?'\n).format(self.name, level, unknown_option, ', or '.join(similar_known_options_names))\nself.log.warning(message)\nreturn typos\ndef load_configuration_models(self, package_path=None):\nif package_path is None:\n# 'datadog_checks.&lt;PACKAGE&gt;.&lt;MODULE&gt;...'\nmodule_parts = self.__module__.split('.')\npackage_path = '{}.config_models'.format('.'.join(module_parts[:2]))\nif self._config_model_shared is None:\nshared_config = copy.deepcopy(self.init_config)\ncontext = self._get_config_model_context(shared_config)\nshared_model = self.load_configuration_model(package_path, 'SharedConfig', shared_config, context)\ntry:\nself.log_typos_in_options(shared_config, shared_model, 'init_config')\nexcept Exception as e:\nself.log.debug(\"Failed to detect typos in `init_config` section: %s\", e)\nif shared_model is not None:\nself._config_model_shared = shared_model\nif self._config_model_instance is None:\ninstance_config = copy.deepcopy(self.instance)\ncontext = self._get_config_model_context(instance_config)\ninstance_model = self.load_configuration_model(package_path, 'InstanceConfig', instance_config, context)\ntry:\nself.log_typos_in_options(instance_config, instance_model, 'instances')\nexcept Exception as e:\nself.log.debug(\"Failed to detect typos in `instances` section: %s\", e)\nif instance_model is not None:\nself._config_model_instance = instance_model\n@staticmethod\ndef load_configuration_model(import_path, model_name, config, context):\ntry:\npackage = importlib.import_module(import_path)\n# TODO: remove the type ignore when we drop Python 2\nexcept ModuleNotFoundError as e:  # type: ignore\n# Don't fail if there are no models\nif str(e).startswith('No module named '):\nreturn\nraise\nmodel = getattr(package, model_name, None)\nif model is not None:\ntry:\nconfig_model = model.model_validate(config, context=context)\n# TODO: remove the type ignore when we drop Python 2\nexcept ValidationError as e:  # type: ignore\nerrors = e.errors()\nnum_errors = len(errors)\nmessage_lines = [\n'Detected {} error{} while loading configuration model `{}`:'.format(\nnum_errors, 's' if num_errors &gt; 1 else '', model_name\n)\n]\nfor error in errors:\nmessage_lines.append(\n' -&gt; '.join(\n# Start array indexes at one for user-friendliness\nstr(loc + 1) if isinstance(loc, int) else str(loc)\nfor loc in error['loc']\n)\n)\nmessage_lines.append('  {}'.format(error['msg']))\nraise_from(ConfigurationError('\\n'.join(message_lines)), None)\nelse:\nreturn config_model\ndef _get_config_model_context(self, config):\nreturn {'logger': self.log, 'warning': self.warning, 'configured_fields': frozenset(config)}\ndef register_secret(self, secret):\n# type: (str) -&gt; None\n\"\"\"\n        Register a secret to be scrubbed by `.sanitize()`.\n        \"\"\"\nif not hasattr(self, '_sanitizer'):\n# Configure lazily so that checks that don't use sanitization aren't affected.\nself._sanitizer = SecretsSanitizer()\nself.log.setup_sanitization(sanitize=self.sanitize)\nself._sanitizer.register(secret)\ndef sanitize(self, text):\n# type: (str) -&gt; str\n\"\"\"\n        Scrub any registered secrets in `text`.\n        \"\"\"\ntry:\nsanitizer = self._sanitizer\nexcept AttributeError:\nreturn text\nelse:\nreturn sanitizer.sanitize(text)\ndef _context_uid(self, mtype, name, tags=None, hostname=None):\n# type: (int, str, Sequence[str], str) -&gt; str\nreturn '{}-{}-{}-{}'.format(mtype, name, tags if tags is None else hash(frozenset(tags)), hostname)\ndef submit_histogram_bucket(\nself, name, value, lower_bound, upper_bound, monotonic, hostname, tags, raw=False, flush_first_value=False\n):\n# type: (str, float, int, int, bool, str, Sequence[str], bool, bool) -&gt; None\nif value is None:\n# ignore metric sample\nreturn\n# make sure the value (bucket count) is an integer\ntry:\nvalue = int(value)\nexcept ValueError:\nerr_msg = 'Histogram: {} has non integer value: {}. Only integer are valid bucket values (count).'.format(\nrepr(name), repr(value)\n)\nif using_stub_aggregator:\nraise ValueError(err_msg)\nself.warning(err_msg)\nreturn\ntags = self._normalize_tags_type(tags, metric_name=name)\nif hostname is None:\nhostname = ''\naggregator.submit_histogram_bucket(\nself,\nself.check_id,\nself._format_namespace(name, raw),\nvalue,\nlower_bound,\nupper_bound,\nmonotonic,\nhostname,\ntags,\nflush_first_value,\n)\ndef database_monitoring_query_sample(self, raw_event):\n# type: (str) -&gt; None\nif raw_event is None:\nreturn\naggregator.submit_event_platform_event(self, self.check_id, to_native_string(raw_event), \"dbm-samples\")\ndef database_monitoring_query_metrics(self, raw_event):\n# type: (str) -&gt; None\nif raw_event is None:\nreturn\naggregator.submit_event_platform_event(self, self.check_id, to_native_string(raw_event), \"dbm-metrics\")\ndef database_monitoring_query_activity(self, raw_event):\n# type: (str) -&gt; None\nif raw_event is None:\nreturn\naggregator.submit_event_platform_event(self, self.check_id, to_native_string(raw_event), \"dbm-activity\")\ndef database_monitoring_metadata(self, raw_event):\n# type: (str) -&gt; None\nif raw_event is None:\nreturn\naggregator.submit_event_platform_event(self, self.check_id, to_native_string(raw_event), \"dbm-metadata\")\ndef should_send_metric(self, metric_name):\nreturn not self._metric_excluded(metric_name) and self._metric_included(metric_name)\ndef _metric_included(self, metric_name):\nif self.include_metrics_pattern is None:\nreturn True\nreturn self.include_metrics_pattern.search(metric_name) is not None\ndef _metric_excluded(self, metric_name):\nif self.exclude_metrics_pattern is None:\nreturn False\nreturn self.exclude_metrics_pattern.search(metric_name) is not None\ndef _submit_metric(\nself, mtype, name, value, tags=None, hostname=None, device_name=None, raw=False, flush_first_value=False\n):\n# type: (int, str, float, Sequence[str], str, str, bool, bool) -&gt; None\nif value is None:\n# ignore metric sample\nreturn\nname = self._format_namespace(name, raw)\nif not self.should_send_metric(name):\nreturn\ntags = self._normalize_tags_type(tags or [], device_name, name)\nif hostname is None:\nhostname = ''\nif self.metric_limiter:\nif mtype in ONE_PER_CONTEXT_METRIC_TYPES:\n# Fast path for gauges, rates, monotonic counters, assume one set of tags per call\nif self.metric_limiter.is_reached():\nreturn\nelse:\n# Other metric types have a legit use case for several calls per set of tags, track unique sets of tags\ncontext = self._context_uid(mtype, name, tags, hostname)\nif self.metric_limiter.is_reached(context):\nreturn\ntry:\nvalue = float(value)\nexcept ValueError:\nerr_msg = 'Metric: {} has non float value: {}. Only float values can be submitted as metrics.'.format(\nrepr(name), repr(value)\n)\nif using_stub_aggregator:\nraise ValueError(err_msg)\nself.warning(err_msg)\nreturn\naggregator.submit_metric(self, self.check_id, mtype, name, value, tags, hostname, flush_first_value)\ndef gauge(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a gauge metric.\n        Parameters:\n            name (str):\n                the name of the metric\n            value (float):\n                the value for the metric\n            tags (list[str]):\n                a list of tags to associate with this metric\n            hostname (str):\n                a hostname to associate with this metric. Defaults to the current host.\n            device_name (str):\n                **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n            raw (bool):\n                whether to ignore any defined namespace prefix\n        \"\"\"\nself._submit_metric(\naggregator.GAUGE, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\ndef count(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a raw count metric.\n        Parameters:\n            name (str):\n                the name of the metric\n            value (float):\n                the value for the metric\n            tags (list[str]):\n                a list of tags to associate with this metric\n            hostname (str):\n                a hostname to associate with this metric. Defaults to the current host.\n            device_name (str):\n                **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n            raw (bool):\n                whether to ignore any defined namespace prefix\n        \"\"\"\nself._submit_metric(\naggregator.COUNT, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\ndef monotonic_count(\nself, name, value, tags=None, hostname=None, device_name=None, raw=False, flush_first_value=False\n):\n# type: (str, float, Sequence[str], str, str, bool, bool) -&gt; None\n\"\"\"Sample an increasing counter metric.\n        Parameters:\n            name (str):\n                the name of the metric\n            value (float):\n                the value for the metric\n            tags (list[str]):\n                a list of tags to associate with this metric\n            hostname (str):\n                a hostname to associate with this metric. Defaults to the current host.\n            device_name (str):\n                **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n            raw (bool):\n                whether to ignore any defined namespace prefix\n            flush_first_value (bool):\n                whether to sample the first value\n        \"\"\"\nself._submit_metric(\naggregator.MONOTONIC_COUNT,\nname,\nvalue,\ntags=tags,\nhostname=hostname,\ndevice_name=device_name,\nraw=raw,\nflush_first_value=flush_first_value,\n)\ndef rate(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a point, with the rate calculated at the end of the check.\n        Parameters:\n            name (str):\n                the name of the metric\n            value (float):\n                the value for the metric\n            tags (list[str]):\n                a list of tags to associate with this metric\n            hostname (str):\n                a hostname to associate with this metric. Defaults to the current host.\n            device_name (str):\n                **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n            raw (bool):\n                whether to ignore any defined namespace prefix\n        \"\"\"\nself._submit_metric(\naggregator.RATE, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\ndef histogram(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a histogram metric.\n        Parameters:\n            name (str):\n                the name of the metric\n            value (float):\n                the value for the metric\n            tags (list[str]):\n                a list of tags to associate with this metric\n            hostname (str):\n                a hostname to associate with this metric. Defaults to the current host.\n            device_name (str):\n                **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n            raw (bool):\n                whether to ignore any defined namespace prefix\n        \"\"\"\nself._submit_metric(\naggregator.HISTOGRAM, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\ndef historate(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a histogram based on rate metrics.\n        Parameters:\n            name (str):\n                the name of the metric\n            value (float):\n                the value for the metric\n            tags (list[str]):\n                a list of tags to associate with this metric\n            hostname (str):\n                a hostname to associate with this metric. Defaults to the current host.\n            device_name (str):\n                **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n            raw (bool):\n                whether to ignore any defined namespace prefix\n        \"\"\"\nself._submit_metric(\naggregator.HISTORATE, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\ndef increment(self, name, value=1, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Increment a counter metric.\n        Parameters:\n            name (str):\n                the name of the metric\n            value (float):\n                the value for the metric\n            tags (list[str]):\n                a list of tags to associate with this metric\n            hostname (str):\n                a hostname to associate with this metric. Defaults to the current host.\n            device_name (str):\n                **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n            raw (bool):\n                whether to ignore any defined namespace prefix\n        \"\"\"\nself._log_deprecation('increment')\nself._submit_metric(\naggregator.COUNTER, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\ndef decrement(self, name, value=-1, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Decrement a counter metric.\n        Parameters:\n            name (str):\n                the name of the metric\n            value (float):\n                the value for the metric\n            tags (list[str]):\n                a list of tags to associate with this metric\n            hostname (str):\n                a hostname to associate with this metric. Defaults to the current host.\n            device_name (str):\n                **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n            raw (bool):\n                whether to ignore any defined namespace prefix\n        \"\"\"\nself._log_deprecation('increment')\nself._submit_metric(\naggregator.COUNTER, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\ndef service_check(self, name, status, tags=None, hostname=None, message=None, raw=False):\n# type: (str, ServiceCheckStatus, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Send the status of a service.\n        Parameters:\n            name (str):\n                the name of the service check\n            status (int):\n                a constant describing the service status\n            tags (list[str]):\n                a list of tags to associate with this service check\n            message (str):\n                additional information or a description of why this status occurred.\n            raw (bool):\n                whether to ignore any defined namespace prefix\n        \"\"\"\ntags = self._normalize_tags_type(tags or [])\nif hostname is None:\nhostname = ''\nif message is None:\nmessage = ''\nelse:\nmessage = to_native_string(message)\nmessage = self.sanitize(message)\naggregator.submit_service_check(\nself, self.check_id, self._format_namespace(name, raw), status, tags, hostname, message\n)\ndef _log_deprecation(self, deprecation_key, *args):\n# type: (str, *str) -&gt; None\n\"\"\"\n        Logs a deprecation notice at most once per AgentCheck instance, for the pre-defined `deprecation_key`\n        \"\"\"\nsent, message = self._deprecations[deprecation_key]\nif sent:\nreturn\nself.warning(message, *args)\nself._deprecations[deprecation_key] = (True, message)\n# TODO: Remove once our checks stop calling it\ndef service_metadata(self, meta_name, value):\n# type: (str, Any) -&gt; None\npass\ndef set_metadata(self, name, value, **options):\n# type: (str, Any, **Any) -&gt; None\n\"\"\"Updates the cached metadata `name` with `value`, which is then sent by the Agent at regular intervals.\n        Parameters:\n            name (str):\n                the name of the metadata\n            value (Any):\n                the value for the metadata. if ``name`` has no transformer defined then the\n                raw ``value`` will be submitted and therefore it must be a ``str``\n            options (Any):\n                keyword arguments to pass to any defined transformer\n        \"\"\"\nself.metadata_manager.submit(name, value, options)\n@staticmethod\ndef is_metadata_collection_enabled():\n# type: () -&gt; bool\nreturn is_affirmative(datadog_agent.get_config('enable_metadata_collection'))\n@classmethod\ndef metadata_entrypoint(cls, method):\n# type: (Callable[..., None]) -&gt; Callable[..., None]\n\"\"\"\n        Skip execution of the decorated method if metadata collection is disabled on the Agent.\n        Usage:\n        ```python\n        class MyCheck(AgentCheck):\n            @AgentCheck.metadata_entrypoint\n            def collect_metadata(self):\n                ...\n        ```\n        \"\"\"\n@functools.wraps(method)\ndef entrypoint(self, *args, **kwargs):\n# type: (AgentCheck, *Any, **Any) -&gt; None\nif not self.is_metadata_collection_enabled():\nreturn\n# NOTE: error handling still at the discretion of the wrapped method.\nmethod(self, *args, **kwargs)\nreturn entrypoint\ndef _persistent_cache_id(self, key):\n# type: (str) -&gt; str\nreturn '{}_{}'.format(self.check_id, key)\ndef read_persistent_cache(self, key):\n# type: (str) -&gt; str\n\"\"\"Returns the value previously stored with `write_persistent_cache` for the same `key`.\n        Parameters:\n            key (str):\n                the key to retrieve\n        \"\"\"\nreturn datadog_agent.read_persistent_cache(self._persistent_cache_id(key))\ndef write_persistent_cache(self, key, value):\n# type: (str, str) -&gt; None\n\"\"\"Stores `value` in a persistent cache for this check instance.\n        The cache is located in a path where the agent is guaranteed to have read &amp; write permissions. Namely in\n            - `%ProgramData%\\\\Datadog\\\\run` on Windows.\n            - `/opt/datadog-agent/run` everywhere else.\n        The cache is persistent between agent restarts but will be rebuilt if the check instance configuration changes.\n        Parameters:\n            key (str):\n                the key to retrieve\n            value (str):\n                the value to store\n        \"\"\"\ndatadog_agent.write_persistent_cache(self._persistent_cache_id(key), value)\ndef set_external_tags(self, external_tags):\n# type: (Sequence[ExternalTagType]) -&gt; None\n# Example of external_tags format\n# [\n#     ('hostname', {'src_name': ['test:t1']}),\n#     ('hostname2', {'src2_name': ['test2:t3']})\n# ]\ntry:\nnew_tags = []\nfor hostname, source_map in external_tags:\nnew_tags.append((to_native_string(hostname), source_map))\nfor src_name, tags in iteritems(source_map):\nsource_map[src_name] = self._normalize_tags_type(tags)\ndatadog_agent.set_external_tags(new_tags)\nexcept IndexError:\nself.log.exception('Unexpected external tags format: %s', external_tags)\nraise\ndef convert_to_underscore_separated(self, name):\n# type: (Union[str, bytes]) -&gt; bytes\n\"\"\"\n        Convert from CamelCase to camel_case\n        And substitute illegal metric characters\n        \"\"\"\nname = ensure_bytes(name)\nmetric_name = self.FIRST_CAP_RE.sub(br'\\1_\\2', name)\nmetric_name = self.ALL_CAP_RE.sub(br'\\1_\\2', metric_name).lower()\nmetric_name = self.METRIC_REPLACEMENT.sub(br'_', metric_name)\nreturn self.DOT_UNDERSCORE_CLEANUP.sub(br'.', metric_name).strip(b'_')\ndef warning(self, warning_message, *args, **kwargs):\n# type: (str, *Any, **Any) -&gt; None\n\"\"\"Log a warning message, display it in the Agent's status page and in-app.\n        Using *args is intended to make warning work like log.warn/debug/info/etc\n        and make it compliant with flake8 logging format linter.\n        Parameters:\n            warning_message (str):\n                the warning message\n            args (Any):\n                format string args used to format the warning message e.g. `warning_message % args`\n            kwargs (Any):\n                not used for now, but added to match Python logger's `warning` method signature\n        \"\"\"\nwarning_message = to_native_string(warning_message)\n# Interpolate message only if args is not empty. Same behavior as python logger:\n# https://github.com/python/cpython/blob/1dbe5373851acb85ba91f0be7b83c69563acd68d/Lib/logging/__init__.py#L368-L369\nif args:\nwarning_message = warning_message % args\nframe = inspect.currentframe().f_back  # type: ignore\nlineno = frame.f_lineno\n# only log the last part of the filename, not the full path\nfilename = basename(frame.f_code.co_filename)\nself.log.warning(warning_message, extra={'_lineno': lineno, '_filename': filename, '_check_id': self.check_id})\nself.warnings.append(warning_message)\ndef get_warnings(self):\n# type: () -&gt; List[str]\n\"\"\"\n        Return the list of warnings messages to be displayed in the info page\n        \"\"\"\nwarnings = self.warnings\nself.warnings = []\nreturn warnings\ndef get_diagnoses(self):\n# type: () -&gt; str\n\"\"\"\n        Return the list of diagnosis as a JSON encoded string.\n        The agent calls this method to retrieve diagnostics from integrations. This method\n        runs explicit diagnostics if available.\n        \"\"\"\nreturn json.dumps([d._asdict() for d in (self.diagnosis.diagnoses + self.diagnosis.run_explicit())])\ndef _get_requests_proxy(self):\n# type: () -&gt; ProxySettings\n# TODO: Remove with Agent 5\nno_proxy_settings = {'http': None, 'https': None, 'no': []}  # type: ProxySettings\n# First we read the proxy configuration from datadog.conf\nproxies = self.agentConfig.get('proxy', datadog_agent.get_config('proxy'))\nif proxies:\nproxies = proxies.copy()\n# requests compliant dict\nif proxies and 'no_proxy' in proxies:\nproxies['no'] = proxies.pop('no_proxy')\nreturn proxies if proxies else no_proxy_settings\ndef _format_namespace(self, s, raw=False):\n# type: (str, bool) -&gt; str\nif not raw and self.__NAMESPACE__:\nreturn '{}.{}'.format(self.__NAMESPACE__, to_native_string(s))\nreturn to_native_string(s)\ndef normalize(self, metric, prefix=None, fix_case=False):\n# type: (Union[str, bytes], Union[str, bytes], bool) -&gt; str\n\"\"\"\n        Turn a metric into a well-formed metric name\n        prefix.b.c\n        :param metric The metric name to normalize\n        :param prefix A prefix to to add to the normalized name, default None\n        :param fix_case A boolean, indicating whether to make sure that the metric name returned is in \"snake_case\"\n        \"\"\"\nif isinstance(metric, text_type):\nmetric = unicodedata.normalize('NFKD', metric).encode('ascii', 'ignore')\nif fix_case:\nname = self.convert_to_underscore_separated(metric)\nif prefix is not None:\nprefix = self.convert_to_underscore_separated(prefix)\nelse:\nname = self.METRIC_REPLACEMENT.sub(br'_', metric)\nname = self.DOT_UNDERSCORE_CLEANUP.sub(br'.', name).strip(b'_')\nname = self.MULTIPLE_UNDERSCORE_CLEANUP.sub(br'_', name)\nif prefix is not None:\nname = ensure_bytes(prefix) + b\".\" + name\nreturn to_native_string(name)\ndef normalize_tag(self, tag):\n# type: (Union[str, bytes]) -&gt; str\n\"\"\"Normalize tag values.\n        This happens for legacy reasons, when we cleaned up some characters (like '-')\n        which are allowed in tags.\n        \"\"\"\nif isinstance(tag, text_type):\ntag = tag.encode('utf-8', 'ignore')\ntag = self.TAG_REPLACEMENT.sub(br'_', tag)\ntag = self.MULTIPLE_UNDERSCORE_CLEANUP.sub(br'_', tag)\ntag = self.DOT_UNDERSCORE_CLEANUP.sub(br'.', tag).strip(b'_')\nreturn to_native_string(tag)\ndef check(self, instance):\n# type: (InstanceType) -&gt; None\nraise NotImplementedError\ndef cancel(self):\n# type: () -&gt; None\n\"\"\"\n        This method is called when the check in unscheduled by the agent. This\n        is SIGNAL that the check is being unscheduled and can be called while\n        the check is running. It's up to the python implementation to make sure\n        cancel is thread safe and won't block.\n        \"\"\"\npass\ndef run(self):\n# type: () -&gt; str\ntry:\nself.diagnosis.clear()\n# Ignore check initializations if running in a separate process\nif is_affirmative(self.instance.get('process_isolation', self.init_config.get('process_isolation', False))):\nfrom ..utils.replay.execute import run_with_isolation\nrun_with_isolation(self, aggregator, datadog_agent)\nelse:\nwhile self.check_initializations:\ninitialization = self.check_initializations.popleft()\ntry:\ninitialization()\nexcept Exception:\nself.check_initializations.appendleft(initialization)\nraise\ninstance = copy.deepcopy(self.instances[0])\nif 'set_breakpoint' in self.init_config:\nfrom ..utils.agent.debug import enter_pdb\nenter_pdb(self.check, line=self.init_config['set_breakpoint'], args=(instance,))\nelif 'profile_memory' in self.init_config or (\ndatadog_agent.tracemalloc_enabled() and should_profile_memory(datadog_agent, self.name)\n):\nfrom ..utils.agent.memory import profile_memory\nmetrics = profile_memory(\nself.check, self.init_config, namespaces=self.check_id.split(':', 1), args=(instance,)\n)\ntags = self.get_debug_metric_tags()\ntags.extend(instance.get('__memory_profiling_tags', []))\nfor m in metrics:\nself.gauge(m.name, m.value, tags=tags, raw=True)\nelse:\nself.check(instance)\nerror_report = ''\nexcept Exception as e:\nmessage = self.sanitize(str(e))\ntb = self.sanitize(traceback.format_exc())\nerror_report = json.dumps([{'message': message, 'traceback': tb}])\nfinally:\nif self.metric_limiter:\nif is_affirmative(self.debug_metrics.get('metric_contexts', False)):\ndebug_metrics = self.metric_limiter.get_debug_metrics()\n# Reset so we can actually submit the metrics\nself.metric_limiter.reset()\ntags = self.get_debug_metric_tags()\nfor metric_name, value in debug_metrics:\nself.gauge(metric_name, value, tags=tags, raw=True)\nself.metric_limiter.reset()\nreturn error_report\ndef event(self, event):\n# type: (Event) -&gt; None\n\"\"\"Send an event.\n        An event is a dictionary with the following keys and data types:\n        ```python\n        {\n            \"timestamp\": int,        # the epoch timestamp for the event\n            \"event_type\": str,       # the event name\n            \"api_key\": str,          # the api key for your account\n            \"msg_title\": str,        # the title of the event\n            \"msg_text\": str,         # the text body of the event\n            \"aggregation_key\": str,  # a key to use for aggregating events\n            \"alert_type\": str,       # (optional) one of ('error', 'warning', 'success', 'info'), defaults to 'info'\n            \"source_type_name\": str, # (optional) the source type name\n            \"host\": str,             # (optional) the name of the host\n            \"tags\": list,            # (optional) a list of tags to associate with this event\n            \"priority\": str,         # (optional) specifies the priority of the event (\"normal\" or \"low\")\n        }\n        ```\n        Parameters:\n            event (dict[str, Any]):\n                the event to be sent\n        \"\"\"\n# Enforce types of some fields, considerably facilitates handling in go bindings downstream\nfor key, value in iteritems(event):\nif not isinstance(value, (text_type, binary_type)):\ncontinue\ntry:\nevent[key] = to_native_string(value)  # type: ignore\n# ^ Mypy complains about dynamic key assignment -- arguably for good reason.\n# Ideally we should convert this to a dict literal so that submitted events only include known keys.\nexcept UnicodeError:\nself.log.warning('Encoding error with field `%s`, cannot submit event', key)\nreturn\nif event.get('tags'):\nevent['tags'] = self._normalize_tags_type(event['tags'])\nif event.get('timestamp'):\nevent['timestamp'] = int(event['timestamp'])\nif event.get('aggregation_key'):\nevent['aggregation_key'] = to_native_string(event['aggregation_key'])\nif self.__NAMESPACE__:\nevent.setdefault('source_type_name', self.__NAMESPACE__)\naggregator.submit_event(self, self.check_id, event)\ndef _normalize_tags_type(self, tags, device_name=None, metric_name=None):\n# type: (Sequence[Union[None, str, bytes]], str, str) -&gt; List[str]\n\"\"\"\n        Normalize tags contents and type:\n        - append `device_name` as `device:` tag\n        - normalize tags type\n        - doesn't mutate the passed list, returns a new list\n        \"\"\"\nnormalized_tags = []\nif device_name:\nself._log_deprecation('device_name')\ntry:\nnormalized_tags.append('device:{}'.format(to_native_string(device_name)))\nexcept UnicodeError:\nself.log.warning(\n'Encoding error with device name `%r` for metric `%r`, ignoring tag', device_name, metric_name\n)\nfor tag in tags:\nif tag is None:\ncontinue\ntry:\ntag = to_native_string(tag)\nexcept UnicodeError:\nself.log.warning('Encoding error with tag `%s` for metric `%s`, ignoring tag', tag, metric_name)\ncontinue\nif self.disable_generic_tags:\nnormalized_tags.append(self.degeneralise_tag(tag))\nelse:\nnormalized_tags.append(tag)\nreturn normalized_tags\ndef degeneralise_tag(self, tag):\nsplit_tag = tag.split(':', 1)\nif len(split_tag) &gt; 1:\ntag_name, value = split_tag\nelse:\ntag_name = tag\nvalue = None\nif tag_name in GENERIC_TAGS:\nnew_name = '{}_{}'.format(self.name, tag_name)\nif value:\nreturn '{}:{}'.format(new_name, value)\nelse:\nreturn new_name\nelse:\nreturn tag\ndef get_debug_metric_tags(self):\ntags = ['check_name:{}'.format(self.name), 'check_version:{}'.format(self.check_version)]\ntags.extend(self.instance.get('tags', []))\nreturn tags\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.gauge","title":"<code>gauge(name, value, tags=None, hostname=None, device_name=None, raw=False)</code>","text":"<p>Sample a gauge metric.</p> <p>Parameters:</p> <pre><code>name (str):\n    the name of the metric\nvalue (float):\n    the value for the metric\ntags (list[str]):\n    a list of tags to associate with this metric\nhostname (str):\n    a hostname to associate with this metric. Defaults to the current host.\ndevice_name (str):\n    **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\nraw (bool):\n    whether to ignore any defined namespace prefix\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def gauge(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a gauge metric.\n    Parameters:\n        name (str):\n            the name of the metric\n        value (float):\n            the value for the metric\n        tags (list[str]):\n            a list of tags to associate with this metric\n        hostname (str):\n            a hostname to associate with this metric. Defaults to the current host.\n        device_name (str):\n            **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n        raw (bool):\n            whether to ignore any defined namespace prefix\n    \"\"\"\nself._submit_metric(\naggregator.GAUGE, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.count","title":"<code>count(name, value, tags=None, hostname=None, device_name=None, raw=False)</code>","text":"<p>Sample a raw count metric.</p> <p>Parameters:</p> <pre><code>name (str):\n    the name of the metric\nvalue (float):\n    the value for the metric\ntags (list[str]):\n    a list of tags to associate with this metric\nhostname (str):\n    a hostname to associate with this metric. Defaults to the current host.\ndevice_name (str):\n    **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\nraw (bool):\n    whether to ignore any defined namespace prefix\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def count(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a raw count metric.\n    Parameters:\n        name (str):\n            the name of the metric\n        value (float):\n            the value for the metric\n        tags (list[str]):\n            a list of tags to associate with this metric\n        hostname (str):\n            a hostname to associate with this metric. Defaults to the current host.\n        device_name (str):\n            **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n        raw (bool):\n            whether to ignore any defined namespace prefix\n    \"\"\"\nself._submit_metric(\naggregator.COUNT, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.monotonic_count","title":"<code>monotonic_count(name, value, tags=None, hostname=None, device_name=None, raw=False, flush_first_value=False)</code>","text":"<p>Sample an increasing counter metric.</p> <p>Parameters:</p> <pre><code>name (str):\n    the name of the metric\nvalue (float):\n    the value for the metric\ntags (list[str]):\n    a list of tags to associate with this metric\nhostname (str):\n    a hostname to associate with this metric. Defaults to the current host.\ndevice_name (str):\n    **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\nraw (bool):\n    whether to ignore any defined namespace prefix\nflush_first_value (bool):\n    whether to sample the first value\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def monotonic_count(\nself, name, value, tags=None, hostname=None, device_name=None, raw=False, flush_first_value=False\n):\n# type: (str, float, Sequence[str], str, str, bool, bool) -&gt; None\n\"\"\"Sample an increasing counter metric.\n    Parameters:\n        name (str):\n            the name of the metric\n        value (float):\n            the value for the metric\n        tags (list[str]):\n            a list of tags to associate with this metric\n        hostname (str):\n            a hostname to associate with this metric. Defaults to the current host.\n        device_name (str):\n            **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n        raw (bool):\n            whether to ignore any defined namespace prefix\n        flush_first_value (bool):\n            whether to sample the first value\n    \"\"\"\nself._submit_metric(\naggregator.MONOTONIC_COUNT,\nname,\nvalue,\ntags=tags,\nhostname=hostname,\ndevice_name=device_name,\nraw=raw,\nflush_first_value=flush_first_value,\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.rate","title":"<code>rate(name, value, tags=None, hostname=None, device_name=None, raw=False)</code>","text":"<p>Sample a point, with the rate calculated at the end of the check.</p> <p>Parameters:</p> <pre><code>name (str):\n    the name of the metric\nvalue (float):\n    the value for the metric\ntags (list[str]):\n    a list of tags to associate with this metric\nhostname (str):\n    a hostname to associate with this metric. Defaults to the current host.\ndevice_name (str):\n    **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\nraw (bool):\n    whether to ignore any defined namespace prefix\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def rate(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a point, with the rate calculated at the end of the check.\n    Parameters:\n        name (str):\n            the name of the metric\n        value (float):\n            the value for the metric\n        tags (list[str]):\n            a list of tags to associate with this metric\n        hostname (str):\n            a hostname to associate with this metric. Defaults to the current host.\n        device_name (str):\n            **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n        raw (bool):\n            whether to ignore any defined namespace prefix\n    \"\"\"\nself._submit_metric(\naggregator.RATE, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.histogram","title":"<code>histogram(name, value, tags=None, hostname=None, device_name=None, raw=False)</code>","text":"<p>Sample a histogram metric.</p> <p>Parameters:</p> <pre><code>name (str):\n    the name of the metric\nvalue (float):\n    the value for the metric\ntags (list[str]):\n    a list of tags to associate with this metric\nhostname (str):\n    a hostname to associate with this metric. Defaults to the current host.\ndevice_name (str):\n    **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\nraw (bool):\n    whether to ignore any defined namespace prefix\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def histogram(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a histogram metric.\n    Parameters:\n        name (str):\n            the name of the metric\n        value (float):\n            the value for the metric\n        tags (list[str]):\n            a list of tags to associate with this metric\n        hostname (str):\n            a hostname to associate with this metric. Defaults to the current host.\n        device_name (str):\n            **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n        raw (bool):\n            whether to ignore any defined namespace prefix\n    \"\"\"\nself._submit_metric(\naggregator.HISTOGRAM, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.historate","title":"<code>historate(name, value, tags=None, hostname=None, device_name=None, raw=False)</code>","text":"<p>Sample a histogram based on rate metrics.</p> <p>Parameters:</p> <pre><code>name (str):\n    the name of the metric\nvalue (float):\n    the value for the metric\ntags (list[str]):\n    a list of tags to associate with this metric\nhostname (str):\n    a hostname to associate with this metric. Defaults to the current host.\ndevice_name (str):\n    **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\nraw (bool):\n    whether to ignore any defined namespace prefix\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def historate(self, name, value, tags=None, hostname=None, device_name=None, raw=False):\n# type: (str, float, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Sample a histogram based on rate metrics.\n    Parameters:\n        name (str):\n            the name of the metric\n        value (float):\n            the value for the metric\n        tags (list[str]):\n            a list of tags to associate with this metric\n        hostname (str):\n            a hostname to associate with this metric. Defaults to the current host.\n        device_name (str):\n            **deprecated** add a tag in the form `device:&lt;device_name&gt;` to the `tags` list instead.\n        raw (bool):\n            whether to ignore any defined namespace prefix\n    \"\"\"\nself._submit_metric(\naggregator.HISTORATE, name, value, tags=tags, hostname=hostname, device_name=device_name, raw=raw\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.service_check","title":"<code>service_check(name, status, tags=None, hostname=None, message=None, raw=False)</code>","text":"<p>Send the status of a service.</p> <p>Parameters:</p> <pre><code>name (str):\n    the name of the service check\nstatus (int):\n    a constant describing the service status\ntags (list[str]):\n    a list of tags to associate with this service check\nmessage (str):\n    additional information or a description of why this status occurred.\nraw (bool):\n    whether to ignore any defined namespace prefix\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def service_check(self, name, status, tags=None, hostname=None, message=None, raw=False):\n# type: (str, ServiceCheckStatus, Sequence[str], str, str, bool) -&gt; None\n\"\"\"Send the status of a service.\n    Parameters:\n        name (str):\n            the name of the service check\n        status (int):\n            a constant describing the service status\n        tags (list[str]):\n            a list of tags to associate with this service check\n        message (str):\n            additional information or a description of why this status occurred.\n        raw (bool):\n            whether to ignore any defined namespace prefix\n    \"\"\"\ntags = self._normalize_tags_type(tags or [])\nif hostname is None:\nhostname = ''\nif message is None:\nmessage = ''\nelse:\nmessage = to_native_string(message)\nmessage = self.sanitize(message)\naggregator.submit_service_check(\nself, self.check_id, self._format_namespace(name, raw), status, tags, hostname, message\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.event","title":"<code>event(event)</code>","text":"<p>Send an event.</p> <p>An event is a dictionary with the following keys and data types:</p> <pre><code>{\n    \"timestamp\": int,        # the epoch timestamp for the event\n    \"event_type\": str,       # the event name\n    \"api_key\": str,          # the api key for your account\n    \"msg_title\": str,        # the title of the event\n    \"msg_text\": str,         # the text body of the event\n    \"aggregation_key\": str,  # a key to use for aggregating events\n    \"alert_type\": str,       # (optional) one of ('error', 'warning', 'success', 'info'), defaults to 'info'\n    \"source_type_name\": str, # (optional) the source type name\n    \"host\": str,             # (optional) the name of the host\n    \"tags\": list,            # (optional) a list of tags to associate with this event\n    \"priority\": str,         # (optional) specifies the priority of the event (\"normal\" or \"low\")\n}\n</code></pre> <p>Parameters:</p> <pre><code>event (dict[str, Any]):\n    the event to be sent\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def event(self, event):\n# type: (Event) -&gt; None\n\"\"\"Send an event.\n    An event is a dictionary with the following keys and data types:\n    ```python\n    {\n        \"timestamp\": int,        # the epoch timestamp for the event\n        \"event_type\": str,       # the event name\n        \"api_key\": str,          # the api key for your account\n        \"msg_title\": str,        # the title of the event\n        \"msg_text\": str,         # the text body of the event\n        \"aggregation_key\": str,  # a key to use for aggregating events\n        \"alert_type\": str,       # (optional) one of ('error', 'warning', 'success', 'info'), defaults to 'info'\n        \"source_type_name\": str, # (optional) the source type name\n        \"host\": str,             # (optional) the name of the host\n        \"tags\": list,            # (optional) a list of tags to associate with this event\n        \"priority\": str,         # (optional) specifies the priority of the event (\"normal\" or \"low\")\n    }\n    ```\n    Parameters:\n        event (dict[str, Any]):\n            the event to be sent\n    \"\"\"\n# Enforce types of some fields, considerably facilitates handling in go bindings downstream\nfor key, value in iteritems(event):\nif not isinstance(value, (text_type, binary_type)):\ncontinue\ntry:\nevent[key] = to_native_string(value)  # type: ignore\n# ^ Mypy complains about dynamic key assignment -- arguably for good reason.\n# Ideally we should convert this to a dict literal so that submitted events only include known keys.\nexcept UnicodeError:\nself.log.warning('Encoding error with field `%s`, cannot submit event', key)\nreturn\nif event.get('tags'):\nevent['tags'] = self._normalize_tags_type(event['tags'])\nif event.get('timestamp'):\nevent['timestamp'] = int(event['timestamp'])\nif event.get('aggregation_key'):\nevent['aggregation_key'] = to_native_string(event['aggregation_key'])\nif self.__NAMESPACE__:\nevent.setdefault('source_type_name', self.__NAMESPACE__)\naggregator.submit_event(self, self.check_id, event)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.set_metadata","title":"<code>set_metadata(name, value, **options)</code>","text":"<p>Updates the cached metadata <code>name</code> with <code>value</code>, which is then sent by the Agent at regular intervals.</p> <p>Parameters:</p> <pre><code>name (str):\n    the name of the metadata\nvalue (Any):\n    the value for the metadata. if ``name`` has no transformer defined then the\n    raw ``value`` will be submitted and therefore it must be a ``str``\noptions (Any):\n    keyword arguments to pass to any defined transformer\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def set_metadata(self, name, value, **options):\n# type: (str, Any, **Any) -&gt; None\n\"\"\"Updates the cached metadata `name` with `value`, which is then sent by the Agent at regular intervals.\n    Parameters:\n        name (str):\n            the name of the metadata\n        value (Any):\n            the value for the metadata. if ``name`` has no transformer defined then the\n            raw ``value`` will be submitted and therefore it must be a ``str``\n        options (Any):\n            keyword arguments to pass to any defined transformer\n    \"\"\"\nself.metadata_manager.submit(name, value, options)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.metadata_entrypoint","title":"<code>metadata_entrypoint(method)</code>  <code>classmethod</code>","text":"<p>Skip execution of the decorated method if metadata collection is disabled on the Agent.</p> <p>Usage:</p> <pre><code>class MyCheck(AgentCheck):\n    @AgentCheck.metadata_entrypoint\n    def collect_metadata(self):\n        ...\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>@classmethod\ndef metadata_entrypoint(cls, method):\n# type: (Callable[..., None]) -&gt; Callable[..., None]\n\"\"\"\n    Skip execution of the decorated method if metadata collection is disabled on the Agent.\n    Usage:\n    ```python\n    class MyCheck(AgentCheck):\n        @AgentCheck.metadata_entrypoint\n        def collect_metadata(self):\n            ...\n    ```\n    \"\"\"\n@functools.wraps(method)\ndef entrypoint(self, *args, **kwargs):\n# type: (AgentCheck, *Any, **Any) -&gt; None\nif not self.is_metadata_collection_enabled():\nreturn\n# NOTE: error handling still at the discretion of the wrapped method.\nmethod(self, *args, **kwargs)\nreturn entrypoint\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.read_persistent_cache","title":"<code>read_persistent_cache(key)</code>","text":"<p>Returns the value previously stored with <code>write_persistent_cache</code> for the same <code>key</code>.</p> <p>Parameters:</p> <pre><code>key (str):\n    the key to retrieve\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def read_persistent_cache(self, key):\n# type: (str) -&gt; str\n\"\"\"Returns the value previously stored with `write_persistent_cache` for the same `key`.\n    Parameters:\n        key (str):\n            the key to retrieve\n    \"\"\"\nreturn datadog_agent.read_persistent_cache(self._persistent_cache_id(key))\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.write_persistent_cache","title":"<code>write_persistent_cache(key, value)</code>","text":"<p>Stores <code>value</code> in a persistent cache for this check instance. The cache is located in a path where the agent is guaranteed to have read &amp; write permissions. Namely in     - <code>%ProgramData%\\Datadog\\run</code> on Windows.     - <code>/opt/datadog-agent/run</code> everywhere else. The cache is persistent between agent restarts but will be rebuilt if the check instance configuration changes.</p> <p>Parameters:</p> <pre><code>key (str):\n    the key to retrieve\nvalue (str):\n    the value to store\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def write_persistent_cache(self, key, value):\n# type: (str, str) -&gt; None\n\"\"\"Stores `value` in a persistent cache for this check instance.\n    The cache is located in a path where the agent is guaranteed to have read &amp; write permissions. Namely in\n        - `%ProgramData%\\\\Datadog\\\\run` on Windows.\n        - `/opt/datadog-agent/run` everywhere else.\n    The cache is persistent between agent restarts but will be rebuilt if the check instance configuration changes.\n    Parameters:\n        key (str):\n            the key to retrieve\n        value (str):\n            the value to store\n    \"\"\"\ndatadog_agent.write_persistent_cache(self._persistent_cache_id(key), value)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.warning","title":"<code>warning(warning_message, *args, **kwargs)</code>","text":"<p>Log a warning message, display it in the Agent's status page and in-app.</p> <p>Using *args is intended to make warning work like log.warn/debug/info/etc and make it compliant with flake8 logging format linter.</p> <p>Parameters:</p> <pre><code>warning_message (str):\n    the warning message\nargs (Any):\n    format string args used to format the warning message e.g. `warning_message % args`\nkwargs (Any):\n    not used for now, but added to match Python logger's `warning` method signature\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def warning(self, warning_message, *args, **kwargs):\n# type: (str, *Any, **Any) -&gt; None\n\"\"\"Log a warning message, display it in the Agent's status page and in-app.\n    Using *args is intended to make warning work like log.warn/debug/info/etc\n    and make it compliant with flake8 logging format linter.\n    Parameters:\n        warning_message (str):\n            the warning message\n        args (Any):\n            format string args used to format the warning message e.g. `warning_message % args`\n        kwargs (Any):\n            not used for now, but added to match Python logger's `warning` method signature\n    \"\"\"\nwarning_message = to_native_string(warning_message)\n# Interpolate message only if args is not empty. Same behavior as python logger:\n# https://github.com/python/cpython/blob/1dbe5373851acb85ba91f0be7b83c69563acd68d/Lib/logging/__init__.py#L368-L369\nif args:\nwarning_message = warning_message % args\nframe = inspect.currentframe().f_back  # type: ignore\nlineno = frame.f_lineno\n# only log the last part of the filename, not the full path\nfilename = basename(frame.f_code.co_filename)\nself.log.warning(warning_message, extra={'_lineno': lineno, '_filename': filename, '_check_id': self.check_id})\nself.warnings.append(warning_message)\n</code></pre>"},{"location":"base/api/#stubs","title":"Stubs","text":""},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub","title":"<code>datadog_checks.base.stubs.aggregator.AggregatorStub</code>","text":"<p>This implements the methods defined by the Agent's C bindings which in turn call the Go backend.</p> <p>It also provides utility methods for test assertions.</p> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>class AggregatorStub(object):\n\"\"\"\n    This implements the methods defined by the Agent's\n    [C bindings](https://github.com/DataDog/datadog-agent/blob/master/rtloader/common/builtins/aggregator.c)\n    which in turn call the\n    [Go backend](https://github.com/DataDog/datadog-agent/blob/master/pkg/collector/python/aggregator.go).\n    It also provides utility methods for test assertions.\n    \"\"\"\n# Replicate the Enum we have on the Agent\nMETRIC_ENUM_MAP = OrderedDict(\n(\n('gauge', 0),\n('rate', 1),\n('count', 2),\n('monotonic_count', 3),\n('counter', 4),\n('histogram', 5),\n('historate', 6),\n)\n)\nMETRIC_ENUM_MAP_REV = {v: k for k, v in iteritems(METRIC_ENUM_MAP)}\nGAUGE, RATE, COUNT, MONOTONIC_COUNT, COUNTER, HISTOGRAM, HISTORATE = list(METRIC_ENUM_MAP.values())\nAGGREGATE_TYPES = {COUNT, COUNTER}\nIGNORED_METRICS = {'datadog.agent.profile.memory.check_run_alloc'}\nMETRIC_TYPE_SUBMISSION_TO_BACKEND_MAP = {\n'gauge': 'gauge',\n'rate': 'gauge',\n'count': 'count',\n'monotonic_count': 'count',\n'counter': 'rate',\n'histogram': 'rate',  # Checking .count only, the other are gauges\n'historate': 'rate',  # Checking .count only, the other are gauges\n}\ndef __init__(self):\nself.reset()\n@classmethod\ndef is_aggregate(cls, mtype):\nreturn mtype in cls.AGGREGATE_TYPES\n@classmethod\ndef ignore_metric(cls, name):\nreturn name in cls.IGNORED_METRICS\ndef submit_metric(self, check, check_id, mtype, name, value, tags, hostname, flush_first_value):\ncheck_tag_names(name, tags)\nif not self.ignore_metric(name):\nself._metrics[name].append(MetricStub(name, mtype, value, tags, hostname, None, flush_first_value))\ndef submit_metric_e2e(\nself, check, check_id, mtype, name, value, tags, hostname, device=None, flush_first_value=False\n):\ncheck_tag_names(name, tags)\n# Device is only present in metrics read from the real agent in e2e tests. Normally it is submitted as a tag\nif not self.ignore_metric(name):\nself._metrics[name].append(MetricStub(name, mtype, value, tags, hostname, device, flush_first_value))\ndef submit_service_check(self, check, check_id, name, status, tags, hostname, message):\nif status == ServiceCheck.OK and message:\nraise Exception(\"Expected empty message on OK service check\")\ncheck_tag_names(name, tags)\nself._service_checks[name].append(ServiceCheckStub(check_id, name, status, tags, hostname, message))\ndef submit_event(self, check, check_id, event):\nself._events.append(event)\ndef submit_event_platform_event(self, check, check_id, raw_event, event_type):\nself._event_platform_events[event_type].append(raw_event)\ndef submit_histogram_bucket(\nself,\ncheck,\ncheck_id,\nname,\nvalue,\nlower_bound,\nupper_bound,\nmonotonic,\nhostname,\ntags,\nflush_first_value=False,\n):\ncheck_tag_names(name, tags)\nself._histogram_buckets[name].append(\nHistogramBucketStub(name, value, lower_bound, upper_bound, monotonic, hostname, tags, flush_first_value)\n)\ndef metrics(self, name):\n\"\"\"\n        Return the metrics received under the given name\n        \"\"\"\nreturn [\nMetricStub(\nensure_unicode(stub.name),\nstub.type,\nstub.value,\nnormalize_tags(stub.tags),\nensure_unicode(stub.hostname),\nstub.device,\nstub.flush_first_value,\n)\nfor stub in self._metrics.get(to_native_string(name), [])\n]\ndef service_checks(self, name):\n\"\"\"\n        Return the service checks received under the given name\n        \"\"\"\nreturn [\nServiceCheckStub(\nensure_unicode(stub.check_id),\nensure_unicode(stub.name),\nstub.status,\nnormalize_tags(stub.tags),\nensure_unicode(stub.hostname),\nensure_unicode(stub.message),\n)\nfor stub in self._service_checks.get(to_native_string(name), [])\n]\n@property\ndef events(self):\n\"\"\"\n        Return all events\n        \"\"\"\nreturn self._events\ndef get_event_platform_events(self, event_type, parse_json=True):\n\"\"\"\n        Return all event platform events for the event_type\n        \"\"\"\nreturn [json.loads(e) if parse_json else e for e in self._event_platform_events[event_type]]\ndef histogram_bucket(self, name):\n\"\"\"\n        Return the histogram buckets received under the given name\n        \"\"\"\nreturn [\nHistogramBucketStub(\nensure_unicode(stub.name),\nstub.value,\nstub.lower_bound,\nstub.upper_bound,\nstub.monotonic,\nensure_unicode(stub.hostname),\nnormalize_tags(stub.tags),\nstub.flush_first_value,\n)\nfor stub in self._histogram_buckets.get(to_native_string(name), [])\n]\ndef assert_metric_has_tag(self, metric_name, tag, count=None, at_least=1):\n\"\"\"\n        Assert a metric is tagged with tag\n        \"\"\"\nself._asserted.add(metric_name)\ncandidates = []\ncandidates_with_tag = []\nfor metric in self.metrics(metric_name):\ncandidates.append(metric)\nif tag in metric.tags:\ncandidates_with_tag.append(metric)\nif candidates_with_tag:  # The metric was found with the tag but not enough times\nmsg = \"The metric '{}' with tag '{}' was only found {}/{} times\".format(metric_name, tag, count, at_least)\nelif candidates:\nmsg = (\n\"The metric '{}' was found but not with the tag '{}'.\\n\".format(metric_name, tag)\n+ \"Similar submitted:\\n\"\n+ \"\\n\".join([\"     {}\".format(m) for m in candidates])\n)\nelse:\nexpected_stub = MetricStub(metric_name, type=None, value=None, tags=[tag], hostname=None, device=None)\nmsg = \"Metric '{}' not found\".format(metric_name)\nmsg = \"{}\\n{}\".format(msg, build_similar_elements_msg(expected_stub, self._metrics))\nif count is not None:\nassert len(candidates_with_tag) == count, msg\nelse:\nassert len(candidates_with_tag) &gt;= at_least, msg\n# Potential kwargs: aggregation_key, alert_type, event_type,\n# msg_title, source_type_name\ndef assert_event(self, msg_text, count=None, at_least=1, exact_match=True, tags=None, **kwargs):\ncandidates = []\nfor e in self.events:\nif exact_match and msg_text != e['msg_text'] or msg_text not in e['msg_text']:\ncontinue\nif tags and set(tags) != set(e['tags']):\ncontinue\nfor name, value in iteritems(kwargs):\nif e[name] != value:\nbreak\nelse:\ncandidates.append(e)\nmsg = \"Candidates size assertion for `{}`, count: {}, at_least: {}) failed\".format(msg_text, count, at_least)\nif count is not None:\nassert len(candidates) == count, msg\nelse:\nassert len(candidates) &gt;= at_least, msg\ndef assert_histogram_bucket(\nself,\nname,\nvalue,\nlower_bound,\nupper_bound,\nmonotonic,\nhostname,\ntags,\ncount=None,\nat_least=1,\nflush_first_value=None,\n):\nexpected_tags = normalize_tags(tags, sort=True)\ncandidates = []\nfor bucket in self.histogram_bucket(name):\nif value is not None and value != bucket.value:\ncontinue\nif expected_tags and expected_tags != sorted(bucket.tags):\ncontinue\nif hostname and hostname != bucket.hostname:\ncontinue\nif monotonic != bucket.monotonic:\ncontinue\nif flush_first_value is not None and flush_first_value != bucket.flush_first_value:\ncontinue\ncandidates.append(bucket)\nexpected_bucket = HistogramBucketStub(\nname, value, lower_bound, upper_bound, monotonic, hostname, tags, flush_first_value\n)\nif count is not None:\nmsg = \"Needed exactly {} candidates for '{}', got {}\".format(count, name, len(candidates))\ncondition = len(candidates) == count\nelse:\nmsg = \"Needed at least {} candidates for '{}', got {}\".format(at_least, name, len(candidates))\ncondition = len(candidates) &gt;= at_least\nself._assert(\ncondition=condition, msg=msg, expected_stub=expected_bucket, submitted_elements=self._histogram_buckets\n)\ndef assert_metric(\nself,\nname,\nvalue=None,\ntags=None,\ncount=None,\nat_least=1,\nhostname=None,\nmetric_type=None,\ndevice=None,\nflush_first_value=None,\n):\n\"\"\"\n        Assert a metric was processed by this stub\n        \"\"\"\nself._asserted.add(name)\nexpected_tags = normalize_tags(tags, sort=True)\ncandidates = []\nfor metric in self.metrics(name):\nif value is not None and not self.is_aggregate(metric.type) and value != metric.value:\ncontinue\nif expected_tags and expected_tags != sorted(metric.tags):\ncontinue\nif hostname is not None and hostname != metric.hostname:\ncontinue\nif metric_type is not None and metric_type != metric.type:\ncontinue\nif device is not None and device != metric.device:\ncontinue\nif flush_first_value is not None and flush_first_value != metric.flush_first_value:\ncontinue\ncandidates.append(metric)\nexpected_metric = MetricStub(name, metric_type, value, expected_tags, hostname, device, flush_first_value)\nif value is not None and candidates and all(self.is_aggregate(m.type) for m in candidates):\ngot = sum(m.value for m in candidates)\nmsg = \"Expected count value for '{}': {}, got {}\".format(name, value, got)\ncondition = value == got\nelif count is not None:\nmsg = \"Needed exactly {} candidates for '{}', got {}\".format(count, name, len(candidates))\ncondition = len(candidates) == count\nelse:\nmsg = \"Needed at least {} candidates for '{}', got {}\".format(at_least, name, len(candidates))\ncondition = len(candidates) &gt;= at_least\nself._assert(condition, msg=msg, expected_stub=expected_metric, submitted_elements=self._metrics)\ndef assert_service_check(self, name, status=None, tags=None, count=None, at_least=1, hostname=None, message=None):\n\"\"\"\n        Assert a service check was processed by this stub\n        \"\"\"\ntags = normalize_tags(tags, sort=True)\ncandidates = []\nfor sc in self.service_checks(name):\nif status is not None and status != sc.status:\ncontinue\nif tags and tags != sorted(sc.tags):\ncontinue\nif hostname is not None and hostname != sc.hostname:\ncontinue\nif message is not None and message != sc.message:\ncontinue\ncandidates.append(sc)\nexpected_service_check = ServiceCheckStub(\nNone, name=name, status=status, tags=tags, hostname=hostname, message=message\n)\nif count is not None:\nmsg = \"Needed exactly {} candidates for '{}', got {}\".format(count, name, len(candidates))\ncondition = len(candidates) == count\nelse:\nmsg = \"Needed at least {} candidates for '{}', got {}\".format(at_least, name, len(candidates))\ncondition = len(candidates) &gt;= at_least\nself._assert(\ncondition=condition, msg=msg, expected_stub=expected_service_check, submitted_elements=self._service_checks\n)\n@staticmethod\ndef _assert(condition, msg, expected_stub, submitted_elements):\nnew_msg = msg\nif not condition:  # It's costly to build the message with similar metrics, so it's built only on failure.\nnew_msg = \"{}\\n{}\".format(msg, build_similar_elements_msg(expected_stub, submitted_elements))\nassert condition, new_msg\ndef assert_all_metrics_covered(self):\n# use `condition` to avoid building the `msg` if not needed\ncondition = self.metrics_asserted_pct &gt;= 100.0\nmsg = ''\nif not condition:\nprefix = '\\n\\t- '\nmsg = 'Some metrics are collected but not asserted:'\nmsg += '\\nAsserted Metrics:{}{}'.format(prefix, prefix.join(sorted(self._asserted)))\nmsg += '\\nFound metrics that are not asserted:{}{}'.format(prefix, prefix.join(sorted(self.not_asserted())))\nassert condition, msg\ndef assert_metrics_using_metadata(\nself, metadata_metrics, check_metric_type=True, check_submission_type=False, exclude=None\n):\n\"\"\"\n        Assert metrics using metadata.csv\n        Checking type: By default we are asserting the in-app metric type (`check_submission_type=False`),\n        asserting this type make sense for e2e (metrics collected from agent).\n        For integrations tests, we can check the submission type with `check_submission_type=True`, or\n        use `check_metric_type=False` not to check types.\n        Usage:\n            from datadog_checks.dev.utils import get_metadata_metrics\n            aggregator.assert_metrics_using_metadata(get_metadata_metrics())\n        \"\"\"\nexclude = exclude or []\nerrors = set()\nfor metric_name, metric_stubs in iteritems(self._metrics):\nif metric_name in exclude:\ncontinue\nfor metric_stub in metric_stubs:\nmetric_stub_name = backend_normalize_metric_name(metric_stub.name)\nactual_metric_type = AggregatorStub.METRIC_ENUM_MAP_REV[metric_stub.type]\n# We only check `*.count` metrics for histogram and historate submissions\n# Note: all Openmetrics histogram and summary metrics are actually separately submitted\nif check_submission_type and actual_metric_type in ['histogram', 'historate']:\nmetric_stub_name += '.count'\n# Checking the metric is in `metadata.csv`\nif metric_stub_name not in metadata_metrics:\nerrors.add(\"Expect `{}` to be in metadata.csv.\".format(metric_stub_name))\ncontinue\nexpected_metric_type = metadata_metrics[metric_stub_name]['metric_type']\nif check_submission_type:\n# Integration tests type mapping\nactual_metric_type = AggregatorStub.METRIC_TYPE_SUBMISSION_TO_BACKEND_MAP[actual_metric_type]\nelse:\n# E2E tests\nif actual_metric_type == 'monotonic_count' and expected_metric_type == 'count':\nactual_metric_type = 'count'\nif check_metric_type:\nif expected_metric_type != actual_metric_type:\nerrors.add(\n\"Expect `{}` to have type `{}` but got `{}`.\".format(\nmetric_stub_name, expected_metric_type, actual_metric_type\n)\n)\nassert not errors, \"Metadata assertion errors using metadata.csv:\" + \"\\n\\t- \".join([''] + sorted(errors))\ndef assert_no_duplicate_all(self):\n\"\"\"\n        Assert no duplicate metrics and service checks have been submitted.\n        \"\"\"\nself.assert_no_duplicate_metrics()\nself.assert_no_duplicate_service_checks()\ndef assert_no_duplicate_metrics(self):\n\"\"\"\n        Assert no duplicate metrics have been submitted.\n        Metrics are considered duplicate when all following fields match:\n        - metric name\n        - type (gauge, rate, etc)\n        - tags\n        - hostname\n        \"\"\"\n# metric types that intended to be called multiple times are ignored\nignored_types = [self.COUNT, self.COUNTER]\nmetric_stubs = [m for metrics in self._metrics.values() for m in metrics if m.type not in ignored_types]\ndef stub_to_key_fn(stub):\nreturn stub.name, stub.type, str(sorted(stub.tags)), stub.hostname\nself._assert_no_duplicate_stub('metric', metric_stubs, stub_to_key_fn)\ndef assert_no_duplicate_service_checks(self):\n\"\"\"\n        Assert no duplicate service checks have been submitted.\n        Service checks are considered duplicate when all following fields match:\n            - metric name\n            - status\n            - tags\n            - hostname\n        \"\"\"\nservice_check_stubs = [m for metrics in self._service_checks.values() for m in metrics]\ndef stub_to_key_fn(stub):\nreturn stub.name, stub.status, str(sorted(stub.tags)), stub.hostname\nself._assert_no_duplicate_stub('service_check', service_check_stubs, stub_to_key_fn)\n@staticmethod\ndef _assert_no_duplicate_stub(stub_type, all_metrics, stub_to_key_fn):\nall_contexts = defaultdict(list)\nfor metric in all_metrics:\ncontext = stub_to_key_fn(metric)\nall_contexts[context].append(metric)\ndup_contexts = defaultdict(list)\nfor context, metrics in iteritems(all_contexts):\nif len(metrics) &gt; 1:\ndup_contexts[context] = metrics\nerr_msg_lines = [\"Duplicate {}s found:\".format(stub_type)]\nfor key in sorted(dup_contexts):\ncontexts = dup_contexts[key]\nerr_msg_lines.append('- {}'.format(contexts[0].name))\nfor metric in contexts:\nerr_msg_lines.append('    ' + str(metric))\nassert len(dup_contexts) == 0, \"\\n\".join(err_msg_lines)\ndef reset(self):\n\"\"\"\n        Set the stub to its initial state\n        \"\"\"\nself._metrics = defaultdict(list)\nself._asserted = set()\nself._service_checks = defaultdict(list)\nself._events = []\n# dict[event_type, [events]]\nself._event_platform_events = defaultdict(list)\nself._histogram_buckets = defaultdict(list)\ndef all_metrics_asserted(self):\nassert self.metrics_asserted_pct &gt;= 100.0\ndef not_asserted(self):\npresent_metrics = {ensure_unicode(m) for m in self._metrics}\nreturn present_metrics - set(self._asserted)\ndef assert_metric_has_tag_prefix(self, metric_name, tag_prefix, count=None, at_least=1):\ncandidates = []\nself._asserted.add(metric_name)\nfor metric in self.metrics(metric_name):\ntags = metric.tags\ngtags = [t for t in tags if t.startswith(tag_prefix)]\nif len(gtags) &gt; 0:\ncandidates.append(metric)\nmsg = \"Candidates size assertion for `{}`, count: {}, at_least: {}) failed\".format(metric_name, count, at_least)\nif count is not None:\nassert len(candidates) == count, msg\nelse:\nassert len(candidates) &gt;= at_least, msg\n@property\ndef metrics_asserted_pct(self):\n\"\"\"\n        Return the metrics assertion coverage\n        \"\"\"\nnum_metrics = len(self._metrics)\nnum_asserted = len(self._asserted)\nif num_metrics == 0:\nif num_asserted == 0:\nreturn 100\nelse:\nreturn 0\n# If it there have been assertions with at_least=0 the length of the num_metrics and num_asserted can match\n# even if there are different metrics in each set\nnot_asserted = self.not_asserted()\nreturn (num_metrics - len(not_asserted)) / num_metrics * 100\n@property\ndef metric_names(self):\n\"\"\"\n        Return all the metric names we've seen so far\n        \"\"\"\nreturn [ensure_unicode(name) for name in self._metrics.keys()]\n@property\ndef service_check_names(self):\n\"\"\"\n        Return all the service checks names seen so far\n        \"\"\"\nreturn [ensure_unicode(name) for name in self._service_checks.keys()]\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_metric","title":"<code>assert_metric(name, value=None, tags=None, count=None, at_least=1, hostname=None, metric_type=None, device=None, flush_first_value=None)</code>","text":"<p>Assert a metric was processed by this stub</p> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_metric(\nself,\nname,\nvalue=None,\ntags=None,\ncount=None,\nat_least=1,\nhostname=None,\nmetric_type=None,\ndevice=None,\nflush_first_value=None,\n):\n\"\"\"\n    Assert a metric was processed by this stub\n    \"\"\"\nself._asserted.add(name)\nexpected_tags = normalize_tags(tags, sort=True)\ncandidates = []\nfor metric in self.metrics(name):\nif value is not None and not self.is_aggregate(metric.type) and value != metric.value:\ncontinue\nif expected_tags and expected_tags != sorted(metric.tags):\ncontinue\nif hostname is not None and hostname != metric.hostname:\ncontinue\nif metric_type is not None and metric_type != metric.type:\ncontinue\nif device is not None and device != metric.device:\ncontinue\nif flush_first_value is not None and flush_first_value != metric.flush_first_value:\ncontinue\ncandidates.append(metric)\nexpected_metric = MetricStub(name, metric_type, value, expected_tags, hostname, device, flush_first_value)\nif value is not None and candidates and all(self.is_aggregate(m.type) for m in candidates):\ngot = sum(m.value for m in candidates)\nmsg = \"Expected count value for '{}': {}, got {}\".format(name, value, got)\ncondition = value == got\nelif count is not None:\nmsg = \"Needed exactly {} candidates for '{}', got {}\".format(count, name, len(candidates))\ncondition = len(candidates) == count\nelse:\nmsg = \"Needed at least {} candidates for '{}', got {}\".format(at_least, name, len(candidates))\ncondition = len(candidates) &gt;= at_least\nself._assert(condition, msg=msg, expected_stub=expected_metric, submitted_elements=self._metrics)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_metric_has_tag","title":"<code>assert_metric_has_tag(metric_name, tag, count=None, at_least=1)</code>","text":"<p>Assert a metric is tagged with tag</p> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_metric_has_tag(self, metric_name, tag, count=None, at_least=1):\n\"\"\"\n    Assert a metric is tagged with tag\n    \"\"\"\nself._asserted.add(metric_name)\ncandidates = []\ncandidates_with_tag = []\nfor metric in self.metrics(metric_name):\ncandidates.append(metric)\nif tag in metric.tags:\ncandidates_with_tag.append(metric)\nif candidates_with_tag:  # The metric was found with the tag but not enough times\nmsg = \"The metric '{}' with tag '{}' was only found {}/{} times\".format(metric_name, tag, count, at_least)\nelif candidates:\nmsg = (\n\"The metric '{}' was found but not with the tag '{}'.\\n\".format(metric_name, tag)\n+ \"Similar submitted:\\n\"\n+ \"\\n\".join([\"     {}\".format(m) for m in candidates])\n)\nelse:\nexpected_stub = MetricStub(metric_name, type=None, value=None, tags=[tag], hostname=None, device=None)\nmsg = \"Metric '{}' not found\".format(metric_name)\nmsg = \"{}\\n{}\".format(msg, build_similar_elements_msg(expected_stub, self._metrics))\nif count is not None:\nassert len(candidates_with_tag) == count, msg\nelse:\nassert len(candidates_with_tag) &gt;= at_least, msg\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_metric_has_tag_prefix","title":"<code>assert_metric_has_tag_prefix(metric_name, tag_prefix, count=None, at_least=1)</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_metric_has_tag_prefix(self, metric_name, tag_prefix, count=None, at_least=1):\ncandidates = []\nself._asserted.add(metric_name)\nfor metric in self.metrics(metric_name):\ntags = metric.tags\ngtags = [t for t in tags if t.startswith(tag_prefix)]\nif len(gtags) &gt; 0:\ncandidates.append(metric)\nmsg = \"Candidates size assertion for `{}`, count: {}, at_least: {}) failed\".format(metric_name, count, at_least)\nif count is not None:\nassert len(candidates) == count, msg\nelse:\nassert len(candidates) &gt;= at_least, msg\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_service_check","title":"<code>assert_service_check(name, status=None, tags=None, count=None, at_least=1, hostname=None, message=None)</code>","text":"<p>Assert a service check was processed by this stub</p> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_service_check(self, name, status=None, tags=None, count=None, at_least=1, hostname=None, message=None):\n\"\"\"\n    Assert a service check was processed by this stub\n    \"\"\"\ntags = normalize_tags(tags, sort=True)\ncandidates = []\nfor sc in self.service_checks(name):\nif status is not None and status != sc.status:\ncontinue\nif tags and tags != sorted(sc.tags):\ncontinue\nif hostname is not None and hostname != sc.hostname:\ncontinue\nif message is not None and message != sc.message:\ncontinue\ncandidates.append(sc)\nexpected_service_check = ServiceCheckStub(\nNone, name=name, status=status, tags=tags, hostname=hostname, message=message\n)\nif count is not None:\nmsg = \"Needed exactly {} candidates for '{}', got {}\".format(count, name, len(candidates))\ncondition = len(candidates) == count\nelse:\nmsg = \"Needed at least {} candidates for '{}', got {}\".format(at_least, name, len(candidates))\ncondition = len(candidates) &gt;= at_least\nself._assert(\ncondition=condition, msg=msg, expected_stub=expected_service_check, submitted_elements=self._service_checks\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_event","title":"<code>assert_event(msg_text, count=None, at_least=1, exact_match=True, tags=None, **kwargs)</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_event(self, msg_text, count=None, at_least=1, exact_match=True, tags=None, **kwargs):\ncandidates = []\nfor e in self.events:\nif exact_match and msg_text != e['msg_text'] or msg_text not in e['msg_text']:\ncontinue\nif tags and set(tags) != set(e['tags']):\ncontinue\nfor name, value in iteritems(kwargs):\nif e[name] != value:\nbreak\nelse:\ncandidates.append(e)\nmsg = \"Candidates size assertion for `{}`, count: {}, at_least: {}) failed\".format(msg_text, count, at_least)\nif count is not None:\nassert len(candidates) == count, msg\nelse:\nassert len(candidates) &gt;= at_least, msg\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_histogram_bucket","title":"<code>assert_histogram_bucket(name, value, lower_bound, upper_bound, monotonic, hostname, tags, count=None, at_least=1, flush_first_value=None)</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_histogram_bucket(\nself,\nname,\nvalue,\nlower_bound,\nupper_bound,\nmonotonic,\nhostname,\ntags,\ncount=None,\nat_least=1,\nflush_first_value=None,\n):\nexpected_tags = normalize_tags(tags, sort=True)\ncandidates = []\nfor bucket in self.histogram_bucket(name):\nif value is not None and value != bucket.value:\ncontinue\nif expected_tags and expected_tags != sorted(bucket.tags):\ncontinue\nif hostname and hostname != bucket.hostname:\ncontinue\nif monotonic != bucket.monotonic:\ncontinue\nif flush_first_value is not None and flush_first_value != bucket.flush_first_value:\ncontinue\ncandidates.append(bucket)\nexpected_bucket = HistogramBucketStub(\nname, value, lower_bound, upper_bound, monotonic, hostname, tags, flush_first_value\n)\nif count is not None:\nmsg = \"Needed exactly {} candidates for '{}', got {}\".format(count, name, len(candidates))\ncondition = len(candidates) == count\nelse:\nmsg = \"Needed at least {} candidates for '{}', got {}\".format(at_least, name, len(candidates))\ncondition = len(candidates) &gt;= at_least\nself._assert(\ncondition=condition, msg=msg, expected_stub=expected_bucket, submitted_elements=self._histogram_buckets\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_metrics_using_metadata","title":"<code>assert_metrics_using_metadata(metadata_metrics, check_metric_type=True, check_submission_type=False, exclude=None)</code>","text":"<p>Assert metrics using metadata.csv</p> <p>Checking type: By default we are asserting the in-app metric type (<code>check_submission_type=False</code>), asserting this type make sense for e2e (metrics collected from agent). For integrations tests, we can check the submission type with <code>check_submission_type=True</code>, or use <code>check_metric_type=False</code> not to check types.</p> <p>Usage:</p> <pre><code>from datadog_checks.dev.utils import get_metadata_metrics\naggregator.assert_metrics_using_metadata(get_metadata_metrics())\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_metrics_using_metadata(\nself, metadata_metrics, check_metric_type=True, check_submission_type=False, exclude=None\n):\n\"\"\"\n    Assert metrics using metadata.csv\n    Checking type: By default we are asserting the in-app metric type (`check_submission_type=False`),\n    asserting this type make sense for e2e (metrics collected from agent).\n    For integrations tests, we can check the submission type with `check_submission_type=True`, or\n    use `check_metric_type=False` not to check types.\n    Usage:\n        from datadog_checks.dev.utils import get_metadata_metrics\n        aggregator.assert_metrics_using_metadata(get_metadata_metrics())\n    \"\"\"\nexclude = exclude or []\nerrors = set()\nfor metric_name, metric_stubs in iteritems(self._metrics):\nif metric_name in exclude:\ncontinue\nfor metric_stub in metric_stubs:\nmetric_stub_name = backend_normalize_metric_name(metric_stub.name)\nactual_metric_type = AggregatorStub.METRIC_ENUM_MAP_REV[metric_stub.type]\n# We only check `*.count` metrics for histogram and historate submissions\n# Note: all Openmetrics histogram and summary metrics are actually separately submitted\nif check_submission_type and actual_metric_type in ['histogram', 'historate']:\nmetric_stub_name += '.count'\n# Checking the metric is in `metadata.csv`\nif metric_stub_name not in metadata_metrics:\nerrors.add(\"Expect `{}` to be in metadata.csv.\".format(metric_stub_name))\ncontinue\nexpected_metric_type = metadata_metrics[metric_stub_name]['metric_type']\nif check_submission_type:\n# Integration tests type mapping\nactual_metric_type = AggregatorStub.METRIC_TYPE_SUBMISSION_TO_BACKEND_MAP[actual_metric_type]\nelse:\n# E2E tests\nif actual_metric_type == 'monotonic_count' and expected_metric_type == 'count':\nactual_metric_type = 'count'\nif check_metric_type:\nif expected_metric_type != actual_metric_type:\nerrors.add(\n\"Expect `{}` to have type `{}` but got `{}`.\".format(\nmetric_stub_name, expected_metric_type, actual_metric_type\n)\n)\nassert not errors, \"Metadata assertion errors using metadata.csv:\" + \"\\n\\t- \".join([''] + sorted(errors))\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_all_metrics_covered","title":"<code>assert_all_metrics_covered()</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_all_metrics_covered(self):\n# use `condition` to avoid building the `msg` if not needed\ncondition = self.metrics_asserted_pct &gt;= 100.0\nmsg = ''\nif not condition:\nprefix = '\\n\\t- '\nmsg = 'Some metrics are collected but not asserted:'\nmsg += '\\nAsserted Metrics:{}{}'.format(prefix, prefix.join(sorted(self._asserted)))\nmsg += '\\nFound metrics that are not asserted:{}{}'.format(prefix, prefix.join(sorted(self.not_asserted())))\nassert condition, msg\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_no_duplicate_metrics","title":"<code>assert_no_duplicate_metrics()</code>","text":"<p>Assert no duplicate metrics have been submitted.</p> <p>Metrics are considered duplicate when all following fields match:</p> <ul> <li>metric name</li> <li>type (gauge, rate, etc)</li> <li>tags</li> <li>hostname</li> </ul> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_no_duplicate_metrics(self):\n\"\"\"\n    Assert no duplicate metrics have been submitted.\n    Metrics are considered duplicate when all following fields match:\n    - metric name\n    - type (gauge, rate, etc)\n    - tags\n    - hostname\n    \"\"\"\n# metric types that intended to be called multiple times are ignored\nignored_types = [self.COUNT, self.COUNTER]\nmetric_stubs = [m for metrics in self._metrics.values() for m in metrics if m.type not in ignored_types]\ndef stub_to_key_fn(stub):\nreturn stub.name, stub.type, str(sorted(stub.tags)), stub.hostname\nself._assert_no_duplicate_stub('metric', metric_stubs, stub_to_key_fn)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_no_duplicate_service_checks","title":"<code>assert_no_duplicate_service_checks()</code>","text":"<p>Assert no duplicate service checks have been submitted.</p> Service checks are considered duplicate when all following fields match <ul> <li>metric name</li> <li>status</li> <li>tags</li> <li>hostname</li> </ul> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_no_duplicate_service_checks(self):\n\"\"\"\n    Assert no duplicate service checks have been submitted.\n    Service checks are considered duplicate when all following fields match:\n        - metric name\n        - status\n        - tags\n        - hostname\n    \"\"\"\nservice_check_stubs = [m for metrics in self._service_checks.values() for m in metrics]\ndef stub_to_key_fn(stub):\nreturn stub.name, stub.status, str(sorted(stub.tags)), stub.hostname\nself._assert_no_duplicate_stub('service_check', service_check_stubs, stub_to_key_fn)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_no_duplicate_all","title":"<code>assert_no_duplicate_all()</code>","text":"<p>Assert no duplicate metrics and service checks have been submitted.</p> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def assert_no_duplicate_all(self):\n\"\"\"\n    Assert no duplicate metrics and service checks have been submitted.\n    \"\"\"\nself.assert_no_duplicate_metrics()\nself.assert_no_duplicate_service_checks()\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.all_metrics_asserted","title":"<code>all_metrics_asserted()</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def all_metrics_asserted(self):\nassert self.metrics_asserted_pct &gt;= 100.0\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.reset","title":"<code>reset()</code>","text":"<p>Set the stub to its initial state</p> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/aggregator.py</code> <pre><code>def reset(self):\n\"\"\"\n    Set the stub to its initial state\n    \"\"\"\nself._metrics = defaultdict(list)\nself._asserted = set()\nself._service_checks = defaultdict(list)\nself._events = []\n# dict[event_type, [events]]\nself._event_platform_events = defaultdict(list)\nself._histogram_buckets = defaultdict(list)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub","title":"<code>datadog_checks.base.stubs.datadog_agent.DatadogAgentStub</code>","text":"<p>This implements the methods defined by the Agent's C bindings which in turn call the Go backend.</p> <p>It also provides utility methods for test assertions.</p> Source code in <code>datadog_checks_base/datadog_checks/base/stubs/datadog_agent.py</code> <pre><code>class DatadogAgentStub(object):\n\"\"\"\n    This implements the methods defined by the Agent's\n    [C bindings](https://github.com/DataDog/datadog-agent/blob/master/rtloader/common/builtins/datadog_agent.c)\n    which in turn call the\n    [Go backend](https://github.com/DataDog/datadog-agent/blob/master/pkg/collector/python/datadog_agent.go).\n    It also provides utility methods for test assertions.\n    \"\"\"\ndef __init__(self):\nself._metadata = {}\nself._cache = {}\nself._config = self.get_default_config()\nself._hostname = 'stubbed.hostname'\nself._process_start_time = 0\ndef get_default_config(self):\nreturn {'enable_metadata_collection': True, 'disable_unsafe_yaml': True}\ndef reset(self):\nself._metadata.clear()\nself._cache.clear()\nself._config = self.get_default_config()\nself._process_start_time = 0\ndef assert_metadata(self, check_id, data):\nactual = {}\nfor name in data:\nkey = (check_id, name)\nif key in self._metadata:\nactual[name] = self._metadata[key]\nassert data == actual\ndef assert_metadata_count(self, count):\nmetadata_items = len(self._metadata)\nassert metadata_items == count, 'Expected {} metadata items, found {}. Submitted metadata: {}'.format(\ncount, metadata_items, repr(self._metadata)\n)\ndef get_hostname(self):\nreturn self._hostname\ndef set_hostname(self, hostname):\nself._hostname = hostname\ndef reset_hostname(self):\nself._hostname = 'stubbed.hostname'\ndef get_config(self, config_option):\nreturn self._config.get(config_option, '')\ndef get_version(self):\nreturn '0.0.0'\ndef log(self, *args, **kwargs):\npass\ndef set_check_metadata(self, check_id, name, value):\nself._metadata[(check_id, name)] = value\ndef set_external_tags(self, *args, **kwargs):\npass\ndef tracemalloc_enabled(self, *args, **kwargs):\nreturn False\ndef write_persistent_cache(self, key, value):\nself._cache[key] = value\ndef read_persistent_cache(self, key):\nreturn self._cache.get(key, '')\ndef obfuscate_sql(self, query, options=None):\n# Full obfuscation implementation is in go code.\nif options:\n# Options provided is a JSON string because the Go stub requires it, whereas\n# the python stub does not for things such as testing.\nif json.loads(options).get('return_json_metadata', False):\nreturn json.dumps({'query': re.sub(r'\\s+', ' ', query or '').strip(), 'metadata': {}})\nreturn re.sub(r'\\s+', ' ', query or '').strip()\ndef obfuscate_sql_exec_plan(self, plan, normalize=False):\n# Passthrough stub: obfuscation implementation is in Go code.\nreturn plan\ndef get_process_start_time(self):\nreturn self._process_start_time\ndef set_process_start_time(self, time):\nself._process_start_time = time\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.assert_metadata","title":"<code>assert_metadata(check_id, data)</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/stubs/datadog_agent.py</code> <pre><code>def assert_metadata(self, check_id, data):\nactual = {}\nfor name in data:\nkey = (check_id, name)\nif key in self._metadata:\nactual[name] = self._metadata[key]\nassert data == actual\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.assert_metadata_count","title":"<code>assert_metadata_count(count)</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/stubs/datadog_agent.py</code> <pre><code>def assert_metadata_count(self, count):\nmetadata_items = len(self._metadata)\nassert metadata_items == count, 'Expected {} metadata items, found {}. Submitted metadata: {}'.format(\ncount, metadata_items, repr(self._metadata)\n)\n</code></pre>"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.reset","title":"<code>reset()</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/stubs/datadog_agent.py</code> <pre><code>def reset(self):\nself._metadata.clear()\nself._cache.clear()\nself._config = self.get_default_config()\nself._process_start_time = 0\n</code></pre>"},{"location":"base/basics/","title":"Basics","text":"<p>The AgentCheck base class contains the logic that all Checks inherit.</p> <p>In addition to the integrations inheriting from AgentCheck, other classes that inherit from AgentCheck include:</p> <ul> <li>PDHBaseCheck</li> <li>OpenMetricsBaseCheck</li> <li>KubeLeaderElectionBaseCheck</li> </ul>"},{"location":"base/basics/#getting-started","title":"Getting Started","text":"<p>The Datadog Agent looks for <code>__version__</code> and a subclass of <code>AgentCheck</code> at the root of every Check package.</p> <p>Below is an example of the <code>__init__.py</code> file for a hypothetical <code>Awesome</code> Check:</p> <pre><code>from .__about__ import __version__\nfrom .check import AwesomeCheck\n\n__all__ = ['__version__', 'AwesomeCheck']\n</code></pre> <p>The version is used in the Agent's status output (if no <code>__version__</code> is found, it will default to <code>0.0.0</code>): <pre><code>=========\nCollector\n=========\n\n  Running Checks\n  ============== \n\n    AwesomeCheck (0.0.1)\n    -------------------\n      Instance ID: 1234 [OK]\n      Configuration Source: file:/etc/datadog-agent/conf.d/awesomecheck.d/awesomecheck.yaml\n      Total Runs: 12\n      Metric Samples: Last Run: 242, Total: 2,904\n      Events: Last Run: 0, Total: 0\n      Service Checks: Last Run: 0, Total: 0\n      Average Execution Time : 49ms\n      Last Execution Date : 2020-10-26 19:09:22.000000 UTC\n      Last Successful Execution Date : 2020-10-26 19:09:22.000000 UTC\n\n...\n</code></pre></p>"},{"location":"base/basics/#checks","title":"Checks","text":"<p>AgentCheck contains functions that you use to execute Checks and submit data to Datadog.</p>"},{"location":"base/basics/#metrics","title":"Metrics","text":"<p>This list enumerates what is collected from your system by each integration. For more information on metrics, see the Metric Types documentation. You can find the metrics for each integration in that integration's <code>metadata.csv</code> file. You can also set up custom metrics, so if the integration doesn\u2019t offer a metric out of the box, you can usually add it.</p>"},{"location":"base/basics/#gauge","title":"Gauge","text":"<p>The gauge metric submission type represents a snapshot of events in one time interval. This representative snapshot value is the last value submitted to the Agent during a time interval. A gauge can be used to take a measure of something reporting continuously\u2014like the available disk space or memory used.</p> <p>For more information, see the API documentation</p>"},{"location":"base/basics/#count","title":"Count","text":"<p>The count metric submission type represents the total number of event occurrences in one time interval. A count can be used to track the total number of connections made to a database or the total number of requests to an endpoint. This number of events can increase or decrease over time\u2014it is not monotonically increasing.</p> <p>For more information, see the API documentation.</p>"},{"location":"base/basics/#monotonic-count","title":"Monotonic Count","text":"<p>Similar to Count, Monotonic Count represents the total number of event occurrences in one time interval. However, this value can ONLY increment.</p> <p>For more information, see the API documentation.</p>"},{"location":"base/basics/#rate","title":"Rate","text":"<p>The rate metric submission type represents the total number of event occurrences per second in one time interval. A rate can be used to track how often something is happening\u2014like the frequency of connections made to a database or the flow of requests made to an endpoint.</p> <p>For more information, see the API documentation.</p>"},{"location":"base/basics/#histogram","title":"Histogram","text":"<p>The histogram metric submission type represents the statistical distribution of a set of values calculated Agent-side in one time interval. Datadog\u2019s histogram metric type is an extension of the StatsD timing metric type: the Agent aggregates the values that are sent in a defined time interval and produces different metrics which represent the set of values.</p> <p>For more information, see the API documentation.</p>"},{"location":"base/basics/#historate","title":"Historate","text":"<p>Similar to the histogram metric, the historate represents statistical distribution over one time interval, although this is based on rate metrics.</p> <p>For more information, see the API documentation.</p>"},{"location":"base/basics/#service-checks","title":"Service Checks","text":"<p>Service checks are a type of monitor used to track the uptime status of the service. For more information, see the Service checks guide.</p> <p>For more information, see the API documentation.</p>"},{"location":"base/basics/#events","title":"Events","text":"<p>Events are informational messages about your system that are consumed by the events stream so that you can build monitors on them.</p> <p>For more information, see the API documentation.</p>"},{"location":"base/basics/#namespacing","title":"Namespacing","text":"<p>Within every integration, you can specify the value of <code>__NAMESPACE__</code>:</p> <pre><code>from datadog_checks.base import AgentCheck\n\n\nclass AwesomeCheck(AgentCheck):\n    __NAMESPACE__ = 'awesome'\n\n...\n</code></pre> <p>This is an optional addition, but it makes submissions easier since it prefixes every metric with the <code>__NAMESPACE__</code> automatically. In this case it would append <code>awesome.</code> to each metric submitted to Datadog.</p> <p>If you wish to ignore the namespace for any reason, you can append an optional Boolean <code>raw=True</code> to each submission:</p> <pre><code>self.gauge('test', 1.23, tags=['foo:bar'], raw=True)\n\n...\n</code></pre> <p>You submitted a gauge metric named <code>test</code> with a value of <code>1.23</code> tagged by <code>foo:bar</code> ignoring the namespace.</p>"},{"location":"base/basics/#check-initializations","title":"Check Initializations","text":"<p>In the AgentCheck class, there is a useful property called <code>check_initializations</code>, which you can use to execute functions that are called once before the first check run. You can fill up <code>check_initializations</code> with instructions in the <code>__init__</code> function of an integration. For example, you could use it to parse configuration information before running a check. Listed below is an example with Airflow:</p> <pre><code>class AirflowCheck(AgentCheck):\n    def __init__(self, name, init_config, instances):\n        super(AirflowCheck, self).__init__(name, init_config, instances)\n\n        self._url = self.instance.get('url', '')\n        self._tags = self.instance.get('tags', [])\n\n        # The Agent only makes one attempt to instantiate each AgentCheck so any errors occurring\n        # in `__init__` are logged just once, making it difficult to spot. Therefore,\n        # potential configuration errors are emitted as part of the check run phase.\n        # The configuration is only parsed once if it succeed, otherwise it's retried.\n        self.check_initializations.append(self._parse_config)\n\n...\n</code></pre>"},{"location":"base/databases/","title":"Databases","text":"<p>No matter the database you wish to monitor, the base package provides a standard way to define and collect data from arbitrary queries.</p> <p>The core premise is that you define a function that accepts a query (usually a <code>str</code>) and it returns a sequence of equal length results.</p>"},{"location":"base/databases/#interface","title":"Interface","text":"<p>All the functionality is exposed by the <code>Query</code> and <code>QueryManager</code> classes.</p>"},{"location":"base/databases/#datadog_checks.base.utils.db.query.Query","title":"<code>datadog_checks.base.utils.db.query.Query</code>","text":"<p>This class accepts a single <code>dict</code> argument which is necessary to run the query. The representation is based on our <code>custom_queries</code> format originally designed and implemented in #1528.</p> <p>It is now part of all our database integrations and other products have since adopted this format.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/query.py</code> <pre><code>class Query(object):\n\"\"\"\n    This class accepts a single `dict` argument which is necessary to run the query. The representation\n    is based on our `custom_queries` format originally designed and implemented in !1528.\n    It is now part of all our database integrations and\n    [other](https://cloud.google.com/solutions/sap/docs/sap-hana-monitoring-agent-planning-guide#defining_custom_queries)\n    products have since adopted this format.\n    \"\"\"\ndef __init__(self, query_data):\n# type: (Dict[str, Any]) -&gt; Query\n# Contains the data to fill the rest of the attributes\nself.query_data = deepcopy(query_data or {})  # type: Dict[str, Any]\nself.name = None  # type: str\n# The actual query\nself.query = None  # type: str\n# Contains a mapping of column_name -&gt; column_type, transformer\nself.column_transformers = None  # type: Tuple[Tuple[str, Tuple[str, Transformer]]]\n# These transformers are used to collect extra metrics calculated from the query result\nself.extra_transformers = None  # type: List[Tuple[str, Transformer]]\n# Contains the tags defined in query_data, more tags can be added later from the query result\nself.base_tags = None  # type: List[str]\ndef compile(\nself,\ncolumn_transformers,  # type: Dict[str, TransformerFactory]\nextra_transformers,  # type: Dict[str, TransformerFactory]\n):\n# type: (...) -&gt; None\n\"\"\"\n        This idempotent method will be called by `QueryManager.compile_queries` so you\n        should never need to call it directly.\n        \"\"\"\n# Check for previous compilation\nif self.name is not None:\nreturn\nquery_name = self.query_data.get('name')\nif not query_name:\nraise ValueError('query field `name` is required')\nelif not isinstance(query_name, str):\nraise ValueError('query field `name` must be a string')\nquery = self.query_data.get('query')\nif not query:\nraise ValueError('field `query` for {} is required'.format(query_name))\nelif query_name.startswith('custom query #') and not isinstance(query, str):\nraise ValueError('field `query` for {} must be a string'.format(query_name))\ncolumns = self.query_data.get('columns')\nif not columns:\nraise ValueError('field `columns` for {} is required'.format(query_name))\nelif not isinstance(columns, list):\nraise ValueError('field `columns` for {} must be a list'.format(query_name))\ntags = self.query_data.get('tags', [])\nif tags is not None and not isinstance(tags, list):\nraise ValueError('field `tags` for {} must be a list'.format(query_name))\n# Keep track of all defined names\nsources = {}\ncolumn_data = []\nfor i, column in enumerate(columns, 1):\n# Columns can be ignored via configuration.\nif not column:\ncolumn_data.append((None, None))\ncontinue\nelif not isinstance(column, dict):\nraise ValueError('column #{} of {} is not a mapping'.format(i, query_name))\ncolumn_name = column.get('name')\nif not column_name:\nraise ValueError('field `name` for column #{} of {} is required'.format(i, query_name))\nelif not isinstance(column_name, str):\nraise ValueError('field `name` for column #{} of {} must be a string'.format(i, query_name))\nelif column_name in sources:\nraise ValueError(\n'the name {} of {} was already defined in {} #{}'.format(\ncolumn_name, query_name, sources[column_name]['type'], sources[column_name]['index']\n)\n)\nsources[column_name] = {'type': 'column', 'index': i}\ncolumn_type = column.get('type')\nif not column_type:\nraise ValueError('field `type` for column {} of {} is required'.format(column_name, query_name))\nelif not isinstance(column_type, str):\nraise ValueError('field `type` for column {} of {} must be a string'.format(column_name, query_name))\nelif column_type == 'source':\ncolumn_data.append((column_name, (None, None)))\ncontinue\nelif column_type not in column_transformers:\nraise ValueError('unknown type `{}` for column {} of {}'.format(column_type, column_name, query_name))\nmodifiers = {key: value for key, value in column.items() if key not in ('name', 'type')}\ntry:\ntransformer = column_transformers[column_type](column_transformers, column_name, **modifiers)\nexcept Exception as e:\nerror = 'error compiling type `{}` for column {} of {}: {}'.format(\ncolumn_type, column_name, query_name, e\n)\n# Prepend helpful error text.\n#\n# When an exception is raised in the context of another one, both will be printed. To avoid\n# this we set the context to None. https://www.python.org/dev/peps/pep-0409/\nraise_from(type(e)(error), None)\nelse:\nif column_type in ('tag', 'tag_list', 'tag_not_null'):\ncolumn_data.append((column_name, (column_type, transformer)))\nelse:\n# All these would actually submit data. As that is the default case, we represent it as\n# a reference to None since if we use e.g. `value` it would never be checked anyway.\ncolumn_data.append((column_name, (None, transformer)))\nsubmission_transformers = column_transformers.copy()  # type: Dict[str, Transformer]\nsubmission_transformers.pop('tag')\nsubmission_transformers.pop('tag_list')\nsubmission_transformers.pop('tag_not_null')\nextras = self.query_data.get('extras', [])  # type: List[Dict[str, Any]]\nif not isinstance(extras, list):\nraise ValueError('field `extras` for {} must be a list'.format(query_name))\nextra_data = []  # type: List[Tuple[str, Transformer]]\nfor i, extra in enumerate(extras, 1):\nif not isinstance(extra, dict):\nraise ValueError('extra #{} of {} is not a mapping'.format(i, query_name))\nextra_name = extra.get('name')  # type: str\nif not extra_name:\nraise ValueError('field `name` for extra #{} of {} is required'.format(i, query_name))\nelif not isinstance(extra_name, str):\nraise ValueError('field `name` for extra #{} of {} must be a string'.format(i, query_name))\nelif extra_name in sources:\nraise ValueError(\n'the name {} of {} was already defined in {} #{}'.format(\nextra_name, query_name, sources[extra_name]['type'], sources[extra_name]['index']\n)\n)\nsources[extra_name] = {'type': 'extra', 'index': i}\nextra_type = extra.get('type')  # type: str  # Is the key in a transformers dict\nif not extra_type:\nif 'expression' in extra:\nextra_type = 'expression'\nelse:\nraise ValueError('field `type` for extra {} of {} is required'.format(extra_name, query_name))\nelif not isinstance(extra_type, str):\nraise ValueError('field `type` for extra {} of {} must be a string'.format(extra_name, query_name))\nelif extra_type not in extra_transformers and extra_type not in submission_transformers:\nraise ValueError('unknown type `{}` for extra {} of {}'.format(extra_type, extra_name, query_name))\ntransformer_factory = extra_transformers.get(\nextra_type, submission_transformers.get(extra_type)\n)  # type: TransformerFactory\nextra_source = extra.get('source')\nif extra_type in submission_transformers:\nif not extra_source:\nraise ValueError('field `source` for extra {} of {} is required'.format(extra_name, query_name))\nmodifiers = {key: value for key, value in extra.items() if key not in ('name', 'type', 'source')}\nelse:\nmodifiers = {key: value for key, value in extra.items() if key not in ('name', 'type')}\nmodifiers['sources'] = sources\ntry:\ntransformer = transformer_factory(submission_transformers, extra_name, **modifiers)\nexcept Exception as e:\nerror = 'error compiling type `{}` for extra {} of {}: {}'.format(extra_type, extra_name, query_name, e)\nraise_from(type(e)(error), None)\nelse:\nif extra_type in submission_transformers:\ntransformer = create_extra_transformer(transformer, extra_source)\nextra_data.append((extra_name, transformer))\nself.name = query_name\nself.query = query\nself.column_transformers = tuple(column_data)\nself.extra_transformers = tuple(extra_data)\nself.base_tags = tags\ndel self.query_data\n</code></pre>"},{"location":"base/databases/#datadog_checks.base.utils.db.query.Query.__init__","title":"<code>__init__(query_data)</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/query.py</code> <pre><code>def __init__(self, query_data):\n# type: (Dict[str, Any]) -&gt; Query\n# Contains the data to fill the rest of the attributes\nself.query_data = deepcopy(query_data or {})  # type: Dict[str, Any]\nself.name = None  # type: str\n# The actual query\nself.query = None  # type: str\n# Contains a mapping of column_name -&gt; column_type, transformer\nself.column_transformers = None  # type: Tuple[Tuple[str, Tuple[str, Transformer]]]\n# These transformers are used to collect extra metrics calculated from the query result\nself.extra_transformers = None  # type: List[Tuple[str, Transformer]]\n# Contains the tags defined in query_data, more tags can be added later from the query result\nself.base_tags = None  # type: List[str]\n</code></pre>"},{"location":"base/databases/#datadog_checks.base.utils.db.query.Query.compile","title":"<code>compile(column_transformers, extra_transformers)</code>","text":"<p>This idempotent method will be called by <code>QueryManager.compile_queries</code> so you should never need to call it directly.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/query.py</code> <pre><code>def compile(\nself,\ncolumn_transformers,  # type: Dict[str, TransformerFactory]\nextra_transformers,  # type: Dict[str, TransformerFactory]\n):\n# type: (...) -&gt; None\n\"\"\"\n    This idempotent method will be called by `QueryManager.compile_queries` so you\n    should never need to call it directly.\n    \"\"\"\n# Check for previous compilation\nif self.name is not None:\nreturn\nquery_name = self.query_data.get('name')\nif not query_name:\nraise ValueError('query field `name` is required')\nelif not isinstance(query_name, str):\nraise ValueError('query field `name` must be a string')\nquery = self.query_data.get('query')\nif not query:\nraise ValueError('field `query` for {} is required'.format(query_name))\nelif query_name.startswith('custom query #') and not isinstance(query, str):\nraise ValueError('field `query` for {} must be a string'.format(query_name))\ncolumns = self.query_data.get('columns')\nif not columns:\nraise ValueError('field `columns` for {} is required'.format(query_name))\nelif not isinstance(columns, list):\nraise ValueError('field `columns` for {} must be a list'.format(query_name))\ntags = self.query_data.get('tags', [])\nif tags is not None and not isinstance(tags, list):\nraise ValueError('field `tags` for {} must be a list'.format(query_name))\n# Keep track of all defined names\nsources = {}\ncolumn_data = []\nfor i, column in enumerate(columns, 1):\n# Columns can be ignored via configuration.\nif not column:\ncolumn_data.append((None, None))\ncontinue\nelif not isinstance(column, dict):\nraise ValueError('column #{} of {} is not a mapping'.format(i, query_name))\ncolumn_name = column.get('name')\nif not column_name:\nraise ValueError('field `name` for column #{} of {} is required'.format(i, query_name))\nelif not isinstance(column_name, str):\nraise ValueError('field `name` for column #{} of {} must be a string'.format(i, query_name))\nelif column_name in sources:\nraise ValueError(\n'the name {} of {} was already defined in {} #{}'.format(\ncolumn_name, query_name, sources[column_name]['type'], sources[column_name]['index']\n)\n)\nsources[column_name] = {'type': 'column', 'index': i}\ncolumn_type = column.get('type')\nif not column_type:\nraise ValueError('field `type` for column {} of {} is required'.format(column_name, query_name))\nelif not isinstance(column_type, str):\nraise ValueError('field `type` for column {} of {} must be a string'.format(column_name, query_name))\nelif column_type == 'source':\ncolumn_data.append((column_name, (None, None)))\ncontinue\nelif column_type not in column_transformers:\nraise ValueError('unknown type `{}` for column {} of {}'.format(column_type, column_name, query_name))\nmodifiers = {key: value for key, value in column.items() if key not in ('name', 'type')}\ntry:\ntransformer = column_transformers[column_type](column_transformers, column_name, **modifiers)\nexcept Exception as e:\nerror = 'error compiling type `{}` for column {} of {}: {}'.format(\ncolumn_type, column_name, query_name, e\n)\n# Prepend helpful error text.\n#\n# When an exception is raised in the context of another one, both will be printed. To avoid\n# this we set the context to None. https://www.python.org/dev/peps/pep-0409/\nraise_from(type(e)(error), None)\nelse:\nif column_type in ('tag', 'tag_list', 'tag_not_null'):\ncolumn_data.append((column_name, (column_type, transformer)))\nelse:\n# All these would actually submit data. As that is the default case, we represent it as\n# a reference to None since if we use e.g. `value` it would never be checked anyway.\ncolumn_data.append((column_name, (None, transformer)))\nsubmission_transformers = column_transformers.copy()  # type: Dict[str, Transformer]\nsubmission_transformers.pop('tag')\nsubmission_transformers.pop('tag_list')\nsubmission_transformers.pop('tag_not_null')\nextras = self.query_data.get('extras', [])  # type: List[Dict[str, Any]]\nif not isinstance(extras, list):\nraise ValueError('field `extras` for {} must be a list'.format(query_name))\nextra_data = []  # type: List[Tuple[str, Transformer]]\nfor i, extra in enumerate(extras, 1):\nif not isinstance(extra, dict):\nraise ValueError('extra #{} of {} is not a mapping'.format(i, query_name))\nextra_name = extra.get('name')  # type: str\nif not extra_name:\nraise ValueError('field `name` for extra #{} of {} is required'.format(i, query_name))\nelif not isinstance(extra_name, str):\nraise ValueError('field `name` for extra #{} of {} must be a string'.format(i, query_name))\nelif extra_name in sources:\nraise ValueError(\n'the name {} of {} was already defined in {} #{}'.format(\nextra_name, query_name, sources[extra_name]['type'], sources[extra_name]['index']\n)\n)\nsources[extra_name] = {'type': 'extra', 'index': i}\nextra_type = extra.get('type')  # type: str  # Is the key in a transformers dict\nif not extra_type:\nif 'expression' in extra:\nextra_type = 'expression'\nelse:\nraise ValueError('field `type` for extra {} of {} is required'.format(extra_name, query_name))\nelif not isinstance(extra_type, str):\nraise ValueError('field `type` for extra {} of {} must be a string'.format(extra_name, query_name))\nelif extra_type not in extra_transformers and extra_type not in submission_transformers:\nraise ValueError('unknown type `{}` for extra {} of {}'.format(extra_type, extra_name, query_name))\ntransformer_factory = extra_transformers.get(\nextra_type, submission_transformers.get(extra_type)\n)  # type: TransformerFactory\nextra_source = extra.get('source')\nif extra_type in submission_transformers:\nif not extra_source:\nraise ValueError('field `source` for extra {} of {} is required'.format(extra_name, query_name))\nmodifiers = {key: value for key, value in extra.items() if key not in ('name', 'type', 'source')}\nelse:\nmodifiers = {key: value for key, value in extra.items() if key not in ('name', 'type')}\nmodifiers['sources'] = sources\ntry:\ntransformer = transformer_factory(submission_transformers, extra_name, **modifiers)\nexcept Exception as e:\nerror = 'error compiling type `{}` for extra {} of {}: {}'.format(extra_type, extra_name, query_name, e)\nraise_from(type(e)(error), None)\nelse:\nif extra_type in submission_transformers:\ntransformer = create_extra_transformer(transformer, extra_source)\nextra_data.append((extra_name, transformer))\nself.name = query_name\nself.query = query\nself.column_transformers = tuple(column_data)\nself.extra_transformers = tuple(extra_data)\nself.base_tags = tags\ndel self.query_data\n</code></pre>"},{"location":"base/databases/#datadog_checks.base.utils.db.core.QueryManager","title":"<code>datadog_checks.base.utils.db.core.QueryManager</code>","text":"<p>This class is in charge of running any number of <code>Query</code> instances for a single Check instance.</p> <p>You will most often see it created during Check initialization like this:</p> <pre><code>self._query_manager = QueryManager(\n    self,\n    self.execute_query,\n    queries=[\n        queries.SomeQuery1,\n        queries.SomeQuery2,\n        queries.SomeQuery3,\n        queries.SomeQuery4,\n        queries.SomeQuery5,\n    ],\n    tags=self.instance.get('tags', []),\n    error_handler=self._error_sanitizer,\n)\nself.check_initializations.append(self._query_manager.compile_queries)\n</code></pre> <p>Note: This class is not in charge of opening or closing connections, just running queries.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/core.py</code> <pre><code>class QueryManager(QueryExecutor):\n\"\"\"\n    This class is in charge of running any number of `Query` instances for a single Check instance.\n    You will most often see it created during Check initialization like this:\n    ```python\n    self._query_manager = QueryManager(\n        self,\n        self.execute_query,\n        queries=[\n            queries.SomeQuery1,\n            queries.SomeQuery2,\n            queries.SomeQuery3,\n            queries.SomeQuery4,\n            queries.SomeQuery5,\n        ],\n        tags=self.instance.get('tags', []),\n        error_handler=self._error_sanitizer,\n    )\n    self.check_initializations.append(self._query_manager.compile_queries)\n    ```\n    Note: This class is not in charge of opening or closing connections, just running queries.\n    \"\"\"\ndef __init__(\nself,\ncheck,  # type: AgentCheck\nexecutor,  # type:  QueriesExecutor\nqueries=None,  # type: List[Dict[str, Any]]\ntags=None,  # type: List[str]\nerror_handler=None,  # type: Callable[[str], str]\nhostname=None,  # type: str\n):  # type: (...) -&gt; QueryManager\n\"\"\"\n        - **check** (_AgentCheck_) - an instance of a Check\n        - **executor** (_callable_) - a callable accepting a `str` query as its sole argument and returning\n          a sequence representing either the full result set or an iterator over the result set\n        - **queries** (_List[Dict]_) - a list of queries in dict format\n        - **tags** (_List[str]_) - a list of tags to associate with every submission\n        - **error_handler** (_callable_) - a callable accepting a `str` error as its sole argument and returning\n          a sanitized string, useful for scrubbing potentially sensitive information libraries emit\n        \"\"\"\nsuper(QueryManager, self).__init__(\nexecutor=executor,\nsubmitter=check,\nqueries=queries,\ntags=tags,\nerror_handler=error_handler,\nhostname=hostname,\nlogger=check.log,\n)\nself.check = check  # type: AgentCheck\nonly_custom_queries = is_affirmative(self.check.instance.get('only_custom_queries', False))  # type: bool\ncustom_queries = list(self.check.instance.get('custom_queries', []))  # type: List[str]\nuse_global_custom_queries = self.check.instance.get('use_global_custom_queries', True)  # type: str\n# Handle overrides\nif use_global_custom_queries == 'extend':\ncustom_queries.extend(self.check.init_config.get('global_custom_queries', []))\nelif (\nnot custom_queries\nand 'global_custom_queries' in self.check.init_config\nand is_affirmative(use_global_custom_queries)\n):\ncustom_queries = self.check.init_config.get('global_custom_queries', [])\n# Override statement queries if only running custom queries\nif only_custom_queries:\nself.queries = []\n# Deduplicate\nfor i, custom_query in enumerate(iter_unique(custom_queries), 1):\nquery = Query(custom_query)\nquery.query_data.setdefault('name', 'custom query #{}'.format(i))\nself.queries.append(query)\nif len(self.queries) == 0:\nself.logger.warning('QueryManager initialized with no query')\ndef execute(self, extra_tags=None):\n# This needs to stay here b/c when we construct a QueryManager in a check's __init__\n# there is no check ID at that point\nself.logger = self.check.log\nreturn super(QueryManager, self).execute(extra_tags)\n</code></pre>"},{"location":"base/databases/#datadog_checks.base.utils.db.core.QueryManager.__init__","title":"<code>__init__(check, executor, queries=None, tags=None, error_handler=None, hostname=None)</code>","text":"<ul> <li>check (AgentCheck) - an instance of a Check</li> <li>executor (callable) - a callable accepting a <code>str</code> query as its sole argument and returning   a sequence representing either the full result set or an iterator over the result set</li> <li>queries (List[Dict]) - a list of queries in dict format</li> <li>tags (List[str]) - a list of tags to associate with every submission</li> <li>error_handler (callable) - a callable accepting a <code>str</code> error as its sole argument and returning   a sanitized string, useful for scrubbing potentially sensitive information libraries emit</li> </ul> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/core.py</code> <pre><code>def __init__(\nself,\ncheck,  # type: AgentCheck\nexecutor,  # type:  QueriesExecutor\nqueries=None,  # type: List[Dict[str, Any]]\ntags=None,  # type: List[str]\nerror_handler=None,  # type: Callable[[str], str]\nhostname=None,  # type: str\n):  # type: (...) -&gt; QueryManager\n\"\"\"\n    - **check** (_AgentCheck_) - an instance of a Check\n    - **executor** (_callable_) - a callable accepting a `str` query as its sole argument and returning\n      a sequence representing either the full result set or an iterator over the result set\n    - **queries** (_List[Dict]_) - a list of queries in dict format\n    - **tags** (_List[str]_) - a list of tags to associate with every submission\n    - **error_handler** (_callable_) - a callable accepting a `str` error as its sole argument and returning\n      a sanitized string, useful for scrubbing potentially sensitive information libraries emit\n    \"\"\"\nsuper(QueryManager, self).__init__(\nexecutor=executor,\nsubmitter=check,\nqueries=queries,\ntags=tags,\nerror_handler=error_handler,\nhostname=hostname,\nlogger=check.log,\n)\nself.check = check  # type: AgentCheck\nonly_custom_queries = is_affirmative(self.check.instance.get('only_custom_queries', False))  # type: bool\ncustom_queries = list(self.check.instance.get('custom_queries', []))  # type: List[str]\nuse_global_custom_queries = self.check.instance.get('use_global_custom_queries', True)  # type: str\n# Handle overrides\nif use_global_custom_queries == 'extend':\ncustom_queries.extend(self.check.init_config.get('global_custom_queries', []))\nelif (\nnot custom_queries\nand 'global_custom_queries' in self.check.init_config\nand is_affirmative(use_global_custom_queries)\n):\ncustom_queries = self.check.init_config.get('global_custom_queries', [])\n# Override statement queries if only running custom queries\nif only_custom_queries:\nself.queries = []\n# Deduplicate\nfor i, custom_query in enumerate(iter_unique(custom_queries), 1):\nquery = Query(custom_query)\nquery.query_data.setdefault('name', 'custom query #{}'.format(i))\nself.queries.append(query)\nif len(self.queries) == 0:\nself.logger.warning('QueryManager initialized with no query')\n</code></pre>"},{"location":"base/databases/#datadog_checks.base.utils.db.core.QueryManager.execute","title":"<code>execute(extra_tags=None)</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/core.py</code> <pre><code>def execute(self, extra_tags=None):\n# This needs to stay here b/c when we construct a QueryManager in a check's __init__\n# there is no check ID at that point\nself.logger = self.check.log\nreturn super(QueryManager, self).execute(extra_tags)\n</code></pre>"},{"location":"base/databases/#transformers","title":"Transformers","text":""},{"location":"base/databases/#column","title":"Column","text":""},{"location":"base/databases/#match","title":"match","text":"<p>This is used for querying unstructured data.</p> <p>For example, say you want to collect the fields named <code>foo</code> and <code>bar</code>. Typically, they would be stored like:</p> foo bar 4 2 <p>and would be queried like:</p> <pre><code>SELECT foo, bar FROM ...\n</code></pre> <p>Often, you will instead find data stored in the following format:</p> metric value foo 4 bar 2 <p>and would be queried like:</p> <pre><code>SELECT metric, value FROM ...\n</code></pre> <p>In this case, the <code>metric</code> column stores the name with which to match on and its <code>value</code> is stored in a separate column.</p> <p>The required <code>items</code> modifier is a mapping of matched names to column data values. Consider the values to be exactly the same as the entries in the <code>columns</code> top level field. You must also define a <code>source</code> modifier either for this transformer itself or in the values of <code>items</code> (which will take precedence). The source will be treated as the value of the match.</p> <p>Say this is your configuration:</p> <pre><code>query: SELECT source1, source2, metric FROM TABLE\ncolumns:\n- name: value1\ntype: source\n- name: value2\ntype: source\n- name: metric_name\ntype: match\nsource: value1\nitems:\nfoo:\nname: test.foo\ntype: gauge\nsource: value2\nbar:\nname: test.bar\ntype: monotonic_gauge\n</code></pre> <p>and the result set is:</p> source1 source2 metric 1 2 foo 3 4 baz 5 6 bar <p>Here's what would be submitted:</p> <ul> <li><code>foo</code> - <code>test.foo</code> as a <code>gauge</code> with a value of <code>2</code></li> <li><code>bar</code> - <code>test.bar.total</code> as a <code>gauge</code> and <code>test.bar.count</code> as a <code>monotonic_count</code>, both with a value of <code>5</code></li> <li><code>baz</code> - nothing since it was not defined as a match item</li> </ul> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/transform.py</code> <pre><code>def get_match(transformers, column_name, **modifiers):\n# type: (Dict[str, Transformer], str, Any) -&gt; Transformer\n\"\"\"\n    This is used for querying unstructured data.\n    For example, say you want to collect the fields named `foo` and `bar`. Typically, they would be stored like:\n    | foo | bar |\n    | --- | --- |\n    | 4   | 2   |\n    and would be queried like:\n    ```sql\n    SELECT foo, bar FROM ...\n    ```\n    Often, you will instead find data stored in the following format:\n    | metric | value |\n    | ------ | ----- |\n    | foo    | 4     |\n    | bar    | 2     |\n    and would be queried like:\n    ```sql\n    SELECT metric, value FROM ...\n    ```\n    In this case, the `metric` column stores the name with which to match on and its `value` is\n    stored in a separate column.\n    The required `items` modifier is a mapping of matched names to column data values. Consider the values\n    to be exactly the same as the entries in the `columns` top level field. You must also define a `source`\n    modifier either for this transformer itself or in the values of `items` (which will take precedence).\n    The source will be treated as the value of the match.\n    Say this is your configuration:\n    ```yaml\n    query: SELECT source1, source2, metric FROM TABLE\n    columns:\n      - name: value1\n        type: source\n      - name: value2\n        type: source\n      - name: metric_name\n        type: match\n        source: value1\n        items:\n          foo:\n            name: test.foo\n            type: gauge\n            source: value2\n          bar:\n            name: test.bar\n            type: monotonic_gauge\n    ```\n    and the result set is:\n    | source1 | source2 | metric |\n    | ------- | ------- | ------ |\n    | 1       | 2       | foo    |\n    | 3       | 4       | baz    |\n    | 5       | 6       | bar    |\n    Here's what would be submitted:\n    - `foo` - `test.foo` as a `gauge` with a value of `2`\n    - `bar` - `test.bar.total` as a `gauge` and `test.bar.count` as a `monotonic_count`, both with a value of `5`\n    - `baz` - nothing since it was not defined as a match item\n    \"\"\"\n# Do work in a separate function to avoid having to `del` a bunch of variables\ncompiled_items = _compile_match_items(transformers, modifiers)  # type: Dict[str, Tuple[str, Transformer]]\ndef match(sources, value, **kwargs):\n# type: (Dict[str, Any], str, Dict[str, Any]) -&gt; None\nif value in compiled_items:\nsource, transformer = compiled_items[value]  # type: str, Transformer\ntransformer(sources, sources[source], **kwargs)\nreturn match\n</code></pre>"},{"location":"base/databases/#temporal_percent","title":"temporal_percent","text":"<p>Send the result as percentage of time since the last check run as a <code>rate</code>.</p> <p>For example, say the result is a forever increasing counter representing the total time spent pausing for garbage collection since start up. That number by itself is quite useless, but as a percentage of time spent pausing since the previous collection interval it becomes a useful metric.</p> <p>There is one required parameter called <code>scale</code> that indicates what unit of time the result should be considered. Valid values are:</p> <ul> <li><code>second</code></li> <li><code>millisecond</code></li> <li><code>microsecond</code></li> <li><code>nanosecond</code></li> </ul> <p>You may also define the unit as an integer number of parts compared to seconds e.g. <code>millisecond</code> is equivalent to <code>1000</code>.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/transform.py</code> <pre><code>def get_temporal_percent(transformers, column_name, **modifiers):\n# type: (Dict[str, Transformer], str, Any) -&gt; Transformer\n\"\"\"\n    Send the result as percentage of time since the last check run as a `rate`.\n    For example, say the result is a forever increasing counter representing the total time spent pausing for\n    garbage collection since start up. That number by itself is quite useless, but as a percentage of time spent\n    pausing since the previous collection interval it becomes a useful metric.\n    There is one required parameter called `scale` that indicates what unit of time the result should be considered.\n    Valid values are:\n    - `second`\n    - `millisecond`\n    - `microsecond`\n    - `nanosecond`\n    You may also define the unit as an integer number of parts compared to seconds e.g. `millisecond` is\n    equivalent to `1000`.\n    \"\"\"\nscale = modifiers.pop('scale', None)\nif scale is None:\nraise ValueError('the `scale` parameter is required')\nif isinstance(scale, str):\nscale = constants.TIME_UNITS.get(scale.lower())\nif scale is None:\nraise ValueError(\n'the `scale` parameter must be one of: {}'.format(' | '.join(sorted(constants.TIME_UNITS)))\n)\nelif not isinstance(scale, int):\nraise ValueError(\n'the `scale` parameter must be an integer representing parts of a second e.g. 1000 for millisecond'\n)\nrate = transformers['rate'](transformers, column_name, **modifiers)  # type: Callable\ndef temporal_percent(_, value, **kwargs):\n# type: (List, str, Dict[str, Any]) -&gt; None\nrate(_, total_time_to_temporal_percent(float(value), scale=scale), **kwargs)\nreturn temporal_percent\n</code></pre>"},{"location":"base/databases/#time_elapsed","title":"time_elapsed","text":"<p>Send the number of seconds elapsed from a time in the past as a <code>gauge</code>.</p> <p>For example, if the result is an instance of datetime.datetime representing 5 seconds ago, then this would submit with a value of <code>5</code>.</p> <p>The optional modifier <code>format</code> indicates what format the result is in. By default it is <code>native</code>, assuming the underlying library provides timestamps as <code>datetime</code> objects.</p> <p>If the value is a UNIX timestamp you can set the <code>format</code> modifier to <code>unix_time</code>.</p> <p>If the value is a string representation of a date, you must provide the expected timestamp format using the supported codes.</p> <p>Example:</p> <pre><code>columns:\n- name: time_since_x\ntype: time_elapsed\nformat: native  # default value and can be omitted\n- name: time_since_y\ntype: time_elapsed\nformat: unix_time\n- name: time_since_z\ntype: time_elapsed\nformat: \"%d/%m/%Y %H:%M:%S\"\n</code></pre> <p>Note</p> <p>The code <code>%z</code> (lower case) is not supported on Windows.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/transform.py</code> <pre><code>def get_time_elapsed(transformers, column_name, **modifiers):\n# type: (Dict[str, Transformer], str, Any) -&gt; Transformer\n\"\"\"\n    Send the number of seconds elapsed from a time in the past as a `gauge`.\n    For example, if the result is an instance of\n    [datetime.datetime](https://docs.python.org/3/library/datetime.html#datetime.datetime) representing 5 seconds ago,\n    then this would submit with a value of `5`.\n    The optional modifier `format` indicates what format the result is in. By default it is `native`, assuming the\n    underlying library provides timestamps as `datetime` objects.\n    If the value is a UNIX timestamp you can set the `format` modifier to `unix_time`.\n    If the value is a string representation of a date, you must provide the expected timestamp format using the\n    [supported codes](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes).\n    Example:\n    ```yaml\n    columns:\n      - name: time_since_x\n        type: time_elapsed\n        format: native  # default value and can be omitted\n      - name: time_since_y\n        type: time_elapsed\n        format: unix_time\n      - name: time_since_z\n        type: time_elapsed\n        format: \"%d/%m/%Y %H:%M:%S\"\n    ```\n    !!! note\n        The code `%z` (lower case) is not supported on Windows.\n    \"\"\"\ntime_format = modifiers.pop('format', 'native')\nif not isinstance(time_format, str):\nraise ValueError('the `format` parameter must be a string')\ngauge = transformers['gauge'](transformers, column_name, **modifiers)\nif time_format == 'native':\ndef time_elapsed(_, value, **kwargs):\n# type: (List, str, Dict[str, Any]) -&gt; None\nvalue = ensure_aware_datetime(value)\ngauge(_, (datetime.now(value.tzinfo) - value).total_seconds(), **kwargs)\nelif time_format == 'unix_time':\ndef time_elapsed(_, value, **kwargs):\ngauge(_, time.time() - value, **kwargs)\nelse:\ndef time_elapsed(_, value, **kwargs):\n# type: (List, str, Dict[str, Any]) -&gt; None\nvalue = ensure_aware_datetime(datetime.strptime(value, time_format))\ngauge(_, (datetime.now(value.tzinfo) - value).total_seconds(), **kwargs)\nreturn time_elapsed\n</code></pre>"},{"location":"base/databases/#monotonic_gauge","title":"monotonic_gauge","text":"<p>Send the result as both a <code>gauge</code> suffixed by <code>.total</code> and a <code>monotonic_count</code> suffixed by <code>.count</code>.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/transform.py</code> <pre><code>def get_monotonic_gauge(transformers, column_name, **modifiers):\n# type: (Dict[str, Transformer], str, Any) -&gt; Transformer\n\"\"\"\n    Send the result as both a `gauge` suffixed by `.total` and a `monotonic_count` suffixed by `.count`.\n    \"\"\"\ngauge = transformers['gauge'](transformers, '{}.total'.format(column_name), **modifiers)  # type: Callable\nmonotonic_count = transformers['monotonic_count'](\ntransformers, '{}.count'.format(column_name), **modifiers\n)  # type: Callable\ndef monotonic_gauge(_, value, **kwargs):\n# type: (List, str, Dict[str, Any]) -&gt; None\ngauge(_, value, **kwargs)\nmonotonic_count(_, value, **kwargs)\nreturn monotonic_gauge\n</code></pre>"},{"location":"base/databases/#service_check","title":"service_check","text":"<p>Submit a service check.</p> <p>The required modifier <code>status_map</code> is a mapping of values to statuses. Valid statuses include:</p> <ul> <li><code>OK</code></li> <li><code>WARNING</code></li> <li><code>CRITICAL</code></li> <li><code>UNKNOWN</code></li> </ul> <p>Any encountered values that are not defined will be sent as <code>UNKNOWN</code>.</p> <p>In addition, a <code>message</code> modifier can be passed which can contain placeholders (based on Python's str.format) for other column names from the same query to add a message dynamically to the service_check.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/transform.py</code> <pre><code>def get_service_check(transformers, column_name, **modifiers):\n# type: (Dict[str, Transformer], str, Any) -&gt; Transformer\n\"\"\"\n    Submit a service check.\n    The required modifier `status_map` is a mapping of values to statuses. Valid statuses include:\n    - `OK`\n    - `WARNING`\n    - `CRITICAL`\n    - `UNKNOWN`\n    Any encountered values that are not defined will be sent as `UNKNOWN`.\n    In addition, a `message` modifier can be passed which can contain placeholders\n    (based on Python's str.format) for other column names from the same query to add a message\n    dynamically to the service_check.\n    \"\"\"\n# Do work in a separate function to avoid having to `del` a bunch of variables\nstatus_map = _compile_service_check_statuses(modifiers)\nmessage_field = modifiers.pop('message', None)\nservice_check_method = transformers['__service_check'](transformers, column_name, **modifiers)  # type: Callable\ndef service_check(sources, value, **kwargs):\n# type: (List, str, Dict[str, Any]) -&gt; None\ncheck_status = status_map.get(value, ServiceCheck.UNKNOWN)\nif not message_field or check_status == ServiceCheck.OK:\nmessage = None\nelse:\nmessage = message_field.format(**sources)\nservice_check_method(sources, check_status, message=message, **kwargs)\nreturn service_check\n</code></pre>"},{"location":"base/databases/#tag","title":"tag","text":"<p>Convert a column to a tag that will be used in every subsequent submission.</p> <p>For example, if you named the column <code>env</code> and the column returned the value <code>prod1</code>, all submissions from that row will be tagged by <code>env:prod1</code>.</p> <p>This also accepts an optional modifier called <code>boolean</code> that when set to <code>true</code> will transform the result to the string <code>true</code> or <code>false</code>. So for example if you named the column <code>alive</code> and the result was the number <code>0</code> the tag will be <code>alive:false</code>.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/transform.py</code> <pre><code>def get_tag(transformers, column_name, **modifiers):\n# type: (Dict[str, Transformer], str, Any) -&gt; Transformer\n\"\"\"\n    Convert a column to a tag that will be used in every subsequent submission.\n    For example, if you named the column `env` and the column returned the value `prod1`, all submissions\n    from that row will be tagged by `env:prod1`.\n    This also accepts an optional modifier called `boolean` that when set to `true` will transform the result\n    to the string `true` or `false`. So for example if you named the column `alive` and the result was the\n    number `0` the tag will be `alive:false`.\n    \"\"\"\ntemplate = '{}:{{}}'.format(column_name)\nboolean = is_affirmative(modifiers.pop('boolean', None))\ndef tag(_, value, **kwargs):\n# type: (List, str, Dict[str, Any]) -&gt; str\nif boolean:\nvalue = str(is_affirmative(value)).lower()\nreturn template.format(value)\nreturn tag\n</code></pre>"},{"location":"base/databases/#tag_list","title":"tag_list","text":"<p>Convert a column to a list of tags that will be used in every submission.</p> <p>Tag name is determined by <code>column_name</code>. The column value represents a list of values. It is expected to be either a list of strings, or a comma-separated string.</p> <p>For example, if the column is named <code>server_tag</code> and the column returned the value <code>us,primary</code>, then all submissions for that row will be tagged by <code>server_tag:us</code> and <code>server_tag:primary</code>.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/transform.py</code> <pre><code>def get_tag_list(transformers, column_name, **modifiers):\n# type: (Dict[str, Transformer], str, Any) -&gt; Transformer\n\"\"\"\n    Convert a column to a list of tags that will be used in every submission.\n    Tag name is determined by `column_name`. The column value represents a list of values. It is expected to be either\n    a list of strings, or a comma-separated string.\n    For example, if the column is named `server_tag` and the column returned the value `us,primary`, then all\n    submissions for that row will be tagged by `server_tag:us` and `server_tag:primary`.\n    \"\"\"\ntemplate = '%s:{}' % column_name\ndef tag_list(_, value, **kwargs):\n# type: (List, str, Dict[str, Any]) -&gt; List[str]\nif isinstance(value, str):\nvalue = [v.strip() for v in value.split(',')]\nreturn [template.format(v) for v in value]\nreturn tag_list\n</code></pre>"},{"location":"base/databases/#extra","title":"Extra","text":"<p>Every column transformer (except <code>tag</code>) is supported at this level, the only difference being one must set a <code>source</code> to retrieve the desired value.</p> <p>So for example here:</p> <pre><code>columns:\n- name: foo.bar\ntype: rate\nextras:\n- name: foo.current\ntype: gauge\nsource: foo.bar\n</code></pre> <p>the metric <code>foo.current</code> will be sent as a gauge with the value of <code>foo.bar</code>.</p>"},{"location":"base/databases/#percent","title":"percent","text":"<p>Send a percentage based on 2 sources as a <code>gauge</code>.</p> <p>The required modifiers are <code>part</code> and <code>total</code>.</p> <p>For example, if you have this configuration:</p> <pre><code>columns:\n- name: disk.total\ntype: gauge\n- name: disk.used\ntype: gauge\nextras:\n- name: disk.utilized\ntype: percent\npart: disk.used\ntotal: disk.total\n</code></pre> <p>then the extra metric <code>disk.utilized</code> would be sent as a <code>gauge</code> calculated as <code>disk.used / disk.total * 100</code>.</p> <p>If the source of <code>total</code> is <code>0</code>, then the submitted value will always be sent as <code>0</code> too.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/transform.py</code> <pre><code>def get_percent(transformers, name, **modifiers):\n# type: (Dict[str, Callable], str, Any) -&gt; Transformer\n\"\"\"\n    Send a percentage based on 2 sources as a `gauge`.\n    The required modifiers are `part` and `total`.\n    For example, if you have this configuration:\n    ```yaml\n    columns:\n      - name: disk.total\n        type: gauge\n      - name: disk.used\n        type: gauge\n    extras:\n      - name: disk.utilized\n        type: percent\n        part: disk.used\n        total: disk.total\n    ```\n    then the extra metric `disk.utilized` would be sent as a `gauge` calculated as `disk.used / disk.total * 100`.\n    If the source of `total` is `0`, then the submitted value will always be sent as `0` too.\n    \"\"\"\navailable_sources = modifiers.pop('sources')\npart = modifiers.pop('part', None)\nif part is None:\nraise ValueError('the `part` parameter is required')\nelif not isinstance(part, str):\nraise ValueError('the `part` parameter must be a string')\nelif part not in available_sources:\nraise ValueError('the `part` parameter `{}` is not an available source'.format(part))\ntotal = modifiers.pop('total', None)\nif total is None:\nraise ValueError('the `total` parameter is required')\nelif not isinstance(total, str):\nraise ValueError('the `total` parameter must be a string')\nelif total not in available_sources:\nraise ValueError('the `total` parameter `{}` is not an available source'.format(total))\ndel available_sources\ngauge = transformers['gauge'](transformers, name, **modifiers)\ngauge = create_extra_transformer(gauge)\ndef percent(sources, **kwargs):\ngauge(sources, compute_percent(sources[part], sources[total]), **kwargs)\nreturn percent\n</code></pre>"},{"location":"base/databases/#expression","title":"expression","text":"<p>This allows the evaluation of a limited subset of Python syntax and built-in functions.</p> <pre><code>columns:\n- name: disk.total\ntype: gauge\n- name: disk.used\ntype: gauge\nextras:\n- name: disk.free\nexpression: disk.total - disk.used\nsubmit_type: gauge\n</code></pre> <p>For brevity, if the <code>expression</code> attribute exists and <code>type</code> does not then it is assumed the type is <code>expression</code>. The <code>submit_type</code> can be any transformer and any extra options are passed down to it.</p> <p>The result of every expression is stored, so in lieu of a <code>submit_type</code> the above example could also be written as:</p> <pre><code>columns:\n- name: disk.total\ntype: gauge\n- name: disk.used\ntype: gauge\nextras:\n- name: free\nexpression: disk.total - disk.used\n- name: disk.free\ntype: gauge\nsource: free\n</code></pre> <p>The order matters though, so for example the following will fail:</p> <pre><code>columns:\n- name: disk.total\ntype: gauge\n- name: disk.used\ntype: gauge\nextras:\n- name: disk.free\ntype: gauge\nsource: free\n- name: free\nexpression: disk.total - disk.used\n</code></pre> <p>since the source <code>free</code> does not yet exist.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/db/transform.py</code> <pre><code>def get_expression(transformers, name, **modifiers):\n# type: (Dict[str, Transformer], str, Dict[str, Any]) -&gt; Transformer\n\"\"\"\n    This allows the evaluation of a limited subset of Python syntax and built-in functions.\n    ```yaml\n    columns:\n      - name: disk.total\n        type: gauge\n      - name: disk.used\n        type: gauge\n    extras:\n      - name: disk.free\n        expression: disk.total - disk.used\n        submit_type: gauge\n    ```\n    For brevity, if the `expression` attribute exists and `type` does not then it is assumed the type is\n    `expression`. The `submit_type` can be any transformer and any extra options are passed down to it.\n    The result of every expression is stored, so in lieu of a `submit_type` the above example could also be written as:\n    ```yaml\n    columns:\n      - name: disk.total\n        type: gauge\n      - name: disk.used\n        type: gauge\n    extras:\n      - name: free\n        expression: disk.total - disk.used\n      - name: disk.free\n        type: gauge\n        source: free\n    ```\n    The order matters though, so for example the following will fail:\n    ```yaml\n    columns:\n      - name: disk.total\n        type: gauge\n      - name: disk.used\n        type: gauge\n    extras:\n      - name: disk.free\n        type: gauge\n        source: free\n      - name: free\n        expression: disk.total - disk.used\n    ```\n    since the source `free` does not yet exist.\n    \"\"\"\navailable_sources = modifiers.pop('sources')\nexpression = modifiers.pop('expression', None)\nif expression is None:\nraise ValueError('the `expression` parameter is required')\nelif not isinstance(expression, str):\nraise ValueError('the `expression` parameter must be a string')\nelif not expression:\nraise ValueError('the `expression` parameter must not be empty')\nif not modifiers.pop('verbose', False):\n# Sort the sources in reverse order of length to prevent greedy matching\navailable_sources = sorted(available_sources, key=lambda s: -len(s))\n# Escape special characters, mostly for the possible dots in metric names\navailable_sources = list(map(re.escape, available_sources))\n# Finally, utilize the order by relying on the guarantees provided by the alternation operator\navailable_sources = '|'.join(available_sources)\nexpression = re.sub(\nSOURCE_PATTERN.format(available_sources),\n# Replace by the particular source that matched\nlambda match_obj: 'SOURCES[\"{}\"]'.format(match_obj.group(1)),\nexpression,\n)\nexpression = compile(expression, filename=name, mode='eval')\ndel available_sources\nif 'submit_type' in modifiers:\nif modifiers['submit_type'] not in transformers:\nraise ValueError('unknown submit_type `{}`'.format(modifiers['submit_type']))\nsubmit_method = transformers[modifiers.pop('submit_type')](transformers, name, **modifiers)  # type: Transformer\nsubmit_method = create_extra_transformer(submit_method)  # type: Callable\ndef execute_expression(sources, **kwargs):\n# type: (Dict[str, Any], Dict[str, Any]) -&gt; float\nresult = eval(expression, ALLOWED_GLOBALS, {'SOURCES': sources})  # type: float\nsubmit_method(sources, result, **kwargs)\nreturn result\nelse:\ndef execute_expression(sources, **kwargs):\n# type: (Dict[str, Any], Dict[str, Any]) -&gt; Any\nreturn eval(expression, ALLOWED_GLOBALS, {'SOURCES': sources})\nreturn execute_expression\n</code></pre>"},{"location":"base/http/","title":"HTTP","text":"<p>Whenever you need to make HTTP requests, the base class provides a convenience member that has the same interface as the popular requests library and ensures consistent behavior across all integrations.</p> <p>The wrapper automatically parses and uses configuration from the <code>instance</code>, <code>init_config</code>, and Agent config. Also, this is only done once during initialization and cached to reduce the overhead of every call.</p> <p>For example, to make a GET request you would use:</p> <pre><code>response = self.http.get(url)\n</code></pre> <p>and the wrapper will pass the right things to <code>requests</code>. All methods accept optional keyword arguments like <code>stream</code>, etc.</p> <p>Any method-level option will override configuration. So for example if <code>tls_verify</code> was set to false and you do <code>self.http.get(url, verify=True)</code>, then SSL certificates will be verified on that particular request. You can use the keyword argument <code>persist</code> to override <code>persist_connections</code>.</p> <p>There is also support for non-standard or legacy configurations with the <code>HTTP_CONFIG_REMAPPER</code> class attribute. For example:</p> <pre><code>class MyCheck(AgentCheck):\n    HTTP_CONFIG_REMAPPER = {\n        'disable_ssl_validation': {\n            'name': 'tls_verify',\n            'default': False,\n            'invert': True,\n        },\n        ...\n    }\n    ...\n</code></pre> <p>Support for Unix socket is provided via requests-unixsocket and allows making UDS requests on the <code>unix://</code> scheme (not supported on Windows until Python adds support for <code>AF_UNIX</code>, see ticket):</p> <pre><code>url = 'unix:///var/run/docker.sock'\nresponse = self.http.get(url)\n</code></pre>"},{"location":"base/http/#options","title":"Options","text":"<p>Some options can be set globally in <code>init_config</code> (with <code>instances</code> taking precedence). For complete documentation of every option, see the associated configuration templates for the instances and init_config sections.</p>"},{"location":"base/http/#future","title":"Future","text":"<ul> <li>Support for configuring cookies! Since they can be set globally, per-domain, and even per-path, the configuration may be complex   if not thought out adequately. We'll discuss options for what that might look like. Only our <code>spark</code> and <code>cisco_aci</code> checks   currently set cookies, and that is based on code logic, not configuration.</li> </ul>"},{"location":"base/metadata/","title":"Metadata","text":"<p>Often, you will want to collect mostly unstructured data that doesn't map well to tags, like fine-grained product version information.</p> <p>The base class provides a method that handles such cases. The collected data is captured by flares, displayed on the Agent's status page, and will eventually be queryable in-app.</p>"},{"location":"base/metadata/#interface","title":"Interface","text":"<p>The <code>set_metadata</code> method of the base class updates cached metadata values, which are then sent by the Agent at regular intervals.</p> <p>It requires 2 arguments:</p> <ol> <li><code>name</code> - The name of the metadata.</li> <li><code>value</code> - The value for the metadata. If <code>name</code> has no transformer defined then the raw <code>value</code> will be    submitted and therefore it must be a <code>str</code>.</li> </ol> <p>The method also accepts arbitrary keyword arguments that are forwarded to any defined transformers.</p>"},{"location":"base/metadata/#transformers","title":"Transformers","text":"<p>Custom transformers may be defined via a class level attribute <code>METADATA_TRANSFORMERS</code>.</p> <p>This is a mapping of metadata names to functions. When you call <code>self.set_metadata(name, value, **options)</code>, if <code>name</code> is in this mapping then the corresponding function will be called with the <code>value</code>, and the return value(s) will be collected instead.</p> <p>Transformer functions must satisfy the following signature:</p> <pre><code>def transform_&lt;NAME&gt;(value: Any, options: dict) -&gt; Union[str, Dict[str, str]]:\n</code></pre> <p>If the return type is <code>str</code>, then it will be sent as the value for <code>name</code>. If the return type is a mapping type, then each key will be considered a <code>name</code> and will be sent with its (<code>str</code>) value.</p> <p>For example, the following would collect an entity named <code>square</code> with a value of <code>'25'</code>:</p> <pre><code>from datadog_checks.base import AgentCheck\n\n\nclass AwesomeCheck(AgentCheck):\n    METADATA_TRANSFORMERS = {\n        'square': lambda value, options: str(int(value) ** 2)\n    }\n\n    def check(self, instance):\n        self.set_metadata('square', '5')\n</code></pre> <p>There are a few default transformers, which can be overridden by custom transformers.</p> Source code in <code>datadog_checks_base/datadog_checks/base/utils/metadata/core.py</code> <pre><code>class MetadataManager(object):\n\"\"\"\n    Custom transformers may be defined via a class level attribute `METADATA_TRANSFORMERS`.\n    This is a mapping of metadata names to functions. When you call\n    `#!python self.set_metadata(name, value, **options)`, if `name` is in this mapping then\n    the corresponding function will be called with the `value`, and the return\n    value(s) will be collected instead.\n    Transformer functions must satisfy the following signature:\n    ```python\n    def transform_&lt;NAME&gt;(value: Any, options: dict) -&gt; Union[str, Dict[str, str]]:\n    ```\n    If the return type is `str`, then it will be sent as the value for `name`. If the return type is a mapping type,\n    then each key will be considered a `name` and will be sent with its (`str`) value.\n    For example, the following would collect an entity named `square` with a value of `'25'`:\n    ```python\n    from datadog_checks.base import AgentCheck\n    class AwesomeCheck(AgentCheck):\n        METADATA_TRANSFORMERS = {\n            'square': lambda value, options: str(int(value) ** 2)\n        }\n        def check(self, instance):\n            self.set_metadata('square', '5')\n    ```\n    There are a few default transformers, which can be overridden by custom transformers.\n    \"\"\"\n__slots__ = ('check_id', 'check_name', 'logger', 'metadata_transformers')\ndef __init__(self, check_name, check_id, logger=None, metadata_transformers=None):\nself.check_name = check_name\nself.check_id = check_id\nself.logger = logger or LOGGER\nself.metadata_transformers = {'version': self.transform_version}\nif metadata_transformers:\nself.metadata_transformers.update(metadata_transformers)\ndef submit_raw(self, name, value):\ndatadog_agent.set_check_metadata(self.check_id, to_native_string(name), to_native_string(value))\ndef submit(self, name, value, options):\ntransformer = self.metadata_transformers.get(name)\nif transformer:\ntry:\ntransformed = transformer(value, options)\nexcept Exception as e:\nif is_primitive(value):\nself.logger.debug('Unable to transform `%s` metadata value `%s`: %s', name, value, e)\nelse:\nself.logger.debug('Unable to transform `%s` metadata: %s', name, e)\nreturn\nif isinstance(transformed, str):\nself.submit_raw(name, transformed)\nelse:\nfor transformed_name, transformed_value in iteritems(transformed):\nself.submit_raw(transformed_name, transformed_value)\nelse:\nself.submit_raw(name, value)\ndef transform_version(self, version, options):\n\"\"\"\n        Transforms a version like `1.2.3-rc.4+5` to its constituent parts. In all cases,\n        the metadata names `version.raw` and `version.scheme` will be collected.\n        If a `scheme` is defined then it will be looked up from our known schemes. If no\n        scheme is defined then it will default to `semver`. The supported schemes are:\n        - `regex` - A `pattern` must also be defined. The pattern must be a `str` or a pre-compiled\n          `re.Pattern`. Any matching named subgroups will then be sent as `version.&lt;GROUP_NAME&gt;`. In this case,\n          the check name will be used as the value of `version.scheme` unless `final_scheme` is also set, which\n          will take precedence.\n        - `parts` - A `part_map` must also be defined. Each key in this mapping will be considered\n          a `name` and will be sent with its (`str`) value.\n        - `semver` - This is essentially the same as `regex` with the `pattern` set to the standard regular\n          expression for semantic versioning.\n        Taking the example above, calling `#!python self.set_metadata('version', '1.2.3-rc.4+5')` would produce:\n        | name | value |\n        | --- | --- |\n        | `version.raw` | `1.2.3-rc.4+5` |\n        | `version.scheme` | `semver` |\n        | `version.major` | `1` |\n        | `version.minor` | `2` |\n        | `version.patch` | `3` |\n        | `version.release` | `rc.4` |\n        | `version.build` | `5` |\n        \"\"\"\nscheme, version_parts = parse_version(version, options)\nif scheme == 'regex' or scheme == 'parts':\nscheme = options.get('final_scheme', self.check_name)\ndata = {'version.{}'.format(part_name): part_value for part_name, part_value in iteritems(version_parts)}\ndata['version.raw'] = version\ndata['version.scheme'] = scheme\nreturn data\n</code></pre>"},{"location":"base/metadata/#datadog_checks.base.utils.metadata.core.MetadataManager.transform_version","title":"<code>transform_version(version, options)</code>","text":"<p>Transforms a version like <code>1.2.3-rc.4+5</code> to its constituent parts. In all cases, the metadata names <code>version.raw</code> and <code>version.scheme</code> will be collected.</p> <p>If a <code>scheme</code> is defined then it will be looked up from our known schemes. If no scheme is defined then it will default to <code>semver</code>. The supported schemes are:</p> <ul> <li><code>regex</code> - A <code>pattern</code> must also be defined. The pattern must be a <code>str</code> or a pre-compiled   <code>re.Pattern</code>. Any matching named subgroups will then be sent as <code>version.&lt;GROUP_NAME&gt;</code>. In this case,   the check name will be used as the value of <code>version.scheme</code> unless <code>final_scheme</code> is also set, which   will take precedence.</li> <li><code>parts</code> - A <code>part_map</code> must also be defined. Each key in this mapping will be considered   a <code>name</code> and will be sent with its (<code>str</code>) value.</li> <li><code>semver</code> - This is essentially the same as <code>regex</code> with the <code>pattern</code> set to the standard regular   expression for semantic versioning.</li> </ul> <p>Taking the example above, calling <code>self.set_metadata('version', '1.2.3-rc.4+5')</code> would produce:</p> name value <code>version.raw</code> <code>1.2.3-rc.4+5</code> <code>version.scheme</code> <code>semver</code> <code>version.major</code> <code>1</code> <code>version.minor</code> <code>2</code> <code>version.patch</code> <code>3</code> <code>version.release</code> <code>rc.4</code> <code>version.build</code> <code>5</code> Source code in <code>datadog_checks_base/datadog_checks/base/utils/metadata/core.py</code> <pre><code>def transform_version(self, version, options):\n\"\"\"\n    Transforms a version like `1.2.3-rc.4+5` to its constituent parts. In all cases,\n    the metadata names `version.raw` and `version.scheme` will be collected.\n    If a `scheme` is defined then it will be looked up from our known schemes. If no\n    scheme is defined then it will default to `semver`. The supported schemes are:\n    - `regex` - A `pattern` must also be defined. The pattern must be a `str` or a pre-compiled\n      `re.Pattern`. Any matching named subgroups will then be sent as `version.&lt;GROUP_NAME&gt;`. In this case,\n      the check name will be used as the value of `version.scheme` unless `final_scheme` is also set, which\n      will take precedence.\n    - `parts` - A `part_map` must also be defined. Each key in this mapping will be considered\n      a `name` and will be sent with its (`str`) value.\n    - `semver` - This is essentially the same as `regex` with the `pattern` set to the standard regular\n      expression for semantic versioning.\n    Taking the example above, calling `#!python self.set_metadata('version', '1.2.3-rc.4+5')` would produce:\n    | name | value |\n    | --- | --- |\n    | `version.raw` | `1.2.3-rc.4+5` |\n    | `version.scheme` | `semver` |\n    | `version.major` | `1` |\n    | `version.minor` | `2` |\n    | `version.patch` | `3` |\n    | `version.release` | `rc.4` |\n    | `version.build` | `5` |\n    \"\"\"\nscheme, version_parts = parse_version(version, options)\nif scheme == 'regex' or scheme == 'parts':\nscheme = options.get('final_scheme', self.check_name)\ndata = {'version.{}'.format(part_name): part_value for part_name, part_value in iteritems(version_parts)}\ndata['version.raw'] = version\ndata['version.scheme'] = scheme\nreturn data\n</code></pre>"},{"location":"base/openmetrics/","title":"OpenMetrics","text":"<p>OpenMetrics is used for collecting metrics using the CNCF-backed OpenMetrics format. This version is the default version for all new OpenMetric-checks, and it is compatible with Python 3 only.</p>"},{"location":"base/openmetrics/#interface","title":"Interface","text":""},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.base.OpenMetricsBaseCheckV2","title":"<code>datadog_checks.base.checks.openmetrics.v2.base.OpenMetricsBaseCheckV2</code>","text":"<p>OpenMetricsBaseCheckV2 is an updated class of OpenMetricsBaseCheck to scrape endpoints that emit Prometheus metrics.</p> <p>Minimal example configuration:</p> <pre><code>instances:\n- openmetrics_endpoint: http://example.com/endpoint\nnamespace: \"foobar\"\nmetrics:\n- bar\n- foo\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/base.py</code> <pre><code>class OpenMetricsBaseCheckV2(AgentCheck):\n\"\"\"\n    OpenMetricsBaseCheckV2 is an updated class of OpenMetricsBaseCheck to scrape endpoints that emit Prometheus metrics.\n    Minimal example configuration:\n    ```yaml\n    instances:\n    - openmetrics_endpoint: http://example.com/endpoint\n      namespace: \"foobar\"\n      metrics:\n      - bar\n      - foo\n    ```\n    \"\"\"\nDEFAULT_METRIC_LIMIT = 2000\n# Allow tracing for openmetrics integrations\ndef __init_subclass__(cls, **kwargs):\nsuper().__init_subclass__(**kwargs)\nreturn traced_class(cls)\ndef __init__(self, name, init_config, instances):\n\"\"\"\n        The base class for any OpenMetrics-based integration.\n        Subclasses are expected to override this to add their custom scrapers or transformers.\n        When overriding, make sure to call this (the parent's) __init__ first!\n        \"\"\"\nsuper(OpenMetricsBaseCheckV2, self).__init__(name, init_config, instances)\n# All desired scraper configurations, which subclasses can override as needed\nself.scraper_configs = [self.instance]\n# All configured scrapers keyed by the endpoint\nself.scrapers = {}\nself.check_initializations.append(self.configure_scrapers)\ndef check(self, _):\n\"\"\"\n        Perform an openmetrics-based check.\n        Subclasses should typically not need to override this, as most common customization\n        needs are covered by the use of custom scrapers.\n        Another thing to note is that this check ignores its instance argument completely.\n        We take care of instance-level customization at initialization time.\n        \"\"\"\nself.refresh_scrapers()\nfor endpoint, scraper in self.scrapers.items():\nself.log.debug('Scraping OpenMetrics endpoint: %s', endpoint)\nwith self.adopt_namespace(scraper.namespace):\ntry:\nscraper.scrape()\nexcept (ConnectionError, RequestException) as e:\nself.log.error(\"There was an error scraping endpoint %s: %s\", endpoint, str(e))\nraise_from(type(e)(\"There was an error scraping endpoint {}: {}\".format(endpoint, e)), None)\ndef configure_scrapers(self):\n\"\"\"\n        Creates a scraper configuration for each instance.\n        \"\"\"\nscrapers = {}\nfor config in self.scraper_configs:\nendpoint = config.get('openmetrics_endpoint', '')\nif not isinstance(endpoint, str):\nraise ConfigurationError('The setting `openmetrics_endpoint` must be a string')\nelif not endpoint:\nraise ConfigurationError('The setting `openmetrics_endpoint` is required')\nscrapers[endpoint] = self.create_scraper(config)\nself.scrapers.clear()\nself.scrapers.update(scrapers)\ndef create_scraper(self, config):\n\"\"\"\n        Subclasses can override to return a custom scraper based on instance configuration.\n        \"\"\"\nreturn OpenMetricsScraper(self, self.get_config_with_defaults(config))\ndef set_dynamic_tags(self, *tags):\nfor scraper in self.scrapers.values():\nscraper.set_dynamic_tags(*tags)\ndef get_config_with_defaults(self, config):\nreturn ChainMap(config, self.get_default_config())\ndef get_default_config(self):\nreturn {}\ndef refresh_scrapers(self):\npass\n@contextmanager\ndef adopt_namespace(self, namespace):\nold_namespace = self.__NAMESPACE__\ntry:\nself.__NAMESPACE__ = namespace or old_namespace\nyield\nfinally:\nself.__NAMESPACE__ = old_namespace\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.base.OpenMetricsBaseCheckV2.__init__","title":"<code>__init__(name, init_config, instances)</code>","text":"<p>The base class for any OpenMetrics-based integration.</p> <p>Subclasses are expected to override this to add their custom scrapers or transformers. When overriding, make sure to call this (the parent's) init first!</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/base.py</code> <pre><code>def __init__(self, name, init_config, instances):\n\"\"\"\n    The base class for any OpenMetrics-based integration.\n    Subclasses are expected to override this to add their custom scrapers or transformers.\n    When overriding, make sure to call this (the parent's) __init__ first!\n    \"\"\"\nsuper(OpenMetricsBaseCheckV2, self).__init__(name, init_config, instances)\n# All desired scraper configurations, which subclasses can override as needed\nself.scraper_configs = [self.instance]\n# All configured scrapers keyed by the endpoint\nself.scrapers = {}\nself.check_initializations.append(self.configure_scrapers)\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.base.OpenMetricsBaseCheckV2.check","title":"<code>check(_)</code>","text":"<p>Perform an openmetrics-based check.</p> <p>Subclasses should typically not need to override this, as most common customization needs are covered by the use of custom scrapers. Another thing to note is that this check ignores its instance argument completely. We take care of instance-level customization at initialization time.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/base.py</code> <pre><code>def check(self, _):\n\"\"\"\n    Perform an openmetrics-based check.\n    Subclasses should typically not need to override this, as most common customization\n    needs are covered by the use of custom scrapers.\n    Another thing to note is that this check ignores its instance argument completely.\n    We take care of instance-level customization at initialization time.\n    \"\"\"\nself.refresh_scrapers()\nfor endpoint, scraper in self.scrapers.items():\nself.log.debug('Scraping OpenMetrics endpoint: %s', endpoint)\nwith self.adopt_namespace(scraper.namespace):\ntry:\nscraper.scrape()\nexcept (ConnectionError, RequestException) as e:\nself.log.error(\"There was an error scraping endpoint %s: %s\", endpoint, str(e))\nraise_from(type(e)(\"There was an error scraping endpoint {}: {}\".format(endpoint, e)), None)\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.base.OpenMetricsBaseCheckV2.configure_scrapers","title":"<code>configure_scrapers()</code>","text":"<p>Creates a scraper configuration for each instance.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/base.py</code> <pre><code>def configure_scrapers(self):\n\"\"\"\n    Creates a scraper configuration for each instance.\n    \"\"\"\nscrapers = {}\nfor config in self.scraper_configs:\nendpoint = config.get('openmetrics_endpoint', '')\nif not isinstance(endpoint, str):\nraise ConfigurationError('The setting `openmetrics_endpoint` must be a string')\nelif not endpoint:\nraise ConfigurationError('The setting `openmetrics_endpoint` is required')\nscrapers[endpoint] = self.create_scraper(config)\nself.scrapers.clear()\nself.scrapers.update(scrapers)\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.base.OpenMetricsBaseCheckV2.create_scraper","title":"<code>create_scraper(config)</code>","text":"<p>Subclasses can override to return a custom scraper based on instance configuration.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/base.py</code> <pre><code>def create_scraper(self, config):\n\"\"\"\n    Subclasses can override to return a custom scraper based on instance configuration.\n    \"\"\"\nreturn OpenMetricsScraper(self, self.get_config_with_defaults(config))\n</code></pre>"},{"location":"base/openmetrics/#scrapers","title":"Scrapers","text":""},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper","title":"<code>datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper</code>","text":"<p>OpenMetricsScraper is a class that can be used to override the default scraping behavior for OpenMetricsBaseCheckV2.</p> <p>Minimal example configuration:</p> <pre><code>- openmetrics_endpoint: http://example.com/endpoint\nnamespace: \"foobar\"\nmetrics:\n- bar\n- foo\nraw_metric_prefix: \"test\"\ntelemetry: \"true\"\nhostname_label: node\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>class OpenMetricsScraper:\n\"\"\"\n    OpenMetricsScraper is a class that can be used to override the default scraping behavior for OpenMetricsBaseCheckV2.\n    Minimal example configuration:\n    ```yaml\n    - openmetrics_endpoint: http://example.com/endpoint\n      namespace: \"foobar\"\n      metrics:\n      - bar\n      - foo\n      raw_metric_prefix: \"test\"\n      telemetry: \"true\"\n      hostname_label: node\n    ```\n    \"\"\"\nSERVICE_CHECK_HEALTH = 'openmetrics.health'\ndef __init__(self, check, config):\n\"\"\"\n        The base class for any scraper overrides.\n        \"\"\"\nself.config = config\n# Save a reference to the check instance\nself.check = check\n# Parse the configuration\nself.endpoint = config['openmetrics_endpoint']\nself.metric_transformer = MetricTransformer(self.check, config)\nself.label_aggregator = LabelAggregator(self.check, config)\nself.enable_telemetry = is_affirmative(config.get('telemetry', False))\n# Make every telemetry submission method a no-op to avoid many lookups of `self.enable_telemetry`\nif not self.enable_telemetry:\nfor name, _ in inspect.getmembers(self, predicate=inspect.ismethod):\nif name.startswith('submit_telemetry_'):\nsetattr(self, name, no_op)\n# Prevent overriding an integration's defined namespace\nself.namespace = check.__NAMESPACE__ or config.get('namespace', '')\nif not isinstance(self.namespace, str):\nraise ConfigurationError('Setting `namespace` must be a string')\nself.raw_metric_prefix = config.get('raw_metric_prefix', '')\nif not isinstance(self.raw_metric_prefix, str):\nraise ConfigurationError('Setting `raw_metric_prefix` must be a string')\nself.enable_health_service_check = is_affirmative(config.get('enable_health_service_check', True))\nself.ignore_connection_errors = is_affirmative(config.get('ignore_connection_errors', False))\nself.hostname_label = config.get('hostname_label', '')\nif not isinstance(self.hostname_label, str):\nraise ConfigurationError('Setting `hostname_label` must be a string')\nhostname_format = config.get('hostname_format', '')\nif not isinstance(hostname_format, str):\nraise ConfigurationError('Setting `hostname_format` must be a string')\nself.hostname_formatter = None\nif self.hostname_label and hostname_format:\nplaceholder = '&lt;HOSTNAME&gt;'\nif placeholder not in hostname_format:\nraise ConfigurationError(f'Setting `hostname_format` does not contain the placeholder `{placeholder}`')\nself.hostname_formatter = lambda hostname: hostname_format.replace('&lt;HOSTNAME&gt;', hostname, 1)\nexclude_labels = config.get('exclude_labels', [])\nif not isinstance(exclude_labels, list):\nraise ConfigurationError('Setting `exclude_labels` must be an array')\nself.exclude_labels = set()\nfor i, entry in enumerate(exclude_labels, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `exclude_labels` must be a string')\nself.exclude_labels.add(entry)\ninclude_labels = config.get('include_labels', [])\nif not isinstance(include_labels, list):\nraise ConfigurationError('Setting `include_labels` must be an array')\nself.include_labels = set()\nfor i, entry in enumerate(include_labels, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `include_labels` must be a string')\nif entry in self.exclude_labels:\nself.log.debug(\n'Label `%s` is set in both `exclude_labels` and `include_labels`. Excluding label.', entry\n)\nself.include_labels.add(entry)\nself.rename_labels = config.get('rename_labels', {})\nif not isinstance(self.rename_labels, dict):\nraise ConfigurationError('Setting `rename_labels` must be a mapping')\nfor key, value in self.rename_labels.items():\nif not isinstance(value, str):\nraise ConfigurationError(f'Value for label `{key}` of setting `rename_labels` must be a string')\nexclude_metrics = config.get('exclude_metrics', [])\nif not isinstance(exclude_metrics, list):\nraise ConfigurationError('Setting `exclude_metrics` must be an array')\nself.exclude_metrics = set()\nself.exclude_metrics_pattern = None\nexclude_metrics_patterns = []\nfor i, entry in enumerate(exclude_metrics, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `exclude_metrics` must be a string')\nescaped_entry = re.escape(entry)\nif entry == escaped_entry:\nself.exclude_metrics.add(entry)\nelse:\nexclude_metrics_patterns.append(entry)\nif exclude_metrics_patterns:\nself.exclude_metrics_pattern = re.compile('|'.join(exclude_metrics_patterns))\nself.exclude_metrics_by_labels = {}\nexclude_metrics_by_labels = config.get('exclude_metrics_by_labels', {})\nif not isinstance(exclude_metrics_by_labels, dict):\nraise ConfigurationError('Setting `exclude_metrics_by_labels` must be a mapping')\nelif exclude_metrics_by_labels:\nfor label, values in exclude_metrics_by_labels.items():\nif values is True:\nself.exclude_metrics_by_labels[label] = return_true\nelif isinstance(values, list):\nfor i, value in enumerate(values, 1):\nif not isinstance(value, str):\nraise ConfigurationError(\nf'Value #{i} for label `{label}` of setting `exclude_metrics_by_labels` '\nf'must be a string'\n)\nself.exclude_metrics_by_labels[label] = (\nlambda label_value, pattern=re.compile('|'.join(values)): pattern.search(  # noqa: B008\nlabel_value\n)  # noqa: B008, E501\nis not None\n)\nelse:\nraise ConfigurationError(\nf'Label `{label}` of setting `exclude_metrics_by_labels` must be an array or set to `true`'\n)\ncustom_tags = config.get('tags', [])  # type: List[str]\nif not isinstance(custom_tags, list):\nraise ConfigurationError('Setting `tags` must be an array')\nfor i, entry in enumerate(custom_tags, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `tags` must be a string')\n# Some tags can be ignored to reduce the cardinality.\n# This can be useful for cost optimization in containerized environments\n# when the openmetrics check is configured to collect custom metrics.\n# Even when the Agent's Tagger is configured to add low-cardinality tags only,\n# some tags can still generate unwanted metric contexts (e.g pod annotations as tags).\nignore_tags = config.get('ignore_tags', [])\nif ignore_tags:\nignored_tags_re = re.compile('|'.join(set(ignore_tags)))\ncustom_tags = [tag for tag in custom_tags if not ignored_tags_re.search(tag)]\nself.static_tags = copy(custom_tags)\nif is_affirmative(self.config.get('tag_by_endpoint', True)):\nself.static_tags.append(f'endpoint:{self.endpoint}')\n# These will be applied only to service checks\nself.static_tags = tuple(self.static_tags)\n# These will be applied to everything except service checks\nself.tags = self.static_tags\nself.raw_line_filter = None\nraw_line_filters = config.get('raw_line_filters', [])\nif not isinstance(raw_line_filters, list):\nraise ConfigurationError('Setting `raw_line_filters` must be an array')\nelif raw_line_filters:\nfor i, entry in enumerate(raw_line_filters, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `raw_line_filters` must be a string')\nself.raw_line_filter = re.compile('|'.join(raw_line_filters))\nself.http = RequestsWrapper(config, self.check.init_config, self.check.HTTP_CONFIG_REMAPPER, self.check.log)\nself._content_type = ''\nself._use_latest_spec = is_affirmative(config.get('use_latest_spec', False))\nif self._use_latest_spec:\naccept_header = 'application/openmetrics-text;version=1.0.0,application/openmetrics-text;version=0.0.1'\nelse:\naccept_header = 'text/plain'\n# Request the appropriate exposition format\nif self.http.options['headers'].get('Accept') == '*/*':\nself.http.options['headers']['Accept'] = accept_header\nself.use_process_start_time = is_affirmative(config.get('use_process_start_time'))\n# Used for monotonic counts\nself.flush_first_value = False\ndef scrape(self):\n\"\"\"\n        Execute a scrape, and for each metric collected, transform the metric.\n        \"\"\"\nruntime_data = {'flush_first_value': self.flush_first_value, 'static_tags': self.static_tags}\nfor metric in self.consume_metrics(runtime_data):\ntransformer = self.metric_transformer.get(metric)\nif transformer is None:\ncontinue\ntransformer(metric, self.generate_sample_data(metric), runtime_data)\nself.flush_first_value = True\ndef consume_metrics(self, runtime_data):\n\"\"\"\n        Yield the processed metrics and filter out excluded metrics.\n        \"\"\"\nmetric_parser = self.parse_metrics()\nif not self.flush_first_value and self.use_process_start_time:\nmetric_parser = first_scrape_handler(metric_parser, runtime_data, datadog_agent.get_process_start_time())\nif self.label_aggregator.configured:\nmetric_parser = self.label_aggregator(metric_parser)\nfor metric in metric_parser:\nif metric.name in self.exclude_metrics or (\nself.exclude_metrics_pattern is not None and self.exclude_metrics_pattern.search(metric.name)\n):\nself.submit_telemetry_number_of_ignored_metric_samples(metric)\ncontinue\nyield metric\ndef parse_metrics(self):\n\"\"\"\n        Get the line streamer and yield processed metrics.\n        \"\"\"\nline_streamer = self.stream_connection_lines()\nif self.raw_line_filter is not None:\nline_streamer = self.filter_connection_lines(line_streamer)\n# Since we determine `self.parse_metric_families` dynamically from the response and that's done as a\n# side effect inside the `line_streamer` generator, we need to consume the first line in order to\n# trigger that side effect.\ntry:\nline_streamer = chain([next(line_streamer)], line_streamer)\nexcept StopIteration:\n# If line_streamer is an empty iterator, next(line_streamer) fails.\nreturn\nfor metric in self.parse_metric_families(line_streamer):\nself.submit_telemetry_number_of_total_metric_samples(metric)\n# It is critical that the prefix is removed immediately so that\n# all other configuration may reference the trimmed metric name\nif self.raw_metric_prefix and metric.name.startswith(self.raw_metric_prefix):\nmetric.name = metric.name[len(self.raw_metric_prefix) :]\nyield metric\n@property\ndef parse_metric_families(self):\nmedia_type = self._content_type.split(';')[0]\n# Setting `use_latest_spec` forces the use of the OpenMetrics format, otherwise\n# the format will be chosen based on the media type specified in the response's content-header.\n# The selection is based on what Prometheus does:\n# https://github.com/prometheus/prometheus/blob/v2.43.0/model/textparse/interface.go#L83-L90\nreturn (\nparse_openmetrics\nif self._use_latest_spec or media_type == 'application/openmetrics-text'\nelse parse_prometheus\n)\ndef generate_sample_data(self, metric):\n\"\"\"\n        Yield a sample of processed data.\n        \"\"\"\nlabel_normalizer = get_label_normalizer(metric.type)\nfor sample in metric.samples:\nvalue = sample.value\nif isnan(value) or isinf(value):\nself.log.debug('Ignoring sample for metric `%s` as it has an invalid value: %s', metric.name, value)\ncontinue\ntags = []\nskip_sample = False\nlabels = sample.labels\nself.label_aggregator.populate(labels)\nlabel_normalizer(labels)\nfor label_name, label_value in labels.items():\nsample_excluder = self.exclude_metrics_by_labels.get(label_name)\nif sample_excluder is not None and sample_excluder(label_value):\nskip_sample = True\nbreak\nelif label_name in self.exclude_labels:\ncontinue\nelif self.include_labels and label_name not in self.include_labels:\ncontinue\nlabel_name = self.rename_labels.get(label_name, label_name)\ntags.append(f'{label_name}:{label_value}')\nif skip_sample:\ncontinue\ntags.extend(self.tags)\nhostname = \"\"\nif self.hostname_label and self.hostname_label in labels:\nhostname = labels[self.hostname_label]\nif self.hostname_formatter is not None:\nhostname = self.hostname_formatter(hostname)\nself.submit_telemetry_number_of_processed_metric_samples()\nyield sample, tags, hostname\ndef stream_connection_lines(self):\n\"\"\"\n        Yield the connection line.\n        \"\"\"\ntry:\nwith self.get_connection() as connection:\n# Media type will be used to select parser dynamically\nself._content_type = connection.headers.get('Content-Type', '')\nfor line in connection.iter_lines(decode_unicode=True):\nyield line\nexcept ConnectionError as e:\nif self.ignore_connection_errors:\nself.log.warning(\"OpenMetrics endpoint %s is not accessible\", self.endpoint)\nelse:\nraise e\ndef filter_connection_lines(self, line_streamer):\n\"\"\"\n        Filter connection lines in the line streamer.\n        \"\"\"\nfor line in line_streamer:\nif self.raw_line_filter.search(line):\nself.submit_telemetry_number_of_ignored_lines()\nelse:\nyield line\ndef get_connection(self):\n\"\"\"\n        Send a request to scrape metrics. Return the response or throw an exception.\n        \"\"\"\ntry:\nresponse = self.send_request()\nexcept Exception as e:\nself.submit_health_check(ServiceCheck.CRITICAL, message=str(e))\nraise\nelse:\ntry:\nresponse.raise_for_status()\nexcept Exception as e:\nself.submit_health_check(ServiceCheck.CRITICAL, message=str(e))\nresponse.close()\nraise\nelse:\nself.submit_health_check(ServiceCheck.OK)\n# Never derive the encoding from the locale\nif response.encoding is None:\nresponse.encoding = 'utf-8'\nself.submit_telemetry_endpoint_response_size(response)\nreturn response\ndef send_request(self, **kwargs):\n\"\"\"\n        Send an HTTP GET request to the `openmetrics_endpoint` value.\n        \"\"\"\nkwargs['stream'] = True\nreturn self.http.get(self.endpoint, **kwargs)\ndef set_dynamic_tags(self, *tags):\n\"\"\"\n        Set dynamic tags.\n        \"\"\"\nself.tags = tuple(chain(self.static_tags, tags))\ndef submit_health_check(self, status, **kwargs):\n\"\"\"\n        If health service check is enabled, send an `openmetrics.health` service check.\n        \"\"\"\nif self.enable_health_service_check:\nself.service_check(self.SERVICE_CHECK_HEALTH, status, tags=self.static_tags, **kwargs)\ndef submit_telemetry_number_of_total_metric_samples(self, metric):\nself.count('telemetry.metrics.input.count', len(metric.samples), tags=self.tags)\ndef submit_telemetry_number_of_ignored_metric_samples(self, metric):\nself.count('telemetry.metrics.ignored.count', len(metric.samples), tags=self.tags)\ndef submit_telemetry_number_of_processed_metric_samples(self):\nself.count('telemetry.metrics.processed.count', 1, tags=self.tags)\ndef submit_telemetry_number_of_ignored_lines(self):\nself.count('telemetry.metrics.blacklist.count', 1, tags=self.tags)\ndef submit_telemetry_endpoint_response_size(self, response):\ncontent_length = response.headers.get('Content-Length')\nif content_length is not None:\ncontent_length = int(content_length)\nelse:\ncontent_length = len(response.content)\nself.gauge('telemetry.payload.size', content_length, tags=self.tags)\ndef __getattr__(self, name):\n# Forward all unknown attribute lookups to the check instance for access to submission methods, hostname, etc.\nattribute = getattr(self.check, name)\nsetattr(self, name, attribute)\nreturn attribute\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.__init__","title":"<code>__init__(check, config)</code>","text":"<p>The base class for any scraper overrides.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def __init__(self, check, config):\n\"\"\"\n    The base class for any scraper overrides.\n    \"\"\"\nself.config = config\n# Save a reference to the check instance\nself.check = check\n# Parse the configuration\nself.endpoint = config['openmetrics_endpoint']\nself.metric_transformer = MetricTransformer(self.check, config)\nself.label_aggregator = LabelAggregator(self.check, config)\nself.enable_telemetry = is_affirmative(config.get('telemetry', False))\n# Make every telemetry submission method a no-op to avoid many lookups of `self.enable_telemetry`\nif not self.enable_telemetry:\nfor name, _ in inspect.getmembers(self, predicate=inspect.ismethod):\nif name.startswith('submit_telemetry_'):\nsetattr(self, name, no_op)\n# Prevent overriding an integration's defined namespace\nself.namespace = check.__NAMESPACE__ or config.get('namespace', '')\nif not isinstance(self.namespace, str):\nraise ConfigurationError('Setting `namespace` must be a string')\nself.raw_metric_prefix = config.get('raw_metric_prefix', '')\nif not isinstance(self.raw_metric_prefix, str):\nraise ConfigurationError('Setting `raw_metric_prefix` must be a string')\nself.enable_health_service_check = is_affirmative(config.get('enable_health_service_check', True))\nself.ignore_connection_errors = is_affirmative(config.get('ignore_connection_errors', False))\nself.hostname_label = config.get('hostname_label', '')\nif not isinstance(self.hostname_label, str):\nraise ConfigurationError('Setting `hostname_label` must be a string')\nhostname_format = config.get('hostname_format', '')\nif not isinstance(hostname_format, str):\nraise ConfigurationError('Setting `hostname_format` must be a string')\nself.hostname_formatter = None\nif self.hostname_label and hostname_format:\nplaceholder = '&lt;HOSTNAME&gt;'\nif placeholder not in hostname_format:\nraise ConfigurationError(f'Setting `hostname_format` does not contain the placeholder `{placeholder}`')\nself.hostname_formatter = lambda hostname: hostname_format.replace('&lt;HOSTNAME&gt;', hostname, 1)\nexclude_labels = config.get('exclude_labels', [])\nif not isinstance(exclude_labels, list):\nraise ConfigurationError('Setting `exclude_labels` must be an array')\nself.exclude_labels = set()\nfor i, entry in enumerate(exclude_labels, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `exclude_labels` must be a string')\nself.exclude_labels.add(entry)\ninclude_labels = config.get('include_labels', [])\nif not isinstance(include_labels, list):\nraise ConfigurationError('Setting `include_labels` must be an array')\nself.include_labels = set()\nfor i, entry in enumerate(include_labels, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `include_labels` must be a string')\nif entry in self.exclude_labels:\nself.log.debug(\n'Label `%s` is set in both `exclude_labels` and `include_labels`. Excluding label.', entry\n)\nself.include_labels.add(entry)\nself.rename_labels = config.get('rename_labels', {})\nif not isinstance(self.rename_labels, dict):\nraise ConfigurationError('Setting `rename_labels` must be a mapping')\nfor key, value in self.rename_labels.items():\nif not isinstance(value, str):\nraise ConfigurationError(f'Value for label `{key}` of setting `rename_labels` must be a string')\nexclude_metrics = config.get('exclude_metrics', [])\nif not isinstance(exclude_metrics, list):\nraise ConfigurationError('Setting `exclude_metrics` must be an array')\nself.exclude_metrics = set()\nself.exclude_metrics_pattern = None\nexclude_metrics_patterns = []\nfor i, entry in enumerate(exclude_metrics, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `exclude_metrics` must be a string')\nescaped_entry = re.escape(entry)\nif entry == escaped_entry:\nself.exclude_metrics.add(entry)\nelse:\nexclude_metrics_patterns.append(entry)\nif exclude_metrics_patterns:\nself.exclude_metrics_pattern = re.compile('|'.join(exclude_metrics_patterns))\nself.exclude_metrics_by_labels = {}\nexclude_metrics_by_labels = config.get('exclude_metrics_by_labels', {})\nif not isinstance(exclude_metrics_by_labels, dict):\nraise ConfigurationError('Setting `exclude_metrics_by_labels` must be a mapping')\nelif exclude_metrics_by_labels:\nfor label, values in exclude_metrics_by_labels.items():\nif values is True:\nself.exclude_metrics_by_labels[label] = return_true\nelif isinstance(values, list):\nfor i, value in enumerate(values, 1):\nif not isinstance(value, str):\nraise ConfigurationError(\nf'Value #{i} for label `{label}` of setting `exclude_metrics_by_labels` '\nf'must be a string'\n)\nself.exclude_metrics_by_labels[label] = (\nlambda label_value, pattern=re.compile('|'.join(values)): pattern.search(  # noqa: B008\nlabel_value\n)  # noqa: B008, E501\nis not None\n)\nelse:\nraise ConfigurationError(\nf'Label `{label}` of setting `exclude_metrics_by_labels` must be an array or set to `true`'\n)\ncustom_tags = config.get('tags', [])  # type: List[str]\nif not isinstance(custom_tags, list):\nraise ConfigurationError('Setting `tags` must be an array')\nfor i, entry in enumerate(custom_tags, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `tags` must be a string')\n# Some tags can be ignored to reduce the cardinality.\n# This can be useful for cost optimization in containerized environments\n# when the openmetrics check is configured to collect custom metrics.\n# Even when the Agent's Tagger is configured to add low-cardinality tags only,\n# some tags can still generate unwanted metric contexts (e.g pod annotations as tags).\nignore_tags = config.get('ignore_tags', [])\nif ignore_tags:\nignored_tags_re = re.compile('|'.join(set(ignore_tags)))\ncustom_tags = [tag for tag in custom_tags if not ignored_tags_re.search(tag)]\nself.static_tags = copy(custom_tags)\nif is_affirmative(self.config.get('tag_by_endpoint', True)):\nself.static_tags.append(f'endpoint:{self.endpoint}')\n# These will be applied only to service checks\nself.static_tags = tuple(self.static_tags)\n# These will be applied to everything except service checks\nself.tags = self.static_tags\nself.raw_line_filter = None\nraw_line_filters = config.get('raw_line_filters', [])\nif not isinstance(raw_line_filters, list):\nraise ConfigurationError('Setting `raw_line_filters` must be an array')\nelif raw_line_filters:\nfor i, entry in enumerate(raw_line_filters, 1):\nif not isinstance(entry, str):\nraise ConfigurationError(f'Entry #{i} of setting `raw_line_filters` must be a string')\nself.raw_line_filter = re.compile('|'.join(raw_line_filters))\nself.http = RequestsWrapper(config, self.check.init_config, self.check.HTTP_CONFIG_REMAPPER, self.check.log)\nself._content_type = ''\nself._use_latest_spec = is_affirmative(config.get('use_latest_spec', False))\nif self._use_latest_spec:\naccept_header = 'application/openmetrics-text;version=1.0.0,application/openmetrics-text;version=0.0.1'\nelse:\naccept_header = 'text/plain'\n# Request the appropriate exposition format\nif self.http.options['headers'].get('Accept') == '*/*':\nself.http.options['headers']['Accept'] = accept_header\nself.use_process_start_time = is_affirmative(config.get('use_process_start_time'))\n# Used for monotonic counts\nself.flush_first_value = False\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.scrape","title":"<code>scrape()</code>","text":"<p>Execute a scrape, and for each metric collected, transform the metric.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def scrape(self):\n\"\"\"\n    Execute a scrape, and for each metric collected, transform the metric.\n    \"\"\"\nruntime_data = {'flush_first_value': self.flush_first_value, 'static_tags': self.static_tags}\nfor metric in self.consume_metrics(runtime_data):\ntransformer = self.metric_transformer.get(metric)\nif transformer is None:\ncontinue\ntransformer(metric, self.generate_sample_data(metric), runtime_data)\nself.flush_first_value = True\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.consume_metrics","title":"<code>consume_metrics(runtime_data)</code>","text":"<p>Yield the processed metrics and filter out excluded metrics.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def consume_metrics(self, runtime_data):\n\"\"\"\n    Yield the processed metrics and filter out excluded metrics.\n    \"\"\"\nmetric_parser = self.parse_metrics()\nif not self.flush_first_value and self.use_process_start_time:\nmetric_parser = first_scrape_handler(metric_parser, runtime_data, datadog_agent.get_process_start_time())\nif self.label_aggregator.configured:\nmetric_parser = self.label_aggregator(metric_parser)\nfor metric in metric_parser:\nif metric.name in self.exclude_metrics or (\nself.exclude_metrics_pattern is not None and self.exclude_metrics_pattern.search(metric.name)\n):\nself.submit_telemetry_number_of_ignored_metric_samples(metric)\ncontinue\nyield metric\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.parse_metrics","title":"<code>parse_metrics()</code>","text":"<p>Get the line streamer and yield processed metrics.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def parse_metrics(self):\n\"\"\"\n    Get the line streamer and yield processed metrics.\n    \"\"\"\nline_streamer = self.stream_connection_lines()\nif self.raw_line_filter is not None:\nline_streamer = self.filter_connection_lines(line_streamer)\n# Since we determine `self.parse_metric_families` dynamically from the response and that's done as a\n# side effect inside the `line_streamer` generator, we need to consume the first line in order to\n# trigger that side effect.\ntry:\nline_streamer = chain([next(line_streamer)], line_streamer)\nexcept StopIteration:\n# If line_streamer is an empty iterator, next(line_streamer) fails.\nreturn\nfor metric in self.parse_metric_families(line_streamer):\nself.submit_telemetry_number_of_total_metric_samples(metric)\n# It is critical that the prefix is removed immediately so that\n# all other configuration may reference the trimmed metric name\nif self.raw_metric_prefix and metric.name.startswith(self.raw_metric_prefix):\nmetric.name = metric.name[len(self.raw_metric_prefix) :]\nyield metric\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.generate_sample_data","title":"<code>generate_sample_data(metric)</code>","text":"<p>Yield a sample of processed data.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def generate_sample_data(self, metric):\n\"\"\"\n    Yield a sample of processed data.\n    \"\"\"\nlabel_normalizer = get_label_normalizer(metric.type)\nfor sample in metric.samples:\nvalue = sample.value\nif isnan(value) or isinf(value):\nself.log.debug('Ignoring sample for metric `%s` as it has an invalid value: %s', metric.name, value)\ncontinue\ntags = []\nskip_sample = False\nlabels = sample.labels\nself.label_aggregator.populate(labels)\nlabel_normalizer(labels)\nfor label_name, label_value in labels.items():\nsample_excluder = self.exclude_metrics_by_labels.get(label_name)\nif sample_excluder is not None and sample_excluder(label_value):\nskip_sample = True\nbreak\nelif label_name in self.exclude_labels:\ncontinue\nelif self.include_labels and label_name not in self.include_labels:\ncontinue\nlabel_name = self.rename_labels.get(label_name, label_name)\ntags.append(f'{label_name}:{label_value}')\nif skip_sample:\ncontinue\ntags.extend(self.tags)\nhostname = \"\"\nif self.hostname_label and self.hostname_label in labels:\nhostname = labels[self.hostname_label]\nif self.hostname_formatter is not None:\nhostname = self.hostname_formatter(hostname)\nself.submit_telemetry_number_of_processed_metric_samples()\nyield sample, tags, hostname\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.stream_connection_lines","title":"<code>stream_connection_lines()</code>","text":"<p>Yield the connection line.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def stream_connection_lines(self):\n\"\"\"\n    Yield the connection line.\n    \"\"\"\ntry:\nwith self.get_connection() as connection:\n# Media type will be used to select parser dynamically\nself._content_type = connection.headers.get('Content-Type', '')\nfor line in connection.iter_lines(decode_unicode=True):\nyield line\nexcept ConnectionError as e:\nif self.ignore_connection_errors:\nself.log.warning(\"OpenMetrics endpoint %s is not accessible\", self.endpoint)\nelse:\nraise e\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.filter_connection_lines","title":"<code>filter_connection_lines(line_streamer)</code>","text":"<p>Filter connection lines in the line streamer.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def filter_connection_lines(self, line_streamer):\n\"\"\"\n    Filter connection lines in the line streamer.\n    \"\"\"\nfor line in line_streamer:\nif self.raw_line_filter.search(line):\nself.submit_telemetry_number_of_ignored_lines()\nelse:\nyield line\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.get_connection","title":"<code>get_connection()</code>","text":"<p>Send a request to scrape metrics. Return the response or throw an exception.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def get_connection(self):\n\"\"\"\n    Send a request to scrape metrics. Return the response or throw an exception.\n    \"\"\"\ntry:\nresponse = self.send_request()\nexcept Exception as e:\nself.submit_health_check(ServiceCheck.CRITICAL, message=str(e))\nraise\nelse:\ntry:\nresponse.raise_for_status()\nexcept Exception as e:\nself.submit_health_check(ServiceCheck.CRITICAL, message=str(e))\nresponse.close()\nraise\nelse:\nself.submit_health_check(ServiceCheck.OK)\n# Never derive the encoding from the locale\nif response.encoding is None:\nresponse.encoding = 'utf-8'\nself.submit_telemetry_endpoint_response_size(response)\nreturn response\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.set_dynamic_tags","title":"<code>set_dynamic_tags(*tags)</code>","text":"<p>Set dynamic tags.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def set_dynamic_tags(self, *tags):\n\"\"\"\n    Set dynamic tags.\n    \"\"\"\nself.tags = tuple(chain(self.static_tags, tags))\n</code></pre>"},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.scraper.OpenMetricsScraper.submit_health_check","title":"<code>submit_health_check(status, **kwargs)</code>","text":"<p>If health service check is enabled, send an <code>openmetrics.health</code> service check.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/scraper.py</code> <pre><code>def submit_health_check(self, status, **kwargs):\n\"\"\"\n    If health service check is enabled, send an `openmetrics.health` service check.\n    \"\"\"\nif self.enable_health_service_check:\nself.service_check(self.SERVICE_CHECK_HEALTH, status, tags=self.static_tags, **kwargs)\n</code></pre>"},{"location":"base/openmetrics/#transformers","title":"Transformers","text":""},{"location":"base/openmetrics/#datadog_checks.base.checks.openmetrics.v2.transform.Transformers","title":"<code>datadog_checks.base.checks.openmetrics.v2.transform.Transformers</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/v2/transform.py</code> <pre><code>class Transformers(object):\npass\n</code></pre>"},{"location":"base/openmetrics/#options","title":"Options","text":"<p>For complete documentation on every option, see the associated templates for the instance and init_config  sections.</p>"},{"location":"base/openmetrics/#legacy","title":"Legacy","text":"<p>This OpenMetrics implementation is the updated version of the original Prometheus/OpenMetrics implementation. The docs for the deprecated implementation are still available as a reference.</p>"},{"location":"base/tls/","title":"TLS/SSL","text":"<p>TLS/SSL is widely used to provide communications over a secure network. Many of the software that Datadog supports has features to allow TLS/SSL. Therefore, the Datadog Agent may need to connect with TLS/SSL to get metrics.</p>"},{"location":"base/tls/#getting-started","title":"Getting started","text":"<p>For Agent v7.24+, checks compatible with TLS/SSL should not manually create a raw <code>ssl.SSLContext</code>. Instead, check implementations should use <code>AgentCheck.get_tls_context()</code> to obtain a TLS/SSL context.</p> <p><code>get_tls_context()</code> allows a few optional parameters which may be helpful when developing integrations.</p>"},{"location":"base/tls/#datadog_checks.base.checks.base.AgentCheck.get_tls_context","title":"<code>datadog_checks.base.checks.base.AgentCheck.get_tls_context(refresh=False, overrides=None)</code>","text":"<p>Creates and cache an SSLContext instance based on user configuration. Note that user configuration can be overridden by using <code>overrides</code>. This should only be applied to older integration that manually set config values.</p> <p>Since: Agent 7.24</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/base.py</code> <pre><code>def get_tls_context(self, refresh=False, overrides=None):\n# type: (bool, Dict[AnyStr, Any]) -&gt; ssl.SSLContext\n\"\"\"\n    Creates and cache an SSLContext instance based on user configuration.\n    Note that user configuration can be overridden by using `overrides`.\n    This should only be applied to older integration that manually set config values.\n    Since: Agent 7.24\n    \"\"\"\nif not hasattr(self, '_tls_context_wrapper'):\nself._tls_context_wrapper = TlsContextWrapper(\nself.instance or {}, self.TLS_CONFIG_REMAPPER, overrides=overrides\n)\nif refresh:\nself._tls_context_wrapper.refresh_tls_context()\nreturn self._tls_context_wrapper.tls_context\n</code></pre>"},{"location":"ddev/about/","title":"What's in the box?","text":"<p>The Dev package, often referred to as its CLI entrypoint <code>ddev</code>, is fundamentally split into 2 parts.</p>"},{"location":"ddev/about/#test-framework","title":"Test framework","text":"<p>The test framework provides everything necessary to test integrations, such as:</p> <ul> <li>Dependencies like pytest, mock, requests, etc.</li> <li>Utilities for consistently handling complex logic or common operations</li> <li>An orchestrator for arbitrary E2E environments</li> </ul> <p>Python 2 Alert!</p> <p>Some integrations still support Python version 2.7 and must be tested with it. As a consequence, so must parts of our test framework, for example the pytest plugin.</p>"},{"location":"ddev/about/#cli","title":"CLI","text":"<p>The CLI provides the interface through which tests are invoked, E2E environments are managed, and general repository maintenance (such as dependency management) occurs.</p>"},{"location":"ddev/about/#separation","title":"Separation","text":"<p>As the dependencies of the test framework are a subset of what is required for the CLI, the CLI tooling may import from the test framework, but not vice versa.</p> <p>The diagram below shows the import hierarchy between each component. Clicking a node will open that component's location in the source code.</p> <pre><code>graph BT\n    A([Plugins])\n    click A \"https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev/plugin\" \"Test framework plugins location\"\n\n    B([Test framework])\n    click B \"https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev\" \"Test framework location\"\n\n    C([CLI])\n    click C \"https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev/tooling\" \"CLI tooling location\"\n\n    A--&gt;B\n    C--&gt;B</code></pre>"},{"location":"ddev/cli/","title":"ddev","text":"<p>Usage:</p> <pre><code>ddev [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--core</code>, <code>-c</code> boolean Work on <code>integrations-core</code>. <code>False</code> <code>--extras</code>, <code>-e</code> boolean Work on <code>integrations-extras</code>. <code>False</code> <code>--marketplace</code>, <code>-m</code> boolean Work on <code>marketplace</code>. <code>False</code> <code>--agent</code>, <code>-a</code> boolean Work on <code>datadog-agent</code>. <code>False</code> <code>--here</code>, <code>-x</code> boolean Work on the current location. <code>False</code> <code>--color</code> / <code>--no-color</code> boolean Whether or not to display colored output (default is auto-detection) [env vars: <code>FORCE_COLOR</code>/<code>NO_COLOR</code>] None <code>--interactive</code> / <code>--no-interactive</code> boolean Whether or not to allow features like prompts and progress bars (default is auto-detection) [env var: <code>DDEV_INTERACTIVE</code>] None <code>--verbose</code>, <code>-v</code> integer range (<code>0</code> and above) Increase verbosity (can be used additively) [env var: <code>DDEV_VERBOSE</code>] <code>0</code> <code>--quiet</code>, <code>-q</code> integer range (<code>0</code> and above) Decrease verbosity (can be used additively) [env var: <code>DDEV_QUIET</code>] <code>0</code> <code>--config</code> text The path to a custom config file to use [env var: <code>DDEV_CONFIG</code>] None <code>--version</code> boolean Show the version and exit. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-ci","title":"ddev ci","text":"<p>CI related utils. Anything here should be considered experimental.</p> <p>Usage:</p> <pre><code>ddev ci [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-ci-setup","title":"ddev ci setup","text":"<p>Run CI setup scripts</p> <p>Usage:</p> <pre><code>ddev ci setup [OPTIONS] [CHECKS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--changed</code> boolean Only target changed checks <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-clean","title":"ddev clean","text":"<p>Remove build and test artifacts for the entire repository.</p> <p>Usage:</p> <pre><code>ddev clean [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-config","title":"ddev config","text":"<p>Manage the config file</p> <p>Usage:</p> <pre><code>ddev config [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-config-edit","title":"ddev config edit","text":"<p>Edit the config file with your default editor.</p> <p>Usage:</p> <pre><code>ddev config edit [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-config-explore","title":"ddev config explore","text":"<p>Open the config location in your file manager.</p> <p>Usage:</p> <pre><code>ddev config explore [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-config-find","title":"ddev config find","text":"<p>Show the location of the config file.</p> <p>Usage:</p> <pre><code>ddev config find [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-config-restore","title":"ddev config restore","text":"<p>Restore the config file to default settings.</p> <p>Usage:</p> <pre><code>ddev config restore [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-config-set","title":"ddev config set","text":"<p>Assign values to config file entries. If the value is omitted, you will be prompted, with the input hidden if it is sensitive.</p> <p>Usage:</p> <pre><code>ddev config set [OPTIONS] KEY [VALUE]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-config-show","title":"ddev config show","text":"<p>Show the contents of the config file.</p> <p>Usage:</p> <pre><code>ddev config show [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--all</code>, <code>-a</code> boolean Do not scrub secret fields <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-config-update","title":"ddev config update","text":"<p>Update the config file with any new fields.</p> <p>Usage:</p> <pre><code>ddev config update [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-create","title":"ddev create","text":"<p>Create scaffolding for a new integration.</p> <p>NAME: The display name of the integration that will appear in documentation.</p> <p>Usage:</p> <pre><code>ddev create [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--type</code>, <code>-t</code> choice (<code>check</code> | <code>jmx</code> | <code>logs</code> | <code>metrics_pull</code> | <code>snmp_tile</code> | <code>tile</code>) The type of integration to create <code>check</code> <code>--location</code>, <code>-l</code> text The directory where files will be written None <code>--non-interactive</code>, <code>-ni</code> boolean Disable prompting for fields <code>False</code> <code>--quiet</code>, <code>-q</code> boolean Show less output <code>False</code> <code>--dry-run</code>, <code>-n</code> boolean Only show what would be created <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-dep","title":"ddev dep","text":"<p>Manage dependencies</p> <p>Usage:</p> <pre><code>ddev dep [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-dep-freeze","title":"ddev dep freeze","text":"<p>Combine all dependencies for the Agent's static environment.</p> <p>Usage:</p> <pre><code>ddev dep freeze [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-dep-pin","title":"ddev dep pin","text":"<p>Pin a dependency for all checks that require it.</p> <p>Usage:</p> <pre><code>ddev dep pin [OPTIONS] DEFINITION\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-dep-sync","title":"ddev dep sync","text":"<p>Update integrations' dependencies so that they match the Agent's static environment</p> <p>Usage:</p> <pre><code>ddev dep sync [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-dep-updates","title":"ddev dep updates","text":"<p>Automatically check for dependency updates</p> <p>Usage:</p> <pre><code>ddev dep updates [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--sync</code>, <code>-s</code> boolean Update the dependency definitions <code>False</code> <code>--include-security-deps</code>, <code>-i</code> boolean Attempt to update security dependencies <code>False</code> <code>--batch-size</code>, <code>-b</code> integer The maximum number of dependencies to upgrade if syncing None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-docs","title":"ddev docs","text":"<p>Manage documentation.</p> <p>Usage:</p> <pre><code>ddev docs [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-docs-build","title":"ddev docs build","text":"<p>Build documentation.</p> <p>Usage:</p> <pre><code>ddev docs build [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--check</code> boolean Ensure links are valid <code>False</code> <code>--pdf</code> boolean Also export the site as PDF <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-docs-serve","title":"ddev docs serve","text":"<p>Serve documentation.</p> <p>Usage:</p> <pre><code>ddev docs serve [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--dirty</code> boolean Speed up reload time by only rebuilding edited pages (based on modified time). For development only. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-env","title":"ddev env","text":"<p>Manage environments.</p> <p>Usage:</p> <pre><code>ddev env [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-env-check","title":"ddev env check","text":"<p>Run an Agent check.</p> <p>Usage:</p> <pre><code>ddev env check [OPTIONS] CHECK [ENV]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--rate</code>, <code>-r</code> boolean Compute rates by running the check twice with a pause between each run <code>False</code> <code>--times</code>, <code>-t</code> integer Number of times to run the check None <code>--pause</code> integer Number of milliseconds to pause between multiple check runs None <code>--delay</code>, <code>-d</code> integer Delay in milliseconds between running the check and grabbing what was collected None <code>--log-level</code>, <code>-l</code> text Set the log level (default <code>off</code>) None <code>--json</code> boolean Format the aggregator and check runner output as JSON <code>False</code> <code>--table</code> boolean Format the aggregator and check runner output as tabular <code>False</code> <code>--breakpoint</code>, <code>-b</code> integer Line number to start a PDB session (0: first line, -1: last line) None <code>--config</code> text Path to a JSON check configuration to use None <code>--jmx-list</code> text JMX metrics listing method None <code>--discovery-timeout</code> integer Max retry duration until Autodiscovery resolves the check template (in seconds) None <code>--discovery-retry-interval</code> integer Duration between retries until Autodiscovery resolves the check template (in seconds) None <code>--discovery-min-instances</code> integer Number of checks to wait, retry until the specified number of checks is reached None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-env-edit","title":"ddev env edit","text":"<p>Start an environment.</p> <p>Usage:</p> <pre><code>ddev env edit [OPTIONS] CHECK ENV\n</code></pre> <p>Options:</p> Name Type Description Default <code>--editor</code>, <code>-e</code> text Editor to use None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-env-ls","title":"ddev env ls","text":"<p>List active or available environments.</p> <p>Usage:</p> <pre><code>ddev env ls [OPTIONS] [CHECKS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-env-prune","title":"ddev env prune","text":"<p>Remove all configuration for environments.</p> <p>Usage:</p> <pre><code>ddev env prune [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--force</code>, <code>-f</code> boolean N/A <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-env-reload","title":"ddev env reload","text":"<p>Restart an Agent to detect environment changes.</p> <p>Usage:</p> <pre><code>ddev env reload [OPTIONS] CHECK [ENV]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-env-shell","title":"ddev env shell","text":"<p>Run a shell inside the Agent docker container.</p> <p>Usage:</p> <pre><code>ddev env shell [OPTIONS] CHECK [ENV]\n</code></pre> <p>Options:</p> Name Type Description Default <code>-c</code>, <code>--exec-command</code> text Optionally execute command inside container, executes after any installs None <code>-v</code>, <code>--install-vim</code> boolean Optionally install editing/viewing tools vim and less <code>False</code> <code>-i</code>, <code>--install-tools</code> text Optionally install custom tools None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-env-start","title":"ddev env start","text":"<p>Start an environment.</p> <p>Usage:</p> <pre><code>ddev env start [OPTIONS] CHECK ENV\n</code></pre> <p>Options:</p> Name Type Description Default <code>--agent</code>, <code>-a</code> text The agent build to use e.g. a Docker image like <code>datadog/agent:latest</code>. You can also use the name of an agent defined in the <code>agents</code> configuration section. None <code>--python</code>, <code>-py</code> integer The version of Python to use. Defaults to 3 if no tox Python is specified. None <code>--dev</code> / <code>--prod</code> boolean By default we use version of the check that is shipped with the agent you are using.Pass --dev to explicitly enforce the local version. Also see the <code>--base</code> option. None <code>--base</code> boolean Pass this flag to mount the local version of the base package. By default we use the version shipped with the agent. Note that passing the flag also mounts the local version of the check. <p>More about the base package: https://datadoghq.dev/integrations-core/base/about/ | <code>False</code> | | <code>--env-vars</code>, <code>-e</code> | text | ENV Variable that should be passed to the Agent container. Ex: -e DD_URL=app.datadoghq.com -e DD_API_KEY=123456 | None | | <code>--org-name</code>, <code>-o</code> | text | The org to use for data submission. | None | | <code>--profile-memory</code>, <code>-pm</code> | boolean | Whether to collect metrics about memory usage | <code>False</code> | | <code>--dogstatsd</code> | boolean | Enable dogstatsd port on agent | <code>False</code> | | <code>--help</code> | boolean | Show this message and exit. | <code>False</code> |</p>"},{"location":"ddev/cli/#ddev-env-stop","title":"ddev env stop","text":"<p>Stop environments, use \"all\" as check argument to stop everything.</p> <p>Usage:</p> <pre><code>ddev env stop [OPTIONS] CHECK [ENV]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-env-test","title":"ddev env test","text":"<p>Test an environment.</p> <p>Usage:</p> <pre><code>ddev env test [OPTIONS] [CHECKS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--agent</code>, <code>-a</code> text The agent build to use e.g. a Docker image like <code>datadog/agent:latest</code>. You can also use the name of an agent defined in the <code>agents</code> configuration section. None <code>--python</code>, <code>-py</code> integer The version of Python to use. Defaults to 3 if no tox Python is specified. None <code>--dev</code> / <code>--prod</code> boolean By default we use version of the check that is shipped with the agent you are using.Pass --dev to explicitly enforce the local version. Also see the <code>--base</code> option. None <code>--base</code> boolean Pass this flag to mount the local version of the base package. By default we use the version shipped with the agent. Note that passing the flag also mounts the local version of the check. <p>More about the base package: https://datadoghq.dev/integrations-core/base/about/ | <code>False</code> | | <code>--env-vars</code>, <code>-e</code> | text | ENV Variable that should be passed to the Agent container. Ex: -e DD_URL=app.datadoghq.com -e DD_API_KEY=123456 | None | | <code>--new-env</code>, <code>-ne</code> | boolean | Execute setup and tear down actions | <code>False</code> | | <code>--profile-memory</code>, <code>-pm</code> | boolean | Whether to collect metrics about memory usage | <code>False</code> | | <code>--junit</code>, <code>-j</code> | boolean | Generate junit reports | <code>False</code> | | <code>--ddtrace</code> | boolean | Run tests using dd-trace-py | <code>False</code> | | <code>--filter</code>, <code>-k</code> | text | Only run tests matching given substring expression | None | | <code>--changed</code> | boolean | Only test changed checks | <code>False</code> | | <code>--debug</code>, <code>-d</code> | boolean | Set the log level to debug | <code>False</code> | | <code>--skip-failed-environments</code>, <code>-sfe</code> | boolean | Skip environments that failed to start and continue | <code>False</code> | | <code>--help</code> | boolean | Show this message and exit. | <code>False</code> |</p>"},{"location":"ddev/cli/#ddev-meta","title":"ddev meta","text":"<p>Anything here should be considered experimental.</p> <p>This <code>meta</code> namespace can be used for an arbitrary number of niche or beta features without bloating the root namespace.</p> <p>Usage:</p> <pre><code>ddev meta [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-catalog","title":"ddev meta catalog","text":"<p>Create a catalog with information about integrations</p> <p>Usage:</p> <pre><code>ddev meta catalog [OPTIONS] CHECKS...\n</code></pre> <p>Options:</p> Name Type Description Default <code>-f</code>, <code>--file</code> text Output to file (it will be overwritten), you can pass \"tmp\" to generate a temporary file None <code>--markdown</code>, <code>-m</code> boolean Output to markdown instead of CSV <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-changes","title":"ddev meta changes","text":"<p>Show changes since a specific date.</p> <p>Usage:</p> <pre><code>ddev meta changes [OPTIONS] SINCE\n</code></pre> <p>Options:</p> Name Type Description Default <code>--out</code>, <code>-o</code> boolean Output to file <code>False</code> <code>--eager</code> boolean Skip validation of commit subjects <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-create-example-commits","title":"ddev meta create-example-commits","text":"<p>Create branch commits from example repo</p> <p>Usage:</p> <pre><code>ddev meta create-example-commits [OPTIONS] SOURCE_DIR\n</code></pre> <p>Options:</p> Name Type Description Default <code>--prefix</code>, <code>-p</code> text Optional text to prefix each commit `` <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-dash","title":"ddev meta dash","text":"<p>Dashboard utilities</p> <p>Usage:</p> <pre><code>ddev meta dash [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-dash-export","title":"ddev meta dash export","text":"<p>Export a Dashboard as JSON</p> <p>Usage:</p> <pre><code>ddev meta dash export [OPTIONS] URL INTEGRATION\n</code></pre> <p>Options:</p> Name Type Description Default <code>--author</code>, <code>-a</code> text The owner of this integration's dashboard. Default is 'Datadog' <code>Datadog</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-jmx","title":"ddev meta jmx","text":"<p>JMX utilities</p> <p>Usage:</p> <pre><code>ddev meta jmx [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-jmx-query-endpoint","title":"ddev meta jmx query-endpoint","text":"<p>Query endpoint for JMX info</p> <p>Usage:</p> <pre><code>ddev meta jmx query-endpoint [OPTIONS] HOST PORT [DOMAIN]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-manifest","title":"ddev meta manifest","text":"<p>Manifest utilities</p> <p>Usage:</p> <pre><code>ddev meta manifest [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-manifest-migrate","title":"ddev meta manifest migrate","text":"<p>Helper tool to ease the migration of a manifest to a newer version, auto-filling fields when possible</p> <p>Inputs:</p> <p>integration: The name of the integration folder to perform the migration on</p> <p>to_version: The schema version to upgrade the manifest to</p> <p>Usage:</p> <pre><code>ddev meta manifest migrate [OPTIONS] INTEGRATION TO_VERSION\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-prom","title":"ddev meta prom","text":"<p>Prometheus utilities</p> <p>Usage:</p> <pre><code>ddev meta prom [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-prom-info","title":"ddev meta prom info","text":"<p>Show metric info from a Prometheus endpoint.</p> <p>Example: <code>$ ddev meta prom info -e :8080/_status/vars</code></p> <p>Usage:</p> <pre><code>ddev meta prom info [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>-e</code>, <code>--endpoint</code> text N/A None <code>-f</code>, <code>--file</code> filename N/A None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-prom-parse","title":"ddev meta prom parse","text":"<p>Interactively parse metric info from a Prometheus endpoint and write it to metadata.csv.</p> <p>Usage:</p> <pre><code>ddev meta prom parse [OPTIONS] CHECK\n</code></pre> <p>Options:</p> Name Type Description Default <code>-e</code>, <code>--endpoint</code> text N/A None <code>-f</code>, <code>--file</code> filename N/A None <code>--here</code>, <code>-x</code> boolean Output to the current location <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-scripts","title":"ddev meta scripts","text":"<p>Miscellaneous scripts that may be useful.</p> <p>Usage:</p> <pre><code>ddev meta scripts [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-scripts-email2ghuser","title":"ddev meta scripts email2ghuser","text":"<p>Given an email, attempt to find a Github username    associated with the email.</p> <p><code>$ ddev meta scripts email2ghuser example@datadoghq.com</code></p> <p>Usage:</p> <pre><code>ddev meta scripts email2ghuser [OPTIONS] EMAIL\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-scripts-metrics2md","title":"ddev meta scripts metrics2md","text":"<p>Convert a check's metadata.csv file to a Markdown table, which will be copied to your clipboard.</p> <p>By default it will be compact and only contain the most useful fields. If you wish to use arbitrary metric data, you may set the check to <code>cb</code> to target the current contents of your clipboard.</p> <p>Usage:</p> <pre><code>ddev meta scripts metrics2md [OPTIONS] CHECK [FIELDS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-scripts-remove-labels","title":"ddev meta scripts remove-labels","text":"<p>Remove all labels from an issue or pull request. This is useful when there are too many labels and its state cannot be modified (known GitHub issue).</p> <p><code>$ ddev meta scripts remove-labels 5626</code></p> <p>Usage:</p> <pre><code>ddev meta scripts remove-labels [OPTIONS] ISSUE_NUMBER\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-scripts-upgrade-python","title":"ddev meta scripts upgrade-python","text":"<p>Upgrade the Python version of all test environments.</p> <p><code>$ ddev meta scripts upgrade-python 3.11</code></p> <p>Usage:</p> <pre><code>ddev meta scripts upgrade-python [OPTIONS] VERSION\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-snmp","title":"ddev meta snmp","text":"<p>SNMP utilities</p> <p>Usage:</p> <pre><code>ddev meta snmp [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-snmp-generate-profile-from-mibs","title":"ddev meta snmp generate-profile-from-mibs","text":"<p>Generate an SNMP profile from MIBs. Accepts a directory path containing mib files to be used as source to generate the profile, along with a filter if a device or family of devices support only a subset of oids from a mib.</p> <p>filters is the path to a yaml file containing a collection of MIBs, with their list of MIB node names to be included. For example: <pre><code>RFC1213-MIB:\n- system\n- interfaces\n- ip\nCISCO-SYSLOG-MIB: []\nSNMP-FRAMEWORK-MIB:\n- snmpEngine\n</code></pre> Note that each <code>MIB:node_name</code> correspond to exactly one and only one OID. However, some MIBs report legacy nodes that are overwritten.</p> <p>To resolve, edit the MIB by removing legacy values manually before loading them with this profile generator. If a MIB is fully supported, it can be omitted from the filter as MIBs not found in a filter will be fully loaded. If a MIB is not fully supported, it can be listed with an empty node list, as <code>CISCO-SYSLOG-MIB</code> in the example.</p> <p><code>-a, --aliases</code> is an option to provide the path to a YAML file containing a list of aliases to be used as metric tags for tables, in the following format: <pre><code>aliases:\n- from:\nMIB: ENTITY-MIB\nname: entPhysicalIndex\nto:\nMIB: ENTITY-MIB\nname: entPhysicalName\n</code></pre> MIBs tables most of the time define a column OID within the table, or from a different table and even different MIB, which value can be used to index entries. This is the <code>INDEX</code> field in row nodes. As an example, entPhysicalContainsTable in ENTITY-MIB <pre><code>entPhysicalContainsEntry OBJECT-TYPE\nSYNTAX      EntPhysicalContainsEntry\nMAX-ACCESS  not-accessible\nSTATUS      current\nDESCRIPTION\n        \"A single container/'containee' relationship.\"\nINDEX       { entPhysicalIndex, entPhysicalChildIndex }\n::= { entPhysicalContainsTable 1 }\n</code></pre> or its json dump, where <code>INDEX</code> is replaced by indices <pre><code>\"entPhysicalContainsEntry\": {\n\"name\": \"entPhysicalContainsEntry\",\n\"oid\": \"1.3.6.1.2.1.47.1.3.3.1\",\n\"nodetype\": \"row\",\n\"class\": \"objecttype\",\n\"maxaccess\": \"not-accessible\",\n\"indices\": [\n{\n\"module\": \"ENTITY-MIB\",\n\"object\": \"entPhysicalIndex\",\n\"implied\": 0\n},\n{\n\"module\": \"ENTITY-MIB\",\n\"object\": \"entPhysicalChildIndex\",\n\"implied\": 0\n}\n],\n\"status\": \"current\",\n\"description\": \"A single container/'containee' relationship.\"\n},\n</code></pre> Sometimes indexes are columns from another table, and we might want to use another column as it could have more human readable information - we might prefer to see the interface name vs its numerical table index. This can be achieved using metric_tag_aliases</p> <p>Return a list of SNMP metrics and copy its yaml dump to the clipboard Metric tags need to be added manually</p> <p>Usage:</p> <pre><code>ddev meta snmp generate-profile-from-mibs [OPTIONS] [MIB_FILES]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>-f</code>, <code>--filters</code> text Path to OIDs filter None <code>-a</code>, <code>--aliases</code> text Path to metric tag aliases None <code>--debug</code>, <code>-d</code> boolean Include debug output <code>False</code> <code>--interactive</code>, <code>-i</code> boolean Prompt to confirm before saving to a file <code>False</code> <code>--source</code>, <code>-s</code> text Source of the MIBs files. Can be a url or a path for a directory <code>https://raw.githubusercontent.com:443/DataDog/mibs.snmplabs.com/master/asn1/@mib@</code> <code>--compiled_mibs_path</code>, <code>-c</code> text Source of compiled MIBs files. Can be a url or a path for a directory <code>https://raw.githubusercontent.com/DataDog/mibs.snmplabs.com/master/json/@mib@</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-snmp-generate-traps-db","title":"ddev meta snmp generate-traps-db","text":"<p>Generate yaml or json formatted documents containing various information about traps. These files can be used by the Datadog Agent to enrich trap data. This command is intended for \"Network Devices Monitoring\" users who need to enrich traps that are not automatically supported by Datadog.</p> <p>The expected workflow is as such:</p> <p>1- Identify a type of device that is sending traps that Datadog does not already recognize.</p> <p>2- Fetch all the MIBs that Datadog does not support.</p> <p>3- Run <code>ddev meta snmp generate-traps-db -o ./output_dir/ /path/to/my/mib1 /path/to/my/mib2</code></p> <p>You'll need to install pysmi manually beforehand.</p> <p>Usage:</p> <pre><code>ddev meta snmp generate-traps-db [OPTIONS] MIB_FILES...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--mib-sources</code>, <code>-s</code> text Url or a path to a directory containing the dependencies for [mib_files...].Traps defined in these files are ignored. None <code>--output-dir</code>, <code>-o</code> directory Path to a directory where to store the created traps database file per MIB.Recommended option, do not use with --output-file None <code>--output-file</code> file Path to a file to store a compacted version of the traps database file. Do not use with --output-dir None <code>--output-format</code> choice (<code>yaml</code> | <code>json</code>) Use json instead of yaml for the output file(s). <code>yaml</code> <code>--no-descr</code> boolean Removes descriptions from the generated file(s) when set (more compact). <code>False</code> <code>--debug</code>, <code>-d</code> boolean Include debug output <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-snmp-translate-profile","title":"ddev meta snmp translate-profile","text":"<p>Do OID translation in a SNMP profile. This isn't a plain replacement, as it doesn't preserve comments and indent, but it should automate most of the work.</p> <p>You'll need to install pysnmp and pysnmp-mibs manually beforehand.</p> <p>Usage:</p> <pre><code>ddev meta snmp translate-profile [OPTIONS] PROFILE_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--mib_source_url</code> text Source url to fetch missing MIBS <code>https://raw.githubusercontent.com:443/DataDog/mibs.snmplabs.com/master/asn1/@mib@</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-snmp-validate-mib-filenames","title":"ddev meta snmp validate-mib-filenames","text":"<p>Validate MIB file names. Frameworks used to load mib files expect MIB file names to match MIB name.</p> <p>Usage:</p> <pre><code>ddev meta snmp validate-mib-filenames [OPTIONS] [MIB_FILES]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--interactive</code>, <code>-i</code> boolean Prompt to confirm before renaming all invalid MIB files <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-snmp-validate-profile","title":"ddev meta snmp validate-profile","text":"<p>Validate SNMP profiles</p> <p>Usage:</p> <pre><code>ddev meta snmp validate-profile [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>-f</code>, <code>--file</code> text Path to a profile file to validate None <code>-d</code>, <code>--directory</code> text Path to a directory of profiles to validate None <code>-v</code>, <code>--verbose</code> boolean Increase verbosity of error messages <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-windows","title":"ddev meta windows","text":"<p>Windows utilities</p> <p>Usage:</p> <pre><code>ddev meta windows [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-windows-pdh","title":"ddev meta windows pdh","text":"<p>PDH utilities</p> <p>Usage:</p> <pre><code>ddev meta windows pdh [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-meta-windows-pdh-browse","title":"ddev meta windows pdh browse","text":"<p>Explore performance counters.</p> <p>You'll need to install pywin32 manually beforehand.</p> <p>Usage:</p> <pre><code>ddev meta windows pdh browse [OPTIONS] [COUNTERSET]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release","title":"ddev release","text":"<p>Manage the release of integrations.</p> <p>Usage:</p> <pre><code>ddev release [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-agent","title":"ddev release agent","text":"<p>A collection of tasks related to the Datadog Agent.</p> <p>Usage:</p> <pre><code>ddev release agent [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-agent-changelog","title":"ddev release agent changelog","text":"<p>Generates a markdown file containing the list of checks that changed for a given Agent release. Agent version numbers are derived inspecting tags on <code>integrations-core</code> so running this tool might provide unexpected results if the repo is not up to date with the Agent release process.</p> <p>If neither <code>--since</code> or <code>--to</code> are passed (the most common use case), the tool will generate the whole changelog since Agent version 6.3.0 (before that point we don't have enough information to build the log).</p> <p>Usage:</p> <pre><code>ddev release agent changelog [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--since</code> text Initial Agent version <code>6.3.0</code> <code>--to</code> text Final Agent version None <code>--write</code>, <code>-w</code> boolean Write to the changelog file, if omitted contents will be printed to stdout <code>False</code> <code>--force</code>, <code>-f</code> boolean Replace an existing file <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-agent-integrations","title":"ddev release agent integrations","text":"<p>Generates a markdown file containing the list of integrations shipped in a given Agent release. Agent version numbers are derived by inspecting tags on <code>integrations-core</code>, so running this tool might provide unexpected results if the repo is not up to date with the Agent release process.</p> <p>If neither <code>--since</code> nor <code>--to</code> are passed (the most common use case), the tool will generate the list for every Agent since version 6.3.0 (before that point we don't have enough information to build the log).</p> <p>Usage:</p> <pre><code>ddev release agent integrations [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--since</code> text Initial Agent version <code>6.3.0</code> <code>--to</code> text Final Agent version None <code>--write</code>, <code>-w</code> boolean Write to file, if omitted contents will be printed to stdout <code>False</code> <code>--force</code>, <code>-f</code> boolean Replace an existing file <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-agent-integrations-changelog","title":"ddev release agent integrations-changelog","text":"<p>Update integration CHANGELOG.md by adding the Agent version.</p> <p>Agent version is only added to the integration versions released with a specific Agent release.</p> <p>Usage:</p> <pre><code>ddev release agent integrations-changelog [OPTIONS] [INTEGRATIONS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--since</code> text Initial Agent version <code>6.3.0</code> <code>--to</code> text Final Agent version None <code>--write</code>, <code>-w</code> boolean Write to the changelog file, if omitted contents will be printed to stdout <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-build","title":"ddev release build","text":"<p>Build a wheel for a check as it is on the repo HEAD</p> <p>Usage:</p> <pre><code>ddev release build [OPTIONS] CHECK\n</code></pre> <p>Options:</p> Name Type Description Default <code>--sdist</code>, <code>-s</code> boolean N/A <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-changelog","title":"ddev release changelog","text":"<p>Manage changelogs.</p> <p>Usage:</p> <pre><code>ddev release changelog [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-changelog-fix","title":"ddev release changelog fix","text":"<p>The first line of every new changelog entry must include the PR number in which the change occurred. This command will apply this suffix to manually added entries if it is missing.</p> <p>Usage:</p> <pre><code>ddev release changelog fix [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-changelog-new","title":"ddev release changelog new","text":"<p>This creates new changelog entries. If the entry type is not specified, you will be prompted.</p> <p>The <code>--message</code> option can be used to specify the changelog text. If this is not supplied, an editor will be opened for you to manually write the entry. The changelog text that is opened defaults to the PR title, followed by the most recent commit subject. If that is sufficient, then you may close the editor tab immediately.</p> <p>By default, changelog entries will be created for all integrations that have changed code. To create entries only for specific targets, you may pass them as additional arguments after the entry type.</p> <p>Usage:</p> <pre><code>ddev release changelog new [OPTIONS] [ENTRY_TYPE] [TARGETS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--message</code>, <code>-m</code> text The changelog text None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-list","title":"ddev release list","text":"<p>Show all versions of an integration.</p> <p>Usage:</p> <pre><code>ddev release list [OPTIONS] INTEGRATION\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-make","title":"ddev release make","text":"<p>Perform a set of operations needed to release checks:</p> <ul> <li>update the version in <code>__about__.py</code></li> <li>update the changelog</li> <li>update the <code>requirements-agent-release.txt</code> file</li> <li>update in-toto metadata</li> <li>commit the above changes</li> </ul> <p>You can release everything at once by setting the check to <code>all</code>.</p> <p>If you run into issues signing:   - Ensure you did <code>gpg --import &lt;YOUR_KEY_ID&gt;.gpg.pub</code></p> <p>Usage:</p> <pre><code>ddev release make [OPTIONS] CHECKS...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--version</code> text N/A None <code>--end</code> text N/A None <code>--new</code> boolean Ensure versions are at 1.0.0 <code>False</code> <code>--skip-sign</code> boolean Skip the signing of release metadata <code>False</code> <code>--sign-only</code> boolean Only sign release metadata <code>False</code> <code>--exclude</code> text Comma-separated list of checks to skip None <code>--allow-master</code> boolean Allow ddev to commit directly to master. Forbidden for core. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-show","title":"ddev release show","text":"<p>To avoid GitHub's public API rate limits, you need to set <code>github.user</code>/<code>github.token</code> in your config file or use the <code>DD_GITHUB_USER</code>/<code>DD_GITHUB_TOKEN</code> environment variables.</p> <p>Usage:</p> <pre><code>ddev release show [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-show-changes","title":"ddev release show changes","text":"<p>Show all the pending PRs for a given check.</p> <p>Usage:</p> <pre><code>ddev release show changes [OPTIONS] CHECK\n</code></pre> <p>Options:</p> Name Type Description Default <code>--organization</code>, <code>-r</code> text The Github organization the repository belongs to <code>DataDog</code> <code>--tag-pattern</code> text The regex pattern for the format of the tag. Required if the tag doesn't follow semver None <code>--tag-prefix</code> text Specify the prefix of the tag to use if the tag doesn't follow semver None <code>--dry-run</code>, <code>-n</code> boolean Run the command in dry-run mode <code>False</code> <code>--since</code> text The git ref to use instead of auto-detecting the tag to view changes since None <code>--end</code> text N/A None <code>--exclude-branch</code> text Exclude changes comming from a specific branch None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-show-ready","title":"ddev release show ready","text":"<p>Show all the checks that can be released.</p> <p>Usage:</p> <pre><code>ddev release show ready [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--quiet</code>, <code>-q</code> boolean N/A <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-stats","title":"ddev release stats","text":"<p>A collection of tasks to generate reports about releases.</p> <p>Usage:</p> <pre><code>ddev release stats [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-stats-merged-prs","title":"ddev release stats merged-prs","text":"<p>Prints the PRs merged between the first RC and the current RC/final build</p> <p>Usage:</p> <pre><code>ddev release stats merged-prs [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--from-ref</code>, <code>-f</code> text Reference to start stats on (first RC tagged) _required <code>--to-ref</code>, <code>-t</code> text Reference to end stats at (current RC/final tag) _required <code>--release-milestone</code>, <code>-r</code> text Github release milestone _required <code>--exclude-releases</code>, <code>-e</code> boolean Flag to exclude the release PRs from the list <code>False</code> <code>--export-csv</code> text CSV file where the list will be exported None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-stats-report","title":"ddev release stats report","text":"<p>Prints some release stats we want to track</p> <p>Usage:</p> <pre><code>ddev release stats report [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--from-ref</code>, <code>-f</code> text Reference to start stats on (first RC tagged) _required <code>--to-ref</code>, <code>-t</code> text Reference to end stats at (current RC/final tag) _required <code>--release-milestone</code>, <code>-r</code> text Github release milestone _required <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-tag","title":"ddev release tag","text":"<p>Tag the HEAD of the git repo with the current release number for a specific check. The tag is pushed to origin by default.</p> <p>You can tag everything at once by setting the check to <code>all</code>.</p> <p>Notice: specifying a different version than the one in <code>__about__.py</code> is a maintenance task that should be run under very specific circumstances (e.g. re-align an old release performed on the wrong commit).</p> <p>Usage:</p> <pre><code>ddev release tag [OPTIONS] CHECK [VERSION]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--push</code> / <code>--no-push</code> boolean N/A <code>True</code> <code>--dry-run</code>, <code>-n</code> boolean N/A <code>False</code> <code>--skip-prerelease</code> boolean N/A <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-trello","title":"ddev release trello","text":"<p>Subcommands for interacting with Trello Release boards.</p> <p>To use Trello: 1. Go to <code>https://trello.com/app-key</code> and copy your API key. 2. Run <code>ddev config set trello.key</code> and paste your API key. 3. Go to <code>https://trello.com/1/authorize?key=key&amp;name=name&amp;scope=read,write&amp;expiration=never&amp;response_type=token</code>,    where <code>key</code> is your API key and <code>name</code> is the name to give your token, e.g. ReleaseTestingYourName.    Authorize access and copy your token. 4. Run <code>ddev config set trello.token</code> and paste your token.</p> <p>Usage:</p> <pre><code>ddev release trello [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-trello-status","title":"ddev release trello status","text":"<p>Print tabular status of Agent Release based on Trello columns.</p> <p>See trello subcommand for details on how to setup access:</p> <p><code>ddev release trello -h</code>.</p> <p>Usage:</p> <pre><code>ddev release trello status [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--verbose</code>, <code>-v</code> boolean Return the detailed results instead of the aggregates <code>False</code> <code>--json</code>, <code>-j</code> boolean Return as raw JSON instead <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-trello-testable","title":"ddev release trello testable","text":"<p>Create a Trello card for changes since a previous release (referenced by <code>BASE_REF</code>) that need to be tested for the next release (referenced by <code>TARGET_REF</code>).</p> <p><code>BASE_REF</code> and <code>TARGET_REF</code> can be any valid git references. It practice, you should use either:</p> <ul> <li> <p>A tag: <code>7.16.1</code>, <code>7.17.0-rc.4</code>, ...</p> </li> <li> <p>A release branch: <code>6.16.x</code>, <code>7.17.x</code>, ...</p> </li> <li> <p>The <code>master</code> branch.</p> </li> </ul> <p>NOTE: using a minor version shorthand (e.g. <code>7.16</code>) is not supported, as it is ambiguous.</p> <p>Example: assuming we are working on the release of 7.17.0, we can...</p> <ul> <li> <p>Create cards for changes between a previous Agent release and <code>master</code> (useful when preparing an initial RC):</p> <p><code>$ ddev release trello testable 7.16.1 origin/master</code></p> </li> <li> <p>Create cards for changes between a previous RC and <code>master</code> (useful when preparing a new RC, and a separate release branch was not created yet):</p> <p><code>$ ddev release trello testable 7.17.0-rc.2 origin/master</code></p> </li> <li> <p>Create cards for changes between a previous RC and a release branch (useful to only review changes in a release branch that has diverged from <code>master</code>):</p> <p><code>$ ddev release trello testable 7.17.0-rc.4 7.17.x</code></p> </li> <li> <p>Create cards for changes between two arbitrary tags, e.g. between RCs:</p> <p><code>$ ddev release trello testable 7.17.0-rc.4 7.17.0-rc.5</code></p> </li> </ul> <p>TIP: run with <code>ddev -x release trello testable</code> to force the use of the current directory. To avoid GitHub's public API rate limits, you need to set <code>github.user</code>/<code>github.token</code> in your config file or use the <code>DD_GITHUB_USER</code>/<code>DD_GITHUB_TOKEN</code> environment variables.</p> <p>See trello subcommand for details on how to setup access:</p> <p><code>ddev release trello -h</code>.</p> <p>Usage:</p> <pre><code>ddev release trello testable [OPTIONS] BASE_REF TARGET_REF\n</code></pre> <p>Options:</p> Name Type Description Default <code>--milestone</code> text The PR milestone to filter by None <code>--dry-run</code>, <code>-n</code> boolean Only show the changes <code>False</code> <code>--update-rc-builds-cards</code> boolean Update cards in RC builds column with <code>target_ref</code> version <code>False</code> <code>--move-cards</code> boolean Do not create a card for a change, but move the existing card from <code>HAVE BUGS - FIXME</code> or <code>FIXED - Ready to Rebuild</code> to INBOX team <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-trello-update-rc-links","title":"ddev release trello update-rc-links","text":"<p>Update links to RCs in the QA board Trello cards</p> <p>Usage:</p> <pre><code>ddev release trello update-rc-links [OPTIONS] TARGET_REF\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-release-upload","title":"ddev release upload","text":"<p>Release a specific check to PyPI as it is on the repo HEAD.</p> <p>Usage:</p> <pre><code>ddev release upload [OPTIONS] CHECK\n</code></pre> <p>Options:</p> Name Type Description Default <code>--sdist</code>, <code>-s</code> boolean N/A <code>False</code> <code>--dry-run</code>, <code>-n</code> boolean N/A <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-run","title":"ddev run","text":"<p>Run commands in the proper repo.</p> <p>Usage:</p> <pre><code>ddev run [OPTIONS] [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-status","title":"ddev status","text":"<p>Show information about the current environment.</p> <p>Usage:</p> <pre><code>ddev status [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-test","title":"ddev test","text":"<p>Run tests.</p> <p>Usage:</p> <pre><code>ddev test [OPTIONS] [TARGET_SPEC] [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--lint</code>, <code>-s</code> boolean Run only lint &amp; style checks <code>False</code> <code>--fmt</code>, <code>-fs</code> boolean Run only the code formatter <code>False</code> <code>--bench</code>, <code>-b</code> boolean Run only benchmarks <code>False</code> <code>--latest</code> boolean Only verify support of new product versions <code>False</code> <code>--cov</code>, <code>-c</code> boolean Measure code coverage <code>False</code> <code>--compat</code> boolean Check compatibility with the minimum allowed Agent version <code>False</code> <code>--ddtrace</code> boolean Enable tracing during test execution <code>False</code> <code>--memray</code> boolean Measure memory usage during test execution <code>False</code> <code>--recreate</code>, <code>-r</code> boolean Recreate environments from scratch <code>False</code> <code>--list</code>, <code>-l</code> boolean Show available test environments <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate","title":"ddev validate","text":"<p>Verify certain aspects of the repo.</p> <p>Usage:</p> <pre><code>ddev validate [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-agent-reqs","title":"ddev validate agent-reqs","text":"<p>Verify that the checks versions are in sync with the requirements-agent-release.txt file.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate agent-reqs [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-all","title":"ddev validate all","text":"<p>Run all CI validations for a repo.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate all [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-ci","title":"ddev validate ci","text":"<p>Validate CI infrastructure configuration.</p> <p>Usage:</p> <pre><code>ddev validate ci [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--sync</code> boolean Update the CI configuration <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-codeowners","title":"ddev validate codeowners","text":"<p>Validate that every integration has an entry in the <code>CODEOWNERS</code> file.</p> <p>Usage:</p> <pre><code>ddev validate codeowners [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-config","title":"ddev validate config","text":"<p>Validate default configuration files.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate config [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--sync</code>, <code>-s</code> boolean Generate example configuration files based on specifications <code>False</code> <code>--verbose</code>, <code>-v</code> boolean Verbose mode <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-dashboards","title":"ddev validate dashboards","text":"<p>Validate all Dashboard definition files.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate dashboards [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--fix</code> boolean Attempt to fix errors <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-dep","title":"ddev validate dep","text":"<p>This command will:</p> <ul> <li>Verify the uniqueness of dependency versions across all checks, or optionally a single check</li> <li>Verify all the dependencies are pinned.</li> <li>Verify the embedded Python environment defined in the base check and requirements   listed in every integration are compatible.</li> <li>Verify each check specifies a <code>CHECKS_BASE_REQ</code> variable for <code>datadog-checks-base</code> requirement</li> <li>Optionally verify that the <code>datadog-checks-base</code> requirement is lower-bounded</li> <li>Optionally verify that the <code>datadog-checks-base</code> requirement satisfies specific version</li> </ul> <p>Usage:</p> <pre><code>ddev validate dep [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--require-base-check-version</code> boolean Require specific version for datadog-checks-base requirement <code>False</code> <code>--min-base-check-version</code> text Specify minimum version for datadog-checks-base requirement, e.g. <code>11.0.0</code> None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-eula","title":"ddev validate eula","text":"<p>Validate all EULA definition files.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate eula [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-http","title":"ddev validate http","text":"<p>Validate all integrations for usage of HTTP wrapper.</p> <p>If <code>integrations</code> is specified, only those will be validated, an 'all' <code>check</code> value will validate all checks.</p> <p>Usage:</p> <pre><code>ddev validate http [OPTIONS] [INTEGRATIONS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-imports","title":"ddev validate imports","text":"<p>Validate proper imports in checks.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate imports [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--autofix</code> boolean Apply suggested fix <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-integration-style","title":"ddev validate integration-style","text":"<p>Validate that check follows style guidelines.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate integration-style [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--verbose</code>, <code>-v</code> boolean Verbose mode <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-jmx-metrics","title":"ddev validate jmx-metrics","text":"<p>Validate all default JMX metrics definitions.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate jmx-metrics [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--verbose</code>, <code>-v</code> boolean Verbose mode <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-legacy-signature","title":"ddev validate legacy-signature","text":"<p>Validate that no integration uses the legacy signature.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate legacy-signature [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-license-headers","title":"ddev validate license-headers","text":"<p>Validate license headers in python code files.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all python files.</p> <p>Usage:</p> <pre><code>ddev validate license-headers [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--fix</code> boolean Attempt to fix errors <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-licenses","title":"ddev validate licenses","text":"<p>Validate third-party license list</p> <p>Usage:</p> <pre><code>ddev validate licenses [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--sync</code>, <code>-s</code> boolean Generate the <code>LICENSE-3rdparty.csv</code> file <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-manifest","title":"ddev validate manifest","text":"<p>Validate integration manifests.</p> <p>Usage:</p> <pre><code>ddev validate manifest [OPTIONS] [INTEGRATIONS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-metadata","title":"ddev validate metadata","text":"<p>Validate <code>metadata.csv</code> files</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate metadata [OPTIONS] [INTEGRATIONS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--check-duplicates</code> boolean Output warnings if there are duplicate short names and descriptions <code>False</code> <code>--show-warnings</code>, <code>-w</code> boolean Show warnings in addition to failures <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-models","title":"ddev validate models","text":"<p>Validate configuration data models.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate models [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--sync</code>, <code>-s</code> boolean Generate data models based on specifications <code>False</code> <code>--verbose</code>, <code>-v</code> boolean Verbose mode <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-openmetrics","title":"ddev validate openmetrics","text":"<p>Validate OpenMetrics metric limit.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate nothing.</p> <p>Usage:</p> <pre><code>ddev validate openmetrics [OPTIONS] [INTEGRATIONS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-package","title":"ddev validate package","text":"<p>Validate all files for Python package metadata.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all files.</p> <p>Usage:</p> <pre><code>ddev validate package [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-readmes","title":"ddev validate readmes","text":"<p>Validates README files.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate readmes [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--format-links</code>, <code>-fl</code> boolean Automatically format links <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-saved-views","title":"ddev validate saved-views","text":"<p>Validates saved view files</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all saved view files.</p> <p>Usage:</p> <pre><code>ddev validate saved-views [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-service-checks","title":"ddev validate service-checks","text":"<p>Validate all <code>service_checks.json</code> files.</p> <p>If <code>check</code> is specified, only the check will be validated, if check value is 'changed' will only apply to changed checks, an 'all' or empty <code>check</code> value will validate all README files.</p> <p>Usage:</p> <pre><code>ddev validate service-checks [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--sync</code> boolean Generate example configuration files based on specifications <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/cli/#ddev-validate-typos","title":"ddev validate typos","text":"<p>Validate spelling in the source code.</p> <p>If <code>check</code> is specified, only the directory is validated. Use codespell command line tool to detect spelling errors.</p> <p>Usage:</p> <pre><code>ddev validate typos [OPTIONS] [CHECK]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--fix</code> boolean Apply suggested fix <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"ddev/configuration/","title":"Configuration","text":"<p>All configuration can be managed entirely by the <code>ddev config</code> command group. To locate the TOML config file, run:</p> <pre><code>ddev config find\n</code></pre>"},{"location":"ddev/configuration/#repository","title":"Repository","text":"<p>All CLI commands are aware of the current repository context, defined by the option <code>repo</code>. This option should be a reference to a key in <code>repos</code> which is set to the path of a supported repository. For example, this configuration:</p> <pre><code>repo = \"core\"\n\n[repos]\ncore = \"/path/to/integrations-core\"\nextras = \"/path/to/integrations-extras\"\nagent = \"/path/to/datadog-agent\"\n</code></pre> <p>would make it so running e.g. <code>ddev test nginx</code> will look for an integration named <code>nginx</code> in <code>/path/to/integrations-core</code> no matter what directory you are in. If the selected path does not exist, then the current directory will be used.</p> <p>By default, <code>repo</code> is set to <code>core</code>.</p>"},{"location":"ddev/configuration/#agent","title":"Agent","text":"<p>For running environments with a live Agent, you can select a specific build version to use with the option <code>agent</code>. This option should be a reference to a key in <code>agents</code> which is a mapping of environment types to Agent versions. For example, this configuration:</p> <pre><code>agent = \"master\"\n\n[agents.master]\ndocker = \"datadog/agent-dev:master\"\nlocal = \"latest\"\n\n[agents.\"7.18.1\"]\ndocker = \"datadog/agent:7.18.1\"\nlocal = \"7.18.1\"\n</code></pre> <p>would make it so environments that define the type as <code>docker</code> will use the Docker image that was built with the latest commit to the datadog-agent repo.</p>"},{"location":"ddev/configuration/#organization","title":"Organization","text":"<p>You can switch to using a particular organization with the option <code>org</code>. This option should be a reference to a key in <code>orgs</code> which is a mapping containing data specific to the organization. For example, this configuration:</p> <pre><code>org = \"staging\"\n\n[orgs.staging]\napi_key = \"&lt;API_KEY&gt;\"\napp_key = \"&lt;APP_KEY&gt;\"\nsite = \"datadoghq.eu\"\n</code></pre> <p>would use the access keys for the organization named <code>staging</code> and would submit data to the EU region.</p> <p>The supported fields are:</p> <ul> <li>api_key</li> <li>app_key</li> <li>site</li> <li>dd_url</li> <li>log_url</li> </ul>"},{"location":"ddev/configuration/#github","title":"GitHub","text":"<p>To avoid GitHub's public API rate limits, you need to set <code>github.user</code>/<code>github.token</code> in your config file or use the <code>DD_GITHUB_USER</code>/<code>DD_GITHUB_TOKEN</code> environment variables.</p> <p>Run <code>ddev config show</code> to see if your GitHub user and token is set.</p> <p>If not:</p> <ol> <li>Run <code>ddev config set github.user &lt;YOUR_GITHUB_USERNAME&gt;</code></li> <li>Create a personal access token with <code>public_repo</code> and <code>read:org</code> permissions</li> <li>Run <code>ddev config set github.token</code> then paste the token</li> <li>Enable single sign-on for the token</li> </ol>"},{"location":"ddev/plugins/","title":"Plugins","text":""},{"location":"ddev/plugins/#tox","title":"tox","text":"<p>Our tox plugin dynamically adds environments based on the presence of options defined in the <code>[testenv]</code> section of each integration's <code>tox.ini</code> file.</p>"},{"location":"ddev/plugins/#style","title":"Style","text":"<p>Setting <code>dd_check_style</code> to <code>true</code> will enable 2 environments for enforcing our style conventions:</p> <ol> <li><code>style</code> - This will check the formatting and will error if any issues are found. You may use the <code>-s/--style</code> flag    of <code>ddev test</code> to execute only this environment.</li> <li><code>format_style</code> - This will format the code for you, resolving the most common issues caught by <code>style</code> environment.    You can run the formatter by using the <code>-fs/--format-style</code> flag of <code>ddev test</code>.</li> </ol>"},{"location":"ddev/plugins/#pytest","title":"pytest","text":"<p>Our pytest plugin makes a few fixtures available globally for use during tests. Also, it's responsible for managing the control flow of E2E environments.</p>"},{"location":"ddev/plugins/#fixtures","title":"Fixtures","text":""},{"location":"ddev/plugins/#agent-stubs","title":"Agent stubs","text":"<p>The stubs provided by each fixture will automatically have their state reset before each test.</p> <ul> <li>aggregator</li> <li>datadog_agent</li> </ul>"},{"location":"ddev/plugins/#check-execution","title":"Check execution","text":"<p>Most tests will execute checks via the <code>run</code> method of the AgentCheck interface (if the check is stateful).</p> <p>A consequence of this is that, unlike the <code>check</code> method, exceptions are not propagated to the caller meaning not only can an exception not be asserted, but also errors are silently ignored.</p> <p>The <code>dd_run_check</code> fixture takes a check instance and executes it while also propagating any exceptions like normal.</p> <pre><code>def test_metrics(aggregator, dd_run_check):\n    check = AwesomeCheck('awesome', {}, [{'port': 8080}])\n    dd_run_check(check)\n    ...\n</code></pre> <p>You can use the <code>extract_message</code> option to condense any exception message to just the original message rather than the full traceback.</p> <pre><code>def test_config(dd_run_check):\n    check = AwesomeCheck('awesome', {}, [{'port': 'foo'}])\n\n    with pytest.raises(Exception, match='^Option `port` must be an integer$'):\n        dd_run_check(check, extract_message=True)\n</code></pre>"},{"location":"ddev/plugins/#e2e","title":"E2E","text":""},{"location":"ddev/plugins/#agent-check-runner","title":"Agent check runner","text":"<p>The <code>dd_agent_check</code> fixture will run the integration with a given configuration on a live Agent and return a populated aggregator. It accepts a single <code>dict</code> configuration representing either:</p> <ul> <li>a single instance</li> <li>a full configuration with top level keys <code>instances</code>, <code>init_config</code>, etc.</li> </ul> <p>Internally, this is a wrapper around <code>ddev env check</code> and you can pass through any supported options or flags.</p> <p>This fixture can only be used from tests marked as <code>e2e</code>. For example:</p> <pre><code>@pytest.mark.e2e\ndef test_e2e_metrics(dd_agent_check, instance):\n    aggregator = dd_agent_check(instance, rate=True)\n    ...\n</code></pre>"},{"location":"ddev/plugins/#state","title":"State","text":"<p>Occasionally, you will need to persist some data only known at the time of environment creation (like a generated token) through the test and environment tear down phases.</p> <p>To do so, use the following fixtures:</p> <ul> <li> <p><code>dd_save_state</code> - When executing the necessary steps to spin up an environment you may use this to save any   object that can be serialized to JSON. For example:</p> <pre><code>dd_save_state('my_data', {'foo': 'bar'})\n</code></pre> </li> <li> <p><code>dd_get_state</code> - This may be used to retrieve the data:</p> <pre><code>my_data = dd_get_state('my_data', default={})\n</code></pre> </li> </ul>"},{"location":"ddev/plugins/#mock-http-response","title":"Mock HTTP response","text":"<p>The <code>mock_http_response</code> fixture mocks HTTP requests for the lifetime of a test.</p> <p>The fixture can be used to mock the response of an endpoint. In the following example, we can mock the Prometheus output.</p> <pre><code>def test(mock_http_response):\n    mock_http_response(\n\"\"\"\n        # HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.\n        # TYPE go_memstats_alloc_bytes gauge\n        go_memstats_alloc_bytes 6.396288e+06\n        \"\"\"\n    )\n    ...\n</code></pre>"},{"location":"ddev/plugins/#environment-manager","title":"Environment manager","text":"<p>The fixture <code>dd_environment_runner</code> manages communication between environments and the <code>ddev env</code> command group. You will never use it directly as it runs automatically.</p> <p>It acts upon a fixture named <code>dd_environment</code> that every integration's test suite will define if E2E testing on a live Agent is desired. This fixture is responsible for starting and stopping environments and must adhere to the following requirements:</p> <ol> <li> <p>It <code>yield</code>s a single <code>dict</code> representing the default configuration the Agent will use. It must be either:</p> <ul> <li>a single instance</li> <li>a full configuration with top level keys <code>instances</code>, <code>init_config</code>, etc.</li> </ul> <p>Additionally, you can pass a second <code>dict</code> containing metadata.</p> </li> <li> <p>The setup logic must occur before the <code>yield</code> and the tear down logic must occur after it. Also, both steps must only    execute based on the value of environment variables.</p> <ul> <li>Setup - only if <code>DDEV_E2E_UP</code> is not set to <code>false</code></li> <li>Tear down - only if <code>DDEV_E2E_DOWN</code> is not set to <code>false</code></li> </ul> <p>Note</p> <p>The provided Docker and Terraform environment runner utilities will do this automatically for you.</p> </li> </ol>"},{"location":"ddev/plugins/#metadata","title":"Metadata","text":"<ul> <li><code>env_type</code> - This is the type of interface that will be used to interact with the Agent. Currently, we support <code>docker</code> (default) and <code>local</code>.</li> <li><code>env_vars</code> - A <code>dict</code> of environment variables and their values that will be present when starting the Agent.</li> <li><code>docker_volumes</code> - A <code>list</code> of <code>str</code> representing Docker volume mounts if <code>env_type</code> is <code>docker</code> e.g. <code>/local/path:/agent/container/path:ro</code>.</li> <li><code>docker_platform</code> - The container architecture to use if <code>env_type</code> is <code>docker</code>. Currently, we support <code>linux</code> (default) and <code>windows</code>.</li> <li><code>logs_config</code> - A <code>list</code> of configs that will be used by the Logs Agent. You will never need to use this directly, but rather via higher level abstractions.</li> </ul>"},{"location":"ddev/test/","title":"Test framework","text":""},{"location":"ddev/test/#environments","title":"Environments","text":"<p>Most integrations monitor services like databases or web servers, rather than system properties like CPU usage. For such cases, you'll want to spin up an environment and gracefully tear it down when tests finish.</p> <p>We define all environment actions in a fixture called <code>dd_environment</code> that looks semantically like this:</p> <pre><code>@pytest.fixture(scope='session')\ndef dd_environment():\n    try:\n        set_up_env()\n        yield some_default_config\n    finally:\n        tear_down_env()\n</code></pre> <p>This is not only used for regular tests, but is also the basis of our E2E testing. The start command executes everything before the <code>yield</code> and the stop command executes everything after it.</p> <p>We provide a few utilities for common environment types.</p>"},{"location":"ddev/test/#docker","title":"Docker","text":"<p>The <code>docker_run</code> utility makes it easy to create services using docker-compose.</p> <pre><code>from datadog_checks.dev import docker_run\n\n@pytest.fixture(scope='session')\ndef dd_environment():\n    with docker_run(os.path.join(HERE, 'docker', 'compose.yaml')):\n        yield ...\n</code></pre> <p>Read the reference for more information.</p>"},{"location":"ddev/test/#terraform","title":"Terraform","text":"<p>The <code>terraform_run</code> utility makes it easy to create services from a directory of Terraform files.</p> <pre><code>from datadog_checks.dev.terraform import terraform_run\n\n@pytest.fixture(scope='session')\ndef dd_environment():\n    with terraform_run(os.path.join(HERE, 'terraform')):\n        yield ...\n</code></pre> <p>Currently, we only use this for services that would be too complex to setup with Docker (like OpenStack) or things that cannot be provided by Docker (like vSphere). We provide some ready-to-use cloud templates that are available for referencing by default. We prefer using GCP when possible.</p> <p>Terraform E2E tests are not run in our public CI as that would needlessly slow down builds.</p> <p>Read the reference for more information.</p>"},{"location":"ddev/test/#mocker","title":"Mocker","text":"<p>The <code>mocker</code> fixture is provided by the pytest-mock plugin. This fixture automatically restores anything that was mocked at the end of each test and is more ergonomic to use than stacking decorators or nesting context managers.</p> <p>Here's an example from their docs:</p> <pre><code>def test_foo(mocker):\n    # all valid calls\n    mocker.patch('os.remove')\n    mocker.patch.object(os, 'listdir', autospec=True)\n    mocked_isfile = mocker.patch('os.path.isfile')\n</code></pre> <p>It also has many other nice features, like using <code>pytest</code> introspection when comparing calls.</p>"},{"location":"ddev/test/#benchmarks","title":"Benchmarks","text":"<p>The <code>benchmark</code> fixture is provided by the pytest-benchmark plugin. It enables the profiling of functions with the low-overhead cProfile module.</p> <p>It is quite useful for seeing the approximate time a given check takes to run, as well as gaining insight into any potential performance bottlenecks. You would use it like this:</p> <pre><code>def test_large_payload(benchmark, dd_run_check):\n    check = AwesomeCheck('awesome', {}, [instance])\n\n    # Run once to get any initialization out of the way.\n    dd_run_check(check)\n\n    benchmark(dd_run_check, check)\n</code></pre> <p>To add benchmarks, define environments in <code>tox.ini</code> with <code>bench</code> somewhere in their names:</p> <pre><code>[tox]\n...\nenvlist =\n...\nbench\n\n...\n\n[testenv:bench]\n</code></pre> <p>By default, the test command skips all benchmark environments. To run only benchmark environments use the <code>--bench</code>/<code>-b</code> flag. The results are sorted by <code>tottime</code>, which is the total time spent in the given function (and excluding time made in calls to sub-functions).</p>"},{"location":"ddev/test/#logs","title":"Logs","text":"<p>We provide an easy way to utilize log collection with E2E Docker environments.</p> <ol> <li> <p>Pass <code>mount_logs=True</code> to docker_run. This will use the logs example in    the integration's config spec. For example, the following defines 2 example log files:</p> <pre><code>- template: logs\nexample:\n- type: file\npath: /var/log/apache2/access.log\nsource: apache\nservice: apache\n- type: file\npath: /var/log/apache2/error.log\nsource: apache\nservice: apache\n</code></pre> Alternatives <ul> <li>If <code>mount_logs</code> is a sequence of <code>int</code>, only the selected indices (starting at 1) will be used. So,   using the Apache example above, to only monitor the error log you would set it to <code>[2]</code>.</li> <li>In lieu of a config spec, for whatever reason, you may set <code>mount_logs</code> to a <code>dict</code> containing the   standard logs key.</li> </ul> </li> <li> <p>All requested log files are available to reference as environment variables for any Docker calls as    <code>DD_LOG_&lt;LOG_CONFIG_INDEX&gt;</code> where the indices start at 1.</p> <pre><code>volumes:\n- ${DD_LOG_1}:/usr/local/apache2/logs/access_log\n- ${DD_LOG_2}:/usr/local/apache2/logs/error_log\n</code></pre> </li> <li> <p>To send logs to a custom URL, set <code>log_url</code> for the configured organization.</p> </li> </ol>"},{"location":"ddev/test/#reference","title":"Reference","text":""},{"location":"ddev/test/#datadog_checks.dev.docker","title":"<code>datadog_checks.dev.docker</code>","text":""},{"location":"ddev/test/#datadog_checks.dev.docker.docker_run","title":"<code>docker_run(compose_file=None, build=False, service_name=None, up=None, down=None, on_error=None, sleep=None, endpoints=None, log_patterns=None, mount_logs=False, conditions=None, env_vars=None, wrappers=None, attempts=None, attempts_wait=1)</code>","text":"<p>A convenient context manager for safely setting up and tearing down Docker environments.</p> <p>Parameters:</p> <pre><code>compose_file (str):\n    A path to a Docker compose file. A custom tear\n    down is not required when using this.\nbuild (bool):\n    Whether or not to build images for when `compose_file` is provided\nservice_name (str):\n    Optional name for when ``compose_file`` is provided\nup (callable):\n    A custom setup callable\ndown (callable):\n    A custom tear down callable. This is required when using a custom setup.\non_error (callable):\n    A callable called in case of an unhandled exception\nsleep (float):\n    Number of seconds to wait before yielding. This occurs after all conditions are successful.\nendpoints (list[str]):\n    Endpoints to verify access for before yielding. Shorthand for adding\n    `CheckEndpoints(endpoints)` to the `conditions` argument.\nlog_patterns (list[str | re.Pattern]):\n    Regular expression patterns to find in Docker logs before yielding.\n    This is only available when `compose_file` is provided. Shorthand for adding\n    `CheckDockerLogs(compose_file, log_patterns, 'all')` to the `conditions` argument.\nmount_logs (bool):\n    Whether or not to mount log files in Agent containers based on example logs configuration\nconditions (callable):\n    A list of callable objects that will be executed before yielding to check for errors\nenv_vars (dict[str, str]):\n    A dictionary to update `os.environ` with during execution\nwrappers (list[callable]):\n    A list of context managers to use during execution\nattempts (int):\n    Number of attempts to run `up` and the `conditions` successfully. Defaults to 2 in CI\nattempts_wait (int):\n    Time to wait between attempts\n</code></pre> Source code in <code>datadog_checks_dev/datadog_checks/dev/docker.py</code> <pre><code>@contextmanager\ndef docker_run(\ncompose_file=None,\nbuild=False,\nservice_name=None,\nup=None,\ndown=None,\non_error=None,\nsleep=None,\nendpoints=None,\nlog_patterns=None,\nmount_logs=False,\nconditions=None,\nenv_vars=None,\nwrappers=None,\nattempts=None,\nattempts_wait=1,\n):\n\"\"\"\n    A convenient context manager for safely setting up and tearing down Docker environments.\n    Parameters:\n        compose_file (str):\n            A path to a Docker compose file. A custom tear\n            down is not required when using this.\n        build (bool):\n            Whether or not to build images for when `compose_file` is provided\n        service_name (str):\n            Optional name for when ``compose_file`` is provided\n        up (callable):\n            A custom setup callable\n        down (callable):\n            A custom tear down callable. This is required when using a custom setup.\n        on_error (callable):\n            A callable called in case of an unhandled exception\n        sleep (float):\n            Number of seconds to wait before yielding. This occurs after all conditions are successful.\n        endpoints (list[str]):\n            Endpoints to verify access for before yielding. Shorthand for adding\n            `CheckEndpoints(endpoints)` to the `conditions` argument.\n        log_patterns (list[str | re.Pattern]):\n            Regular expression patterns to find in Docker logs before yielding.\n            This is only available when `compose_file` is provided. Shorthand for adding\n            `CheckDockerLogs(compose_file, log_patterns, 'all')` to the `conditions` argument.\n        mount_logs (bool):\n            Whether or not to mount log files in Agent containers based on example logs configuration\n        conditions (callable):\n            A list of callable objects that will be executed before yielding to check for errors\n        env_vars (dict[str, str]):\n            A dictionary to update `os.environ` with during execution\n        wrappers (list[callable]):\n            A list of context managers to use during execution\n        attempts (int):\n            Number of attempts to run `up` and the `conditions` successfully. Defaults to 2 in CI\n        attempts_wait (int):\n            Time to wait between attempts\n    \"\"\"\nif compose_file and up:\nraise TypeError('You must select either a compose file or a custom setup callable, not both.')\nif compose_file is not None:\nif not isinstance(compose_file, string_types):\nraise TypeError('The path to the compose file is not a string: {}'.format(repr(compose_file)))\nset_up = ComposeFileUp(compose_file, build=build, service_name=service_name)\nif down is not None:\ntear_down = down\nelse:\ntear_down = ComposeFileDown(compose_file)\nif on_error is None:\non_error = ComposeFileLogs(compose_file)\nelse:\nset_up = up\ntear_down = down\ndocker_conditions = []\nif log_patterns is not None:\nif compose_file is None:\nraise ValueError(\n'The `log_patterns` convenience is unavailable when using '\n'a custom setup. Please use a custom condition instead.'\n)\ndocker_conditions.append(CheckDockerLogs(compose_file, log_patterns, 'all'))\nif conditions is not None:\ndocker_conditions.extend(conditions)\nwrappers = list(wrappers) if wrappers is not None else []\nif mount_logs:\nif isinstance(mount_logs, dict):\nwrappers.append(shared_logs(mount_logs['logs']))\n# Easy mode, read example config\nelse:\n# An extra level deep because of the context manager\ncheck_root = find_check_root(depth=2)\nexample_log_configs = _read_example_logs_config(check_root)\nif mount_logs is True:\nwrappers.append(shared_logs(example_log_configs))\nelif isinstance(mount_logs, (list, set)):\nwrappers.append(shared_logs(example_log_configs, mount_whitelist=mount_logs))\nelse:\nraise TypeError(\n'mount_logs: expected True, a list or a set, but got {}'.format(type(mount_logs).__name__)\n)\nwith environment_run(\nup=set_up,\ndown=tear_down,\non_error=on_error,\nsleep=sleep,\nendpoints=endpoints,\nconditions=docker_conditions,\nenv_vars=env_vars,\nwrappers=wrappers,\nattempts=attempts,\nattempts_wait=attempts_wait,\n) as result:\nyield result\n</code></pre>"},{"location":"ddev/test/#datadog_checks.dev.docker.get_docker_hostname","title":"<code>get_docker_hostname()</code>","text":"<p>Determine the hostname Docker uses based on the environment, defaulting to <code>localhost</code>.</p> Source code in <code>datadog_checks_dev/datadog_checks/dev/docker.py</code> <pre><code>def get_docker_hostname():\n\"\"\"\n    Determine the hostname Docker uses based on the environment, defaulting to `localhost`.\n    \"\"\"\nreturn urlparse(os.getenv('DOCKER_HOST', '')).hostname or 'localhost'\n</code></pre>"},{"location":"ddev/test/#datadog_checks.dev.docker.get_container_ip","title":"<code>get_container_ip(container_id_or_name)</code>","text":"<p>Get a Docker container's IP address from its ID or name.</p> Source code in <code>datadog_checks_dev/datadog_checks/dev/docker.py</code> <pre><code>def get_container_ip(container_id_or_name):\n\"\"\"\n    Get a Docker container's IP address from its ID or name.\n    \"\"\"\ncommand = [\n'docker',\n'inspect',\n'-f',\n'{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}',\ncontainer_id_or_name,\n]\nreturn run_command(command, capture='out', check=True).stdout.strip()\n</code></pre>"},{"location":"ddev/test/#datadog_checks.dev.docker.compose_file_active","title":"<code>compose_file_active(compose_file)</code>","text":"<p>Returns a <code>bool</code> indicating whether or not a compose file has any active services.</p> Source code in <code>datadog_checks_dev/datadog_checks/dev/docker.py</code> <pre><code>def compose_file_active(compose_file):\n\"\"\"\n    Returns a `bool` indicating whether or not a compose file has any active services.\n    \"\"\"\ncommand = ['docker', 'compose', '-f', compose_file, 'ps']\nlines = run_command(command, capture='out', check=True).stdout.strip().splitlines()\nreturn len(lines) &gt; 1\n</code></pre>"},{"location":"ddev/test/#datadog_checks.dev.terraform","title":"<code>datadog_checks.dev.terraform</code>","text":""},{"location":"ddev/test/#datadog_checks.dev.terraform.terraform_run","title":"<code>terraform_run(directory, sleep=None, endpoints=None, conditions=None, env_vars=None, wrappers=None)</code>","text":"<p>A convenient context manager for safely setting up and tearing down Terraform environments.</p> <p>Parameters:</p> <pre><code>directory (str):\n    A path containing Terraform files\nsleep (float):\n    Number of seconds to wait before yielding. This occurs after all conditions are successful.\nendpoints (list[str]):\n    Endpoints to verify access for before yielding. Shorthand for adding\n    `CheckEndpoints(endpoints)` to the `conditions` argument.\nconditions (list[callable]):\n    A list of callable objects that will be executed before yielding to check for errors\nenv_vars (dict[str, str]):\n    A dictionary to update `os.environ` with during execution\nwrappers (list[callable]):\n    A list of context managers to use during execution\n</code></pre> Source code in <code>datadog_checks_dev/datadog_checks/dev/terraform.py</code> <pre><code>@contextmanager\ndef terraform_run(directory, sleep=None, endpoints=None, conditions=None, env_vars=None, wrappers=None):\n\"\"\"\n    A convenient context manager for safely setting up and tearing down Terraform environments.\n    Parameters:\n        directory (str):\n            A path containing Terraform files\n        sleep (float):\n            Number of seconds to wait before yielding. This occurs after all conditions are successful.\n        endpoints (list[str]):\n            Endpoints to verify access for before yielding. Shorthand for adding\n            `CheckEndpoints(endpoints)` to the `conditions` argument.\n        conditions (list[callable]):\n            A list of callable objects that will be executed before yielding to check for errors\n        env_vars (dict[str, str]):\n            A dictionary to update `os.environ` with during execution\n        wrappers (list[callable]):\n            A list of context managers to use during execution\n    \"\"\"\nif not which('terraform'):\npytest.skip('Terraform not available')\nset_up = TerraformUp(directory)\ntear_down = TerraformDown(directory)\nwith environment_run(\nup=set_up,\ndown=tear_down,\nsleep=sleep,\nendpoints=endpoints,\nconditions=conditions,\nenv_vars=env_vars,\nwrappers=wrappers,\n) as result:\nyield result\n</code></pre>"},{"location":"faq/acknowledgements/","title":"Acknowledgements","text":"<p>This is not meant to be an exhaustive list of all the things we use, but rather a token of appreciation for the services and open source software we publicly benefit from.</p>"},{"location":"faq/acknowledgements/#base","title":"Base","text":"<ul> <li>The Python programming language, the default language of Agent Integrations, enables us and   contributors to think about problems abstractly and express intent as clearly and concisely as possible.</li> </ul>"},{"location":"faq/acknowledgements/#dependencies","title":"Dependencies","text":"<p>We would be unable to move as fast as we do without the massive ecosystem of established software others have built.</p> <p>If you've contributed to one of the following projects, thank you! Your code is deployed on many systems and devices across the world.</p> <p>We stand on the shoulders of giants.</p> Dependencies CoreOther <ul> <li>aerospike</li> <li>aws-requests-auth</li> <li>azure-identity</li> <li>beautifulsoup4</li> <li>binary</li> <li>boto</li> <li>boto3</li> <li>botocore</li> <li>cachetools</li> <li>clickhouse-cityhash</li> <li>clickhouse-driver</li> <li>cm-client</li> <li>confluent-kafka</li> <li>contextlib2</li> <li>cryptography</li> <li>ddtrace</li> <li>dnspython</li> <li>enum34</li> <li>foundationdb</li> <li>futures</li> <li>gearman</li> <li>importlib-metadata</li> <li>in-toto</li> <li>ipaddress</li> <li>jaydebeapi</li> <li>jellyfish</li> <li>jpype1</li> <li>kubernetes</li> <li>ldap3</li> <li>lxml</li> <li>lz4</li> <li>mmh3</li> <li>oauthlib</li> <li>openstacksdk</li> <li>oracledb</li> <li>orjson</li> <li>packaging</li> <li>paramiko</li> <li>ply</li> <li>prometheus-client</li> <li>protobuf</li> <li>psutil</li> <li>psycopg</li> <li>psycopg-pool</li> <li>psycopg2-binary</li> <li>pyasn1</li> <li>pycryptodomex</li> <li>pydantic</li> <li>pyjwt</li> <li>pymongo</li> <li>pymqi</li> <li>pymysql</li> <li>pyodbc</li> <li>pyro4</li> <li>pysmi</li> <li>pysnmp</li> <li>pysnmp-mibs</li> <li>pysocks</li> <li>python-binary-memcached</li> <li>python-dateutil</li> <li>python3-gearman</li> <li>pyvmomi</li> <li>pywin32</li> <li>pyyaml</li> <li>redis</li> <li>requests</li> <li>requests-kerberos</li> <li>requests-ntlm</li> <li>requests-oauthlib</li> <li>requests-toolbelt</li> <li>requests-unixsocket</li> <li>rethinkdb</li> <li>scandir</li> <li>securesystemslib</li> <li>semver</li> <li>serpent</li> <li>service-identity</li> <li>simplejson</li> <li>six</li> <li>snowflake-connector-python</li> <li>supervisor</li> <li>tuf</li> <li>typing</li> <li>uptime</li> <li>vertica-python</li> <li>win-inet-pton</li> <li>wrapt</li> </ul> <ul> <li>Rick</li> </ul>"},{"location":"faq/acknowledgements/#hosting","title":"Hosting","text":"<p>A huge thanks to everyone involved in maintaining PyPI. We rely on it for providing all dependencies for not only tests, but also all Datadog Agent deployments.</p>"},{"location":"faq/acknowledgements/#documentation","title":"Documentation","text":"<ul> <li>MkDocs provides us with powerful and extensible static site generation capabilities, leading to an   equally impressive community around it.</li> <li>The Material for MkDocs theme allows us to create beautiful documentation with cross-browser and mobile support.</li> <li>PyMdown Extensions gives us the ability to use advanced HTML, CSS, and JavaScript functionality with   simple, easy to use Markdown.</li> </ul>"},{"location":"faq/acknowledgements/#cicd","title":"CI/CD","text":"<ul> <li>Azure Pipelines is used for testing all Agent Integrations. A special shout-out to   Microsoft for being extremely generous with our allowance of parallel   runners; only they were able to meet the requirements of our unique monorepo.</li> <li>GitHub Actions is used for all repository automation, like documentation deployment and pull request labeling.</li> </ul>"},{"location":"faq/faq/","title":"FAQ","text":""},{"location":"faq/faq/#integration-vs-check","title":"Integration vs Check","text":"<p>A Check is any integration whose execution is triggered directly in code by the Datadog Agent. Therefore, all Agent-based integrations written in Python or Go are considered Checks.</p>"},{"location":"faq/faq/#why-test-tests","title":"Why test tests","text":"<p>We track the coverage of tests in all cases as a drop in test coverage for test code means a test function or part of it is not called. For an example see this test bug fixed thanks to test coverage. See pyca/pynacl#290 and #4280 for more details.</p>"},{"location":"guidelines/conventions/","title":"Conventions","text":""},{"location":"guidelines/conventions/#file-naming","title":"File naming","text":"<p>Often, libraries that interact with a product will name their packages after the product. So if you name a file <code>&lt;PRODUCT_NAME&gt;.py</code>, and inside try to import the library of the same name, you will get import errors that will be difficult to diagnose.</p> <p>Never name a Python file the same as the integration's name.</p>"},{"location":"guidelines/conventions/#attribute-naming","title":"Attribute naming","text":"<p>The base classes may freely add new attributes for new features. Therefore to avoid collisions it is recommended that attribute names be prefixed with underscores, especially for names that are generic. For an example, see below.</p>"},{"location":"guidelines/conventions/#stateful-checks","title":"Stateful checks","text":"<p>Since Agent v6, every instance of AgentCheck corresponds to a single YAML instance of an integration defined in the <code>instances</code> array of user configuration. As such, the <code>instance</code> argument the <code>check</code> method accepts is redundant and wasteful since you are parsing the same configuration at every run.</p> <p>Parse configuration once and save the results.</p> Do thisDo NOT do this <pre><code>class AwesomeCheck(AgentCheck):\n    def __init__(self, name, init_config, instances):\n        super(AwesomeCheck, self).__init__(name, init_config, instances)\n\n        self._server = self.instance.get('server', '')\n        self._port = int(self.instance.get('port', 8080))\n\n        self._tags = list(self.instance.get('tags', []))\n        self._tags.append('server:{}'.format(self._server))\n        self._tags.append('port:{}'.format(self._port))\n\n    def check(self, _):\n        ...\n</code></pre> <pre><code>class AwesomeCheck(AgentCheck):\n    def check(self, instance):\n        server = instance.get('server', '')\n        port = int(instance.get('port', 8080))\n\n        tags = list(instance.get('tags', []))\n        tags.append('server:{}'.format(server))\n        tags.append('port:{}'.format(port))\n        ...\n</code></pre>"},{"location":"guidelines/dashboards/","title":"Dashboards","text":"<p>Datadog dashboards enable you to efficiently monitor your infrastructure and integrations by displaying and tracking key metrics.</p>"},{"location":"guidelines/dashboards/#integration-preset-dashboards","title":"Integration Preset Dashboards","text":"<p>If you would like to create a default dashboard for an integration, follow the guidelines in the Best Practices section.</p>"},{"location":"guidelines/dashboards/#exporting-a-dashboard-payload","title":"Exporting a dashboard payload","text":"<p>When you've created a dashboard in the Datadog UI, you can export the dashboard payload to be included in its integration's assets directory.</p> <p>Ensure that you have set an <code>api_key</code> and <code>app_key</code> for the org that contains the new dashboard in the <code>ddev</code> configuration.</p> <p>Run the following command to export the dashboard:</p> <pre><code>ddev meta dash export &lt;URL_OF_DASHBOARD&gt; &lt;INTEGRATION&gt;\n</code></pre> <p>Tip</p> <p>If the dashboard is for a contributor-maintained integration in the <code>integration-extras</code> repo, run the command with the <code>--extras</code> or <code>-e</code> flag.</p> <p>The command will add the dashboard definition to the <code>manifest.json</code> file of the integration. The dashboard JSON payload will be available in <code>/assets/dashboards/&lt;DASHBOARD_TITLE&gt;.json</code>.</p> <p>Tip</p> <p>The dashboard is available at the following address <code>/dash/integration/&lt;DASHBOARD_KEY&gt;</code> in each region, where <code>&lt;DASHBOARD_KEY&gt;</code> is the one you have in the <code>manifest.json</code> file of the integration for this dashboard. This can be useful when you want to add a link to another dashboard inside your dashboard.</p> <p>Commit the changes and create a pull request.</p>"},{"location":"guidelines/dashboards/#verify-the-preset-dashboard","title":"Verify the Preset Dashboard","text":"<p>Once your PR is merged and synced on production, you can find your dashboard in the Dashboard List page.</p> <p>Tip</p> <p>Make sure the integration tile is <code>Installed</code> in order to see the preset dashboard in the list.</p> <p>Ensure logos render correctly on the Dashboard List page and within the preset dashboard.</p>"},{"location":"guidelines/dashboards/#best-practices","title":"Best Practices","text":""},{"location":"guidelines/dashboards/#example-of-a-great-integration-dashboard","title":"Example of a great integration dashboard","text":"<ul> <li> Attention-grabbing \"about\" section with a banner image, concise copy, useful links, and a good typography hierarchy</li> <li> A brief, annotated \"overview\" section with the most important statistics, right at the top</li> <li> Simple graph titles and title-case group names</li> <li> Nearly symmetrical in high density mode</li> <li> Well formatted, concise notes</li> <li> Color coordination between related groups, notes within groups, and graphs within groups</li> </ul> <p>Follow the guidelines below as you build your dashboard to achieve something similar.</p>"},{"location":"guidelines/dashboards/#general","title":"General","text":"<ol> <li> <p>When creating a new dashboard, select the default dashboard type (internally called multisize layout).</p> </li> <li> <p>Dashboard titles should contain the integration name. Some examples of a good dashboard title are <code>Syclla</code> and <code>Cilium Overview</code>.</p> <p>Warning</p> <p>Avoid using - (hyphen) in the dashboard title as the dashboard URL is generated from the title.</p> </li> <li> <p>Add a logo to the dashboard header. The integration logo will automatically appear in the header if the icon exists here and the <code>integration_id</code> matches the icon name. That means it will only appear when the dashboard you're working on is made into the official integration board.    </p> </li> <li> <p>Always include an \"about\" group for the integration containing a brief description and helpful links. Also include an \"overview\" group containing a few of the most important metrics, and place it at the top of the dashboard. Edit the \"about\" group and select the \"banner\" display option, then link to a banner image like this: <code>/static/images/integration_dashboard/your-image.png</code>. For instructions on how to create and upload a banner image, go to the DRUIDS logo gallery, click the relevant logo, and click the \"Dashboard Banner\" tab. The \"about\" section should contain content, not data; the \"overview\" section should contain data. Avoid making the \"about\" section full-width.    </p> </li> <li> <p>Research the metrics supported by the integration and consider grouping them in relevant categories. Important metrics that are key to the performance and overview of the integration should be at the top.</p> </li> <li> <p>Use Group widgets to title and group sections, rather than note widgets as you might on a screenboard. Use partial width groups to display groups side-by-side. Most dashboards should display every widget within a group.    </p> </li> <li> <p>Timeseries widgets should be at least 4 columns wide in order not to appear squashed on smaller displays</p> </li> <li> <p>Stream widgets should be at least 6 columns wide (half the dashboard width) for readability. You should place them at the end of a dashboard so they don't \"trap\" scrolling. It's useful to put stream widgets in a group by themselves so they can be collapsed. Add an event stream only if the service monitored by the dashboard is reporting events. Use <code>sources:service_name</code>.    </p> </li> <li> <p>Which widgets best represent your data? Try using a mix of widget types and sizes. Explore visualizations and formatting options until you're confident your dashboard is as clear as it can be. Sometimes a whole dashboard of timeseries is ok, but other times variety can improve things. The most commonly used metric widgets are timeseries, query values, and tables. For more information on the available widget types, see the list of supported dashboard widgets.</p> </li> <li> <p>Try to make the left and right halves of your dashboard symmetrical in high density mode. Users with large monitors will see your dashboard in high density mode by default, so it's important to make sure the group relationships make sense, and the dashboard looks good. You can adjust group heights to achieve this, and move groups between the left and right halves.</p> <p>a. (perfectly symmetrical) </p> <p>b. (close enough) </p> </li> <li> <p>Template variables allow you to dynamically filter one or more widgets in a dashboard. Template variables must be universal and accessible by any user or account using the monitored service. Make sure all relevant graphs are listening to the relevant template variable filters.</p> <p>Tip</p> <p>Adding <code>*=scope</code> as a template variable is useful since users can access all their own tags.</p> </li> </ol>"},{"location":"guidelines/dashboards/#copy","title":"Copy","text":"<ol> <li> <p>Prefer concise graph titles that start with the most important information. Avoid common phrases such as \"number of\", and don't include the integration title e.g. \"Memcached Load\".</p> Concise title (good) Verbose title (bad) Events per node Number of Kubernetes events per node Pending tasks: [$node_name] Total number of pending tasks in [$node_name] Read/write operations Number of read/write operations Connections to server - rate Rate of connections to server Load Memcached Load </li> <li> <p>Avoid repeating the group title or integration name in every widget in a group, especially if the widgets are query values with a custom unit of the same name. Note the word \"shards\" in each widget title in the group named \"shards\".    </p> </li> <li> <p>Always alias formulas</p> </li> <li> <p>Group titles should be title case. Widget titles should be sentence case.</p> </li> <li> <p>If you're showing a legend, make sure the aliases are easy to understand.</p> </li> <li> <p>Graph titles should summarize the queried metric. Do not indicate the unit in the graph title because unit types are displayed automatically from metadata. An exception to this is if the calculation of the query represents a different type of unit.</p> </li> </ol>"},{"location":"guidelines/dashboards/#qa","title":"QA","text":"<ol> <li> <p>Always check a dashboard at 1280px wide and 2560px wide to see how it looks on a smaller laptop and a larger monitor. The most common screen widths for dashboards are 1920, 1680, 1440, 2560, and 1280px, making up more than half of all dashboard page views combined.    </p> <p>Tip</p> <p>If your monitor isn't large enough for high density mode, use the browser zoom controls to zoom out.</p> </li> </ol>"},{"location":"guidelines/dashboards/#visual-style","title":"Visual Style","text":"<ol> <li> <p>Format notes to make them fit their use case. Try the presets \"caption\", \"annotation\", or \"header\", or pick your own combination of styles. Avoid using the smallest font size for notes that are long or include complex formatting, like bulleted lists or code blocks.</p> </li> <li> <p>Use colors to highlight important relationships and to improve readability, not for style. If several groups are related, apply the same group header color to all of them. If you've applied a green header color to a group, try making its notes green as well. If two groups are related, but one is more important, try using the \"vivid\" color on the important group and the \"light\" color on the less important group. Don't be afraid to leave groups with white headers, and be careful not to overuse color e.g. don't make every group on a dashboard vivid blue. Also avoid using gray headers.    </p> </li> <li> <p>Use legends when they make sense. Legends make it easy to read a graph without having to hover over each series or maximize the widget. Make sure you use aliases so the legend is easy to read. Automatic mode for legends is a great option that hides legends when space is tight and shows them when there's room.    </p> </li> <li> <p>If you want users to compare two graphs side-by-side, make sure their x-axes align. If one graph is showing a legend and the other isn't, the x-axes won't align - make sure they either both show a legend or both do not.    </p> </li> <li> <p>For timeseries, base the display type on the type of metric</p> Types of metric Display type Volume (e.g. Number of connections) <code>area</code> Counts (e.g. Number of errors) <code>bars</code> Multiple groups or default <code>lines</code> </li> </ol>"},{"location":"guidelines/pr/","title":"Pull requests","text":""},{"location":"guidelines/pr/#changelog-entries","title":"Changelog entries","text":"<p>Every PR must add a changelog entry to each integration that has had its shipped code modified.</p> <p>Each integration that can be installed on the Agent has its own <code>CHANGELOG.md</code> file at the root of its directory. Entries accumulate under the <code>Unreleased</code> section and at release time get put under their own section. For example:</p> <pre><code># CHANGELOG - Foo\n\n## Unreleased\n\n***Changed***:\n\n* Made a breaking change ([#9000](https://github.com/DataDog/repo/pull/9000))\n\n    Here's some extra context [...]\n\n***Added***:\n\n* Add a cool feature ([#42](https://github.com/DataDog/repo/pull/42))\n\n## 1.2.3 / 2081-04-01\n\n***Fixed***:\n\n...\n</code></pre> <p>For changelog types, we adhere to those defined by Keep a Changelog:</p> <ul> <li><code>Added</code> for new features or any non-trivial refactors.</li> <li><code>Changed</code> for changes in existing functionality.</li> <li><code>Deprecated</code> for soon-to-be removed features.</li> <li><code>Removed</code> for now removed features.</li> <li><code>Fixed</code> for any bug fixes.</li> <li><code>Security</code> in case of vulnerabilities.</li> </ul> <p>The first line of every new changelog entry must end with a link to the PR in which the change occurred. To automatically apply this suffix to manually added entries, you may run the <code>release changelog fix</code> command. To create new entries, you may use the <code>release changelog new</code> command.</p> <p>Tip</p> <p>You may apply the <code>changelog/no-changelog</code> label to remove the CI check for changelog entries.</p> Formatting rules"},{"location":"guidelines/pr/#spacing","title":"Spacing","text":"<ul> <li>There should be a blank line between each section. This means that there should be a line between the following sections of text:</li> <li>Changelog file header</li> <li>Unreleased header</li> <li>Version / Date header</li> <li>Change type (ex: fixed, added, etc)</li> <li>Specific descriptions of changes (Note: Within this section, there should not be new lines between bullet points,)</li> <li><code>Extra spacing on line {line number}</code>: There is an extra blank line on the line referenced in the error.</li> <li><code>Missing spacing on line {line number}</code>: Add an empty line above or below the referenced line.</li> </ul>"},{"location":"guidelines/pr/#version-header","title":"Version header","text":"<ul> <li>The header for an integration version should be in the following format: <code>version number / YYYY-MM-DD / Agent Version Number</code>. The Agent version number is not necessary, but a valid version number and date are required. The first header after the file's title can be <code>Unreleased</code>. The content under this section is the same as any other.</li> <li><code>Version is formatted incorrectly on line {line number}</code>: The version you inputted is not a valid version, or there is no / separator between the version and date in your header.</li> <li><code>Date is formatted incorrectly on line {line number}</code>: The date must be formatted as YYYY-MM-DD, with no spaces in between.</li> </ul>"},{"location":"guidelines/pr/#content","title":"Content","text":"<ul> <li>The changelog header must be capitalized and written in this format: <code>***HEADER***:</code>. Note that it should be bold and italicized.</li> <li><code>Changelog type is incorrect on line {line count}</code>: The changelog header on that line is not one of the six valid changelog types.</li> <li><code>Changelog header order is incorrect on line {line count}</code>: The changelog header on that line is in the wrong order. Double check the ordering of the changelogs and ensure that the headers for the changelog types are correctly ordered by priority.</li> <li><code>Changelogs should start with asterisks, on line {line count}</code>: All changelog details below each header should be bullet points, using asterisks.</li> </ul>"},{"location":"guidelines/pr/#separation-of-concerns","title":"Separation of concerns","text":"<p>Every pull request should do one thing only for easier Git management. For example, if you are editing documentation and notice an error in the shipped example configuration, you should fix the error in a separate pull request. Doing so will enable a clean cherry-pick or revert of the bug fix should the need arise.</p>"},{"location":"guidelines/pr/#merges","title":"Merges","text":"<p>We only allow GitHub's squash and merge to keep a clean Git history.</p>"},{"location":"guidelines/style/","title":"Style","text":"<p>These are all the checkers used by our style enforcement.</p>"},{"location":"guidelines/style/#black","title":"black","text":"<p>An opinionated formatter, like JavaScript's prettier and Golang's gofmt.</p>"},{"location":"guidelines/style/#isort","title":"isort","text":"<p>A tool to sort imports lexicographically, by section, and by type. We use the 5 standard sections: <code>__future__</code>, stdlib, third party, first party, and local.</p> <p><code>datadog_checks</code> is configured as a first party namespace.</p>"},{"location":"guidelines/style/#flake8","title":"flake8","text":"<p>An easy-to-use wrapper around pycodestyle and pyflakes. We select everything it provides and only ignore a few things to give precedence to other tools.</p>"},{"location":"guidelines/style/#bugbear","title":"bugbear","text":"<p>A <code>flake8</code> plugin for finding likely bugs and design problems in programs. We enable:</p> <ul> <li><code>B001</code>: Do not use bare <code>except:</code>, it also catches unexpected events like memory errors, interrupts, system exit, and so on. Prefer <code>except Exception:</code>.</li> <li><code>B003</code>: Assigning to <code>os.environ</code> doesn't clear the environment. Subprocesses are going to see outdated variables, in disagreement with the current process. Use <code>os.environ.clear()</code> or the <code>env=</code> argument to Popen.</li> <li><code>B006</code>: Do not use mutable data structures for argument defaults. All calls reuse one instance of that data structure, persisting changes between them.</li> <li><code>B007</code>: Loop control variable not used within the loop body. If this is intended, start the name with an underscore.</li> <li><code>B301</code>: Python 3 does not include <code>.iter*</code> methods on dictionaries. The default behavior is to return iterables. Simply remove the <code>iter</code> prefix from the method. For Python 2 compatibility, also prefer the Python 3 equivalent if you expect that the size of the dict to be small and bounded. The performance regression on Python 2 will be negligible and the code is going to be the clearest. Alternatively, use <code>six.iter*</code>.</li> <li><code>B305</code>: <code>.next()</code> is not a thing on Python 3. Use the <code>next()</code> builtin. For Python 2 compatibility, use <code>six.next()</code>.</li> <li><code>B306</code>: <code>BaseException.message</code> has been deprecated as of Python 2.6 and is removed in Python 3. Use <code>str(e)</code> to access the user-readable message. Use <code>e.args</code> to access arguments passed to the exception.</li> <li><code>B902</code>: Invalid first argument used for method. Use <code>self</code> for instance methods, and <code>cls</code> for class methods.</li> </ul>"},{"location":"guidelines/style/#logging-format","title":"logging-format","text":"<p>A <code>flake8</code> plugin for ensuring a consistent logging format. We enable:</p> <ul> <li><code>G001</code>: Logging statements should not use <code>string.format()</code> for their first argument</li> <li><code>G002</code>: Logging statements should not use <code>%</code> formatting for their first argument</li> <li><code>G003</code>: Logging statements should not use <code>+</code> concatenation for their first argument</li> <li><code>G004</code>: Logging statements should not use <code>f\"...\"</code> for their first argument (only in Python 3.6+)</li> <li><code>G010</code>: Logging statements should not use <code>warn</code> (use <code>warning</code> instead)</li> <li><code>G100</code>: Logging statements should not use <code>extra</code> arguments unless whitelisted</li> <li><code>G201</code>: Logging statements should not use <code>error(..., exc_info=True)</code> (use <code>exception(...)</code> instead)</li> <li><code>G202</code>: Logging statements should not use redundant <code>exc_info=True</code> in <code>exception</code></li> </ul>"},{"location":"guidelines/style/#mypy","title":"Mypy","text":"<p>A comment-based type checker allowing a mix of dynamic and static typing. This is optional for now. In order to enable <code>mypy</code> for a specific integration, open its <code>tox.ini</code> file and add the 2 lines in the correct section:</p> <pre><code>[testenv]\ndd_check_types = true\ndd_mypy_args = &lt;FLAGS&gt; --py2 datadog_checks/ tests/\n...\n</code></pre> <p>The <code>dd_mypy_args</code> defines the mypy command line option for this specific integration. <code>--py2</code> is here to make sure the integration is Python2.7 compatible. Here are some useful flags you can add:</p> <ul> <li><code>--check-untyped-defs</code>: Type-checks the interior of functions without type annotations.</li> <li><code>--disallow-untyped-defs</code>: Disallows defining functions without type annotations or with incomplete type annotations.</li> </ul> <p>The <code>datadog_checks/ tests/</code> arguments represent the list of files that <code>mypy</code> should type check. Feel free to edit them as desired, including removing <code>tests/</code> (if you'd prefer to not type-check the test suite), or targeting specific files (when doing partial type checking).</p> <p>For a complete example, see the <code>datadog_checks_base</code> tox configuration.</p> <p>Note that there is a default configuration in the <code>mypy.ini</code> file.</p>"},{"location":"guidelines/style/#example","title":"Example","text":"<p>Extracted from <code>rethinkdb</code>:</p> <pre><code>from typing import Any, Iterator # Contains the different types used\n\nimport rethinkdb\n\nfrom .document_db.types import Metric\n\nclass RethinkDBCheck(AgentCheck):\n    def __init__(self, *args, **kwargs):\n        # type: (*Any, **Any) -&gt; None\n        super(RethinkDBCheck, self).__init__(*args, **kwargs)\n\n    def collect_metrics(self, conn):\n        # type: (rethinkdb.net.Connection) -&gt; Iterator[Metric]\n\"\"\"\n        Collect metrics from the RethinkDB cluster we are connected to.\n        \"\"\"\n        for query in self.queries:\n            for metric in query.run(logger=self.log, conn=conn, config=self._config):\n                yield metric\n</code></pre> <p>Take a look at <code>vsphere</code> or <code>ibm_mq</code> integrations for more examples.</p>"},{"location":"legacy/prometheus/","title":"Prometheus/OpenMetrics V1","text":"<p>Prometheus is an open source monitoring system for timeseries metric data. Many Datadog integrations collect metrics based on Prometheus exported data sets.</p> <p>Prometheus-based integrations use the OpenMetrics exposition format to collect metrics.</p>"},{"location":"legacy/prometheus/#interface","title":"Interface","text":"<p>All functionality is exposed by the <code>OpenMetricsBaseCheck</code> and <code>OpenMetricsScraperMixin</code> classes.</p>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.base_check.OpenMetricsBaseCheck","title":"<code>datadog_checks.base.checks.openmetrics.base_check.OpenMetricsBaseCheck</code>","text":"<p>OpenMetricsBaseCheck is a class that helps scrape endpoints that emit Prometheus metrics only with YAML configurations.</p> <p>Minimal example configuration:</p> <pre><code>instances:\n- prometheus_url: http://example.com/endpoint\n    namespace: \"foobar\"\n    metrics:\n    - bar\n    - foo\n</code></pre> <p>Agent 6 signature:</p> <pre><code>OpenMetricsBaseCheck(name, init_config, instances, default_instances=None, default_namespace=None)\n</code></pre> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/base_check.py</code> <pre><code>class OpenMetricsBaseCheck(OpenMetricsScraperMixin, AgentCheck):\n\"\"\"\n    OpenMetricsBaseCheck is a class that helps scrape endpoints that emit Prometheus metrics only\n    with YAML configurations.\n    Minimal example configuration:\n        instances:\n        - prometheus_url: http://example.com/endpoint\n            namespace: \"foobar\"\n            metrics:\n            - bar\n            - foo\n    Agent 6 signature:\n        OpenMetricsBaseCheck(name, init_config, instances, default_instances=None, default_namespace=None)\n    \"\"\"\nDEFAULT_METRIC_LIMIT = 2000\nHTTP_CONFIG_REMAPPER = {\n'ssl_verify': {'name': 'tls_verify'},\n'ssl_cert': {'name': 'tls_cert'},\n'ssl_private_key': {'name': 'tls_private_key'},\n'ssl_ca_cert': {'name': 'tls_ca_cert'},\n'prometheus_timeout': {'name': 'timeout'},\n'request_size': {'name': 'request_size', 'default': 10},\n}\n# Allow tracing for openmetrics integrations\ndef __init_subclass__(cls, **kwargs):\nsuper().__init_subclass__(**kwargs)\nreturn traced_class(cls)\ndef __init__(self, *args, **kwargs):\n\"\"\"\n        The base class for any Prometheus-based integration.\n        \"\"\"\nargs = list(args)\ndefault_instances = kwargs.pop('default_instances', None) or {}\ndefault_namespace = kwargs.pop('default_namespace', None)\nlegacy_kwargs_in_args = args[4:]\ndel args[4:]\nif len(legacy_kwargs_in_args) &gt; 0:\ndefault_instances = legacy_kwargs_in_args[0] or {}\nif len(legacy_kwargs_in_args) &gt; 1:\ndefault_namespace = legacy_kwargs_in_args[1]\nsuper(OpenMetricsBaseCheck, self).__init__(*args, **kwargs)\nself.config_map = {}\nself._http_handlers = {}\nself.default_instances = default_instances\nself.default_namespace = default_namespace\n# pre-generate the scraper configurations\nif 'instances' in kwargs:\ninstances = kwargs['instances']\nelif len(args) == 4:\n# instances from agent 5 signature\ninstances = args[3]\nelif isinstance(args[2], (tuple, list)):\n# instances from agent 6 signature\ninstances = args[2]\nelse:\ninstances = None\nif instances is not None:\nfor instance in instances:\npossible_urls = instance.get('possible_prometheus_urls')\nif possible_urls is not None:\nfor url in possible_urls:\ntry:\nnew_instance = deepcopy(instance)\nnew_instance.update({'prometheus_url': url})\nscraper_config = self.get_scraper_config(new_instance)\nresponse = self.send_request(url, scraper_config)\nresponse.raise_for_status()\ninstance['prometheus_url'] = url\nself.get_scraper_config(instance)\nbreak\nexcept (IOError, requests.HTTPError, requests.exceptions.SSLError) as e:\nself.log.info(\"Couldn't connect to %s: %s, trying next possible URL.\", url, str(e))\nelse:\nraise CheckException(\n\"The agent could not connect to any of the following URLs: %s.\" % possible_urls\n)\nelse:\nself.get_scraper_config(instance)\ndef check(self, instance):\n# Get the configuration for this specific instance\nscraper_config = self.get_scraper_config(instance)\n# We should be specifying metrics for checks that are vanilla OpenMetricsBaseCheck-based\nif not scraper_config['metrics_mapper']:\nraise CheckException(\n\"You have to collect at least one metric from the endpoint: {}\".format(scraper_config['prometheus_url'])\n)\nself.process(scraper_config)\ndef get_scraper_config(self, instance):\n\"\"\"\n        Validates the instance configuration and creates a scraper configuration for a new instance.\n        If the endpoint already has a corresponding configuration, return the cached configuration.\n        \"\"\"\nendpoint = instance.get('prometheus_url')\nif endpoint is None:\nraise CheckException(\"Unable to find prometheus URL in config file.\")\n# If we've already created the corresponding scraper configuration, return it\nif endpoint in self.config_map:\nreturn self.config_map[endpoint]\n# Otherwise, we create the scraper configuration\nconfig = self.create_scraper_configuration(instance)\n# Add this configuration to the config_map\nself.config_map[endpoint] = config\nreturn config\ndef _finalize_tags_to_submit(self, _tags, metric_name, val, metric, custom_tags=None, hostname=None):\n\"\"\"\n        Format the finalized tags\n        This is generally a noop, but it can be used to change the tags before sending metrics\n        \"\"\"\nreturn _tags\ndef _filter_metric(self, metric, scraper_config):\n\"\"\"\n        Used to filter metrics at the beginning of the processing, by default no metric is filtered\n        \"\"\"\nreturn False\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.base_check.OpenMetricsBaseCheck.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>The base class for any Prometheus-based integration.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/base_check.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"\n    The base class for any Prometheus-based integration.\n    \"\"\"\nargs = list(args)\ndefault_instances = kwargs.pop('default_instances', None) or {}\ndefault_namespace = kwargs.pop('default_namespace', None)\nlegacy_kwargs_in_args = args[4:]\ndel args[4:]\nif len(legacy_kwargs_in_args) &gt; 0:\ndefault_instances = legacy_kwargs_in_args[0] or {}\nif len(legacy_kwargs_in_args) &gt; 1:\ndefault_namespace = legacy_kwargs_in_args[1]\nsuper(OpenMetricsBaseCheck, self).__init__(*args, **kwargs)\nself.config_map = {}\nself._http_handlers = {}\nself.default_instances = default_instances\nself.default_namespace = default_namespace\n# pre-generate the scraper configurations\nif 'instances' in kwargs:\ninstances = kwargs['instances']\nelif len(args) == 4:\n# instances from agent 5 signature\ninstances = args[3]\nelif isinstance(args[2], (tuple, list)):\n# instances from agent 6 signature\ninstances = args[2]\nelse:\ninstances = None\nif instances is not None:\nfor instance in instances:\npossible_urls = instance.get('possible_prometheus_urls')\nif possible_urls is not None:\nfor url in possible_urls:\ntry:\nnew_instance = deepcopy(instance)\nnew_instance.update({'prometheus_url': url})\nscraper_config = self.get_scraper_config(new_instance)\nresponse = self.send_request(url, scraper_config)\nresponse.raise_for_status()\ninstance['prometheus_url'] = url\nself.get_scraper_config(instance)\nbreak\nexcept (IOError, requests.HTTPError, requests.exceptions.SSLError) as e:\nself.log.info(\"Couldn't connect to %s: %s, trying next possible URL.\", url, str(e))\nelse:\nraise CheckException(\n\"The agent could not connect to any of the following URLs: %s.\" % possible_urls\n)\nelse:\nself.get_scraper_config(instance)\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.base_check.OpenMetricsBaseCheck.check","title":"<code>check(instance)</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/base_check.py</code> <pre><code>def check(self, instance):\n# Get the configuration for this specific instance\nscraper_config = self.get_scraper_config(instance)\n# We should be specifying metrics for checks that are vanilla OpenMetricsBaseCheck-based\nif not scraper_config['metrics_mapper']:\nraise CheckException(\n\"You have to collect at least one metric from the endpoint: {}\".format(scraper_config['prometheus_url'])\n)\nself.process(scraper_config)\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.base_check.OpenMetricsBaseCheck.get_scraper_config","title":"<code>get_scraper_config(instance)</code>","text":"<p>Validates the instance configuration and creates a scraper configuration for a new instance. If the endpoint already has a corresponding configuration, return the cached configuration.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/base_check.py</code> <pre><code>def get_scraper_config(self, instance):\n\"\"\"\n    Validates the instance configuration and creates a scraper configuration for a new instance.\n    If the endpoint already has a corresponding configuration, return the cached configuration.\n    \"\"\"\nendpoint = instance.get('prometheus_url')\nif endpoint is None:\nraise CheckException(\"Unable to find prometheus URL in config file.\")\n# If we've already created the corresponding scraper configuration, return it\nif endpoint in self.config_map:\nreturn self.config_map[endpoint]\n# Otherwise, we create the scraper configuration\nconfig = self.create_scraper_configuration(instance)\n# Add this configuration to the config_map\nself.config_map[endpoint] = config\nreturn config\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.mixins.OpenMetricsScraperMixin","title":"<code>datadog_checks.base.checks.openmetrics.mixins.OpenMetricsScraperMixin</code>","text":"Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/mixins.py</code> <pre><code>class OpenMetricsScraperMixin(object):\n# pylint: disable=E1101\n# This class is not supposed to be used by itself, it provides scraping behavior but\n# need to be within a check in the end\n# indexes in the sample tuple of core.Metric\nSAMPLE_NAME = 0\nSAMPLE_LABELS = 1\nSAMPLE_VALUE = 2\nMICROS_IN_S = 1000000\nMINUS_INF = float(\"-inf\")\nTELEMETRY_GAUGE_MESSAGE_SIZE = \"payload.size\"\nTELEMETRY_COUNTER_METRICS_BLACKLIST_COUNT = \"metrics.blacklist.count\"\nTELEMETRY_COUNTER_METRICS_INPUT_COUNT = \"metrics.input.count\"\nTELEMETRY_COUNTER_METRICS_IGNORE_COUNT = \"metrics.ignored.count\"\nTELEMETRY_COUNTER_METRICS_PROCESS_COUNT = \"metrics.processed.count\"\nMETRIC_TYPES = ['counter', 'gauge', 'summary', 'histogram']\nKUBERNETES_TOKEN_PATH = '/var/run/secrets/kubernetes.io/serviceaccount/token'\nMETRICS_WITH_COUNTERS = {\"counter\", \"histogram\", \"summary\"}\ndef __init__(self, *args, **kwargs):\n# Initialize AgentCheck's base class\nsuper(OpenMetricsScraperMixin, self).__init__(*args, **kwargs)\ndef create_scraper_configuration(self, instance=None):\n\"\"\"\n        Creates a scraper configuration.\n        If instance does not specify a value for a configuration option, the value will default to the `init_config`.\n        Otherwise, the `default_instance` value will be used.\n        A default mixin configuration will be returned if there is no instance.\n        \"\"\"\nif 'openmetrics_endpoint' in instance:\nraise CheckException('The setting `openmetrics_endpoint` is only available for Agent version 7 or later')\n# We can choose to create a default mixin configuration for an empty instance\nif instance is None:\ninstance = {}\n# Supports new configuration options\nconfig = copy.deepcopy(instance)\n# Set the endpoint\nendpoint = instance.get('prometheus_url')\nif instance and endpoint is None:\nraise CheckException(\"You have to define a prometheus_url for each prometheus instance\")\n# Set the bearer token authorization to customer value, then get the bearer token\nself.update_prometheus_url(instance, config, endpoint)\n# `NAMESPACE` is the prefix metrics will have. Need to be hardcoded in the\n# child check class.\nnamespace = instance.get('namespace')\n# Check if we have a namespace\nif instance and namespace is None:\nif self.default_namespace is None:\nraise CheckException(\"You have to define a namespace for each prometheus check\")\nnamespace = self.default_namespace\nconfig['namespace'] = namespace\n# Retrieve potential default instance settings for the namespace\ndefault_instance = self.default_instances.get(namespace, {})\ndef _get_setting(name, default):\nreturn instance.get(name, default_instance.get(name, default))\n# `metrics_mapper` is a dictionary where the keys are the metrics to capture\n# and the values are the corresponding metrics names to have in datadog.\n# Note: it is empty in the parent class but will need to be\n# overloaded/hardcoded in the final check not to be counted as custom metric.\n# Metrics are preprocessed if no mapping\nmetrics_mapper = {}\n# We merge list and dictionaries from optional defaults &amp; instance settings\nmetrics = default_instance.get('metrics', []) + instance.get('metrics', [])\nfor metric in metrics:\nif isinstance(metric, string_types):\nmetrics_mapper[metric] = metric\nelse:\nmetrics_mapper.update(metric)\nconfig['metrics_mapper'] = metrics_mapper\n# `_wildcards_re` is a Pattern object used to match metric wildcards\nconfig['_wildcards_re'] = None\nwildcards = set()\nfor metric in config['metrics_mapper']:\nif \"*\" in metric:\nwildcards.add(translate(metric))\nif wildcards:\nconfig['_wildcards_re'] = compile('|'.join(wildcards))\n# `prometheus_metrics_prefix` allows to specify a prefix that all\n# prometheus metrics should have. This can be used when the prometheus\n# endpoint we are scrapping allows to add a custom prefix to it's\n# metrics.\nconfig['prometheus_metrics_prefix'] = instance.get(\n'prometheus_metrics_prefix', default_instance.get('prometheus_metrics_prefix', '')\n)\n# `label_joins` holds the configuration for extracting 1:1 labels from\n# a target metric to all metric matching the label, example:\n# self.label_joins = {\n#     'kube_pod_info': {\n#         'labels_to_match': ['pod'],\n#         'labels_to_get': ['node', 'host_ip']\n#     }\n# }\nconfig['label_joins'] = default_instance.get('label_joins', {})\nconfig['label_joins'].update(instance.get('label_joins', {}))\n# `_label_mapping` holds the additionals label info to add for a specific\n# label value, example:\n# self._label_mapping = {\n#     'pod': {\n#         'dd-agent-9s1l1': {\n#             \"node\": \"yolo\",\n#             \"host_ip\": \"yey\"\n#         }\n#     }\n# }\nconfig['_label_mapping'] = {}\n# `_active_label_mapping` holds a dictionary of label values found during the run\n# to cleanup the label_mapping of unused values, example:\n# self._active_label_mapping = {\n#     'pod': {\n#         'dd-agent-9s1l1': True\n#     }\n# }\nconfig['_active_label_mapping'] = {}\n# `_watched_labels` holds the sets of labels to watch for enrichment\nconfig['_watched_labels'] = {}\nconfig['_dry_run'] = True\n# Some metrics are ignored because they are duplicates or introduce a\n# very high cardinality. Metrics included in this list will be silently\n# skipped without a 'Unable to handle metric' debug line in the logs\nconfig['ignore_metrics'] = instance.get('ignore_metrics', default_instance.get('ignore_metrics', []))\nconfig['_ignored_metrics'] = set()\n# `_ignored_re` is a Pattern object used to match ignored metric patterns\nconfig['_ignored_re'] = None\nignored_patterns = set()\n# Separate ignored metric names and ignored patterns in different sets for faster lookup later\nfor metric in config['ignore_metrics']:\nif '*' in metric:\nignored_patterns.add(translate(metric))\nelse:\nconfig['_ignored_metrics'].add(metric)\nif ignored_patterns:\nconfig['_ignored_re'] = compile('|'.join(ignored_patterns))\n# Ignore metrics based on label keys or specific label values\nconfig['ignore_metrics_by_labels'] = instance.get(\n'ignore_metrics_by_labels', default_instance.get('ignore_metrics_by_labels', {})\n)\n# If you want to send the buckets as tagged values when dealing with histograms,\n# set send_histograms_buckets to True, set to False otherwise.\nconfig['send_histograms_buckets'] = is_affirmative(\ninstance.get('send_histograms_buckets', default_instance.get('send_histograms_buckets', True))\n)\n# If you want the bucket to be non cumulative and to come with upper/lower bound tags\n# set non_cumulative_buckets to True, enabled when distribution metrics are enabled.\nconfig['non_cumulative_buckets'] = is_affirmative(\ninstance.get('non_cumulative_buckets', default_instance.get('non_cumulative_buckets', False))\n)\n# Send histograms as datadog distribution metrics\nconfig['send_distribution_buckets'] = is_affirmative(\ninstance.get('send_distribution_buckets', default_instance.get('send_distribution_buckets', False))\n)\n# Non cumulative buckets are mandatory for distribution metrics\nif config['send_distribution_buckets'] is True:\nconfig['non_cumulative_buckets'] = True\n# If you want to send `counter` metrics as monotonic counts, set this value to True.\n# Set to False if you want to instead send those metrics as `gauge`.\nconfig['send_monotonic_counter'] = is_affirmative(\ninstance.get('send_monotonic_counter', default_instance.get('send_monotonic_counter', True))\n)\n# If you want `counter` metrics to be submitted as both gauges and monotonic counts. Set this value to True.\nconfig['send_monotonic_with_gauge'] = is_affirmative(\ninstance.get('send_monotonic_with_gauge', default_instance.get('send_monotonic_with_gauge', False))\n)\nconfig['send_distribution_counts_as_monotonic'] = is_affirmative(\ninstance.get(\n'send_distribution_counts_as_monotonic',\ndefault_instance.get('send_distribution_counts_as_monotonic', False),\n)\n)\nconfig['send_distribution_sums_as_monotonic'] = is_affirmative(\ninstance.get(\n'send_distribution_sums_as_monotonic',\ndefault_instance.get('send_distribution_sums_as_monotonic', False),\n)\n)\n# If the `labels_mapper` dictionary is provided, the metrics labels names\n# in the `labels_mapper` will use the corresponding value as tag name\n# when sending the gauges.\nconfig['labels_mapper'] = default_instance.get('labels_mapper', {})\nconfig['labels_mapper'].update(instance.get('labels_mapper', {}))\n# Rename bucket \"le\" label to \"upper_bound\"\nconfig['labels_mapper']['le'] = 'upper_bound'\n# `exclude_labels` is an array of label names to exclude. Those labels\n# will just not be added as tags when submitting the metric.\nconfig['exclude_labels'] = default_instance.get('exclude_labels', []) + instance.get('exclude_labels', [])\n# `include_labels` is an array of label names to include. If these labels are not in\n# the `exclude_labels` list, then they are added as tags when submitting the metric.\nconfig['include_labels'] = default_instance.get('include_labels', []) + instance.get('include_labels', [])\n# `type_overrides` is a dictionary where the keys are prometheus metric names\n# and the values are a metric type (name as string) to use instead of the one\n# listed in the payload. It can be used to force a type on untyped metrics.\n# Note: it is empty in the parent class but will need to be\n# overloaded/hardcoded in the final check not to be counted as custom metric.\nconfig['type_overrides'] = default_instance.get('type_overrides', {})\nconfig['type_overrides'].update(instance.get('type_overrides', {}))\n# `_type_override_patterns` is a dictionary where we store Pattern objects\n# that match metric names as keys, and their corresponding metric type overrides as values.\nconfig['_type_override_patterns'] = {}\nwith_wildcards = set()\nfor metric, type in iteritems(config['type_overrides']):\nif '*' in metric:\nconfig['_type_override_patterns'][compile(translate(metric))] = type\nwith_wildcards.add(metric)\n# cleanup metric names with wildcards from the 'type_overrides' dict\nfor metric in with_wildcards:\ndel config['type_overrides'][metric]\n# Some metrics are retrieved from different hosts and often\n# a label can hold this information, this transfers it to the hostname\nconfig['label_to_hostname'] = instance.get('label_to_hostname', default_instance.get('label_to_hostname', None))\n# In combination to label_as_hostname, allows to add a common suffix to the hostnames\n# submitted. This can be used for instance to discriminate hosts between clusters.\nconfig['label_to_hostname_suffix'] = instance.get(\n'label_to_hostname_suffix', default_instance.get('label_to_hostname_suffix', None)\n)\n# Add a 'health' service check for the prometheus endpoint\nconfig['health_service_check'] = is_affirmative(\ninstance.get('health_service_check', default_instance.get('health_service_check', True))\n)\n# Can either be only the path to the certificate and thus you should specify the private key\n# or it can be the path to a file containing both the certificate &amp; the private key\nconfig['ssl_cert'] = instance.get('ssl_cert', default_instance.get('ssl_cert', None))\n# Needed if the certificate does not include the private key\n#\n# /!\\ The private key to your local certificate must be unencrypted.\n# Currently, Requests does not support using encrypted keys.\nconfig['ssl_private_key'] = instance.get('ssl_private_key', default_instance.get('ssl_private_key', None))\n# The path to the trusted CA used for generating custom certificates\nconfig['ssl_ca_cert'] = instance.get('ssl_ca_cert', default_instance.get('ssl_ca_cert', None))\n# Whether or not to validate SSL certificates\nconfig['ssl_verify'] = is_affirmative(instance.get('ssl_verify', default_instance.get('ssl_verify', True)))\n# Extra http headers to be sent when polling endpoint\nconfig['extra_headers'] = default_instance.get('extra_headers', {})\nconfig['extra_headers'].update(instance.get('extra_headers', {}))\n# Timeout used during the network request\nconfig['prometheus_timeout'] = instance.get(\n'prometheus_timeout', default_instance.get('prometheus_timeout', 10)\n)\n# Authentication used when polling endpoint\nconfig['username'] = instance.get('username', default_instance.get('username', None))\nconfig['password'] = instance.get('password', default_instance.get('password', None))\n# Custom tags that will be sent with each metric\nconfig['custom_tags'] = instance.get('tags', [])\n# Some tags can be ignored to reduce the cardinality.\n# This can be useful for cost optimization in containerized environments\n# when the openmetrics check is configured to collect custom metrics.\n# Even when the Agent's Tagger is configured to add low-cardinality tags only,\n# some tags can still generate unwanted metric contexts (e.g pod annotations as tags).\nignore_tags = instance.get('ignore_tags', default_instance.get('ignore_tags', []))\nif ignore_tags:\nignored_tags_re = compile('|'.join(set(ignore_tags)))\nconfig['custom_tags'] = [tag for tag in config['custom_tags'] if not ignored_tags_re.search(tag)]\n# Additional tags to be sent with each metric\nconfig['_metric_tags'] = []\n# List of strings to filter the input text payload on. If any line contains\n# one of these strings, it will be filtered out before being parsed.\n# INTERNAL FEATURE, might be removed in future versions\nconfig['_text_filter_blacklist'] = []\n# Refresh the bearer token every 60 seconds by default.\n# Ref https://github.com/DataDog/datadog-agent/pull/11686\nconfig['bearer_token_refresh_interval'] = instance.get(\n'bearer_token_refresh_interval', default_instance.get('bearer_token_refresh_interval', 60)\n)\nconfig['telemetry'] = is_affirmative(instance.get('telemetry', default_instance.get('telemetry', False)))\n# The metric name services use to indicate build information\nconfig['metadata_metric_name'] = instance.get(\n'metadata_metric_name', default_instance.get('metadata_metric_name')\n)\n# Map of metadata key names to label names\nconfig['metadata_label_map'] = instance.get(\n'metadata_label_map', default_instance.get('metadata_label_map', {})\n)\nconfig['_default_metric_transformers'] = {}\nif config['metadata_metric_name'] and config['metadata_label_map']:\nconfig['_default_metric_transformers'][config['metadata_metric_name']] = self.transform_metadata\n# Whether or not to enable flushing of the first value of monotonic counts\nconfig['_flush_first_value'] = False\n# Whether to use process_start_time_seconds to decide if counter-like values should  be flushed\n# on first scrape.\nconfig['use_process_start_time'] = is_affirmative(_get_setting('use_process_start_time', False))\nreturn config\ndef get_http_handler(self, scraper_config):\n\"\"\"\n        Get http handler for a specific scraper config.\n        The http handler is cached using `prometheus_url` as key.\n        The http handler doesn't use the cache if a bearer token is used to allow refreshing it.\n        \"\"\"\nprometheus_url = scraper_config['prometheus_url']\nbearer_token = scraper_config['_bearer_token']\nif prometheus_url in self._http_handlers and bearer_token is None:\nreturn self._http_handlers[prometheus_url]\n# TODO: Deprecate this behavior in Agent 8\nif scraper_config['ssl_ca_cert'] is False:\nscraper_config['ssl_verify'] = False\n# TODO: Deprecate this behavior in Agent 8\nif scraper_config['ssl_verify'] is False:\nscraper_config.setdefault('tls_ignore_warning', True)\nhttp_handler = self._http_handlers[prometheus_url] = RequestsWrapper(\nscraper_config, self.init_config, self.HTTP_CONFIG_REMAPPER, self.log\n)\nheaders = http_handler.options['headers']\nbearer_token = scraper_config['_bearer_token']\nif bearer_token is not None:\nheaders['Authorization'] = 'Bearer {}'.format(bearer_token)\n# TODO: Determine if we really need this\nheaders.setdefault('accept-encoding', 'gzip')\n# Explicitly set the content type we accept\nheaders.setdefault('accept', 'text/plain')\nreturn http_handler\ndef reset_http_config(self):\n\"\"\"\n        You may need to use this when configuration is determined dynamically during every\n        check run, such as when polling an external resource like the Kubelet.\n        \"\"\"\nself._http_handlers.clear()\ndef update_prometheus_url(self, instance, config, endpoint):\nif not endpoint:\nreturn\nconfig['prometheus_url'] = endpoint\n# Whether or not to use the service account bearer token for authentication.\n# Can be explicitly set to true or false to send or not the bearer token.\n# If set to the `tls_only` value, the bearer token will be sent only to https endpoints.\n# If 'bearer_token_path' is not set, we use /var/run/secrets/kubernetes.io/serviceaccount/token\n# as a default path to get the token.\nnamespace = instance.get('namespace')\ndefault_instance = self.default_instances.get(namespace, {})\nbearer_token_auth = instance.get('bearer_token_auth', default_instance.get('bearer_token_auth', False))\nif bearer_token_auth == 'tls_only':\nconfig['bearer_token_auth'] = config['prometheus_url'].startswith(\"https://\")\nelse:\nconfig['bearer_token_auth'] = is_affirmative(bearer_token_auth)\n# Can be used to get a service account bearer token from files\n# other than /var/run/secrets/kubernetes.io/serviceaccount/token\n# 'bearer_token_auth' should be enabled.\nconfig['bearer_token_path'] = instance.get('bearer_token_path', default_instance.get('bearer_token_path', None))\n# The service account bearer token to be used for authentication\nconfig['_bearer_token'] = self._get_bearer_token(config['bearer_token_auth'], config['bearer_token_path'])\nconfig['_bearer_token_last_refresh'] = time.time()\ndef parse_metric_family(self, response, scraper_config):\n\"\"\"\n        Parse the MetricFamily from a valid `requests.Response` object to provide a MetricFamily object.\n        The text format uses iter_lines() generator.\n        \"\"\"\nif response.encoding is None:\nresponse.encoding = 'utf-8'\ninput_gen = response.iter_lines(decode_unicode=True)\nif scraper_config['_text_filter_blacklist']:\ninput_gen = self._text_filter_input(input_gen, scraper_config)\nfor metric in text_fd_to_metric_families(input_gen):\nself._send_telemetry_counter(\nself.TELEMETRY_COUNTER_METRICS_INPUT_COUNT, len(metric.samples), scraper_config\n)\ntype_override = scraper_config['type_overrides'].get(metric.name)\nif type_override:\nmetric.type = type_override\nelif scraper_config['_type_override_patterns']:\nfor pattern, new_type in iteritems(scraper_config['_type_override_patterns']):\nif pattern.search(metric.name):\nmetric.type = new_type\nbreak\nif metric.type not in self.METRIC_TYPES:\ncontinue\nmetric.name = self._remove_metric_prefix(metric.name, scraper_config)\nyield metric\ndef _text_filter_input(self, input_gen, scraper_config):\n\"\"\"\n        Filters out the text input line by line to avoid parsing and processing\n        metrics we know we don't want to process. This only works on `text/plain`\n        payloads, and is an INTERNAL FEATURE implemented for the kubelet check\n        :param input_get: line generator\n        :output: generator of filtered lines\n        \"\"\"\nfor line in input_gen:\nfor item in scraper_config['_text_filter_blacklist']:\nif item in line:\nself._send_telemetry_counter(self.TELEMETRY_COUNTER_METRICS_BLACKLIST_COUNT, 1, scraper_config)\nbreak\nelse:\n# No blacklist matches, passing the line through\nyield line\ndef _remove_metric_prefix(self, metric, scraper_config):\nprometheus_metrics_prefix = scraper_config['prometheus_metrics_prefix']\nreturn metric[len(prometheus_metrics_prefix) :] if metric.startswith(prometheus_metrics_prefix) else metric\ndef scrape_metrics(self, scraper_config):\n\"\"\"\n        Poll the data from Prometheus and return the metrics as a generator.\n        \"\"\"\nresponse = self.poll(scraper_config)\nif scraper_config['telemetry']:\nif 'content-length' in response.headers:\ncontent_len = int(response.headers['content-length'])\nelse:\ncontent_len = len(response.content)\nself._send_telemetry_gauge(self.TELEMETRY_GAUGE_MESSAGE_SIZE, content_len, scraper_config)\ntry:\n# no dry run if no label joins\nif not scraper_config['label_joins']:\nscraper_config['_dry_run'] = False\nelif not scraper_config['_watched_labels']:\nwatched = scraper_config['_watched_labels']\nwatched['sets'] = {}\nwatched['keys'] = {}\nwatched['singles'] = set()\nfor key, val in iteritems(scraper_config['label_joins']):\nlabels = []\nif 'labels_to_match' in val:\nlabels = val['labels_to_match']\nelif 'label_to_match' in val:\nself.log.warning(\"`label_to_match` is being deprecated, please use `labels_to_match`\")\nif isinstance(val['label_to_match'], list):\nlabels = val['label_to_match']\nelse:\nlabels = [val['label_to_match']]\nif labels:\ns = frozenset(labels)\nwatched['sets'][key] = s\nwatched['keys'][key] = ','.join(s)\nif len(labels) == 1:\nwatched['singles'].add(labels[0])\nfor metric in self.parse_metric_family(response, scraper_config):\nyield metric\n# Set dry run off\nscraper_config['_dry_run'] = False\n# Garbage collect unused mapping and reset active labels\nfor metric, mapping in list(iteritems(scraper_config['_label_mapping'])):\nfor key in list(mapping):\nif (\nmetric in scraper_config['_active_label_mapping']\nand key not in scraper_config['_active_label_mapping'][metric]\n):\ndel scraper_config['_label_mapping'][metric][key]\nscraper_config['_active_label_mapping'] = {}\nfinally:\nresponse.close()\ndef process(self, scraper_config, metric_transformers=None):\n\"\"\"\n        Polls the data from Prometheus and submits them as Datadog metrics.\n        `endpoint` is the metrics endpoint to use to poll metrics from Prometheus\n        Note that if the instance has a `tags` attribute, it will be pushed\n        automatically as additional custom tags and added to the metrics\n        \"\"\"\ntransformers = scraper_config['_default_metric_transformers'].copy()\nif metric_transformers:\ntransformers.update(metric_transformers)\ncounter_buffer = []\nagent_start_time = None\nprocess_start_time = None\nif not scraper_config['_flush_first_value'] and scraper_config['use_process_start_time']:\nagent_start_time = datadog_agent.get_process_start_time()\nif scraper_config['bearer_token_auth']:\nself._refresh_bearer_token(scraper_config)\nfor metric in self.scrape_metrics(scraper_config):\nif agent_start_time is not None:\nif metric.name == 'process_start_time_seconds' and metric.samples:\nmin_metric_value = min(s[self.SAMPLE_VALUE] for s in metric.samples)\nif process_start_time is None or min_metric_value &lt; process_start_time:\nprocess_start_time = min_metric_value\nif metric.type in self.METRICS_WITH_COUNTERS:\ncounter_buffer.append(metric)\ncontinue\nself.process_metric(metric, scraper_config, metric_transformers=transformers)\nif agent_start_time and process_start_time and agent_start_time &lt; process_start_time:\n# If agent was started before the process, we assume counters were started recently from zero,\n# and thus we can compute the rates.\nscraper_config['_flush_first_value'] = True\nfor metric in counter_buffer:\nself.process_metric(metric, scraper_config, metric_transformers=transformers)\nscraper_config['_flush_first_value'] = True\ndef transform_metadata(self, metric, scraper_config):\nlabels = metric.samples[0][self.SAMPLE_LABELS]\nfor metadata_name, label_name in iteritems(scraper_config['metadata_label_map']):\nif label_name in labels:\nself.set_metadata(metadata_name, labels[label_name])\ndef _metric_name_with_namespace(self, metric_name, scraper_config):\nnamespace = scraper_config['namespace']\nif not namespace:\nreturn metric_name\nreturn '{}.{}'.format(namespace, metric_name)\ndef _telemetry_metric_name_with_namespace(self, metric_name, scraper_config):\nnamespace = scraper_config['namespace']\nif not namespace:\nreturn '{}.{}'.format('telemetry', metric_name)\nreturn '{}.{}.{}'.format(namespace, 'telemetry', metric_name)\ndef _send_telemetry_gauge(self, metric_name, val, scraper_config):\nif scraper_config['telemetry']:\nmetric_name_with_namespace = self._telemetry_metric_name_with_namespace(metric_name, scraper_config)\n# Determine the tags to send\ncustom_tags = scraper_config['custom_tags']\ntags = list(custom_tags)\ntags.extend(scraper_config['_metric_tags'])\nself.gauge(metric_name_with_namespace, val, tags=tags)\ndef _send_telemetry_counter(self, metric_name, val, scraper_config, extra_tags=None):\nif scraper_config['telemetry']:\nmetric_name_with_namespace = self._telemetry_metric_name_with_namespace(metric_name, scraper_config)\n# Determine the tags to send\ncustom_tags = scraper_config['custom_tags']\ntags = list(custom_tags)\ntags.extend(scraper_config['_metric_tags'])\nif extra_tags:\ntags.extend(extra_tags)\nself.count(metric_name_with_namespace, val, tags=tags)\ndef _store_labels(self, metric, scraper_config):\n# If targeted metric, store labels\nif metric.name not in scraper_config['label_joins']:\nreturn\nwatched = scraper_config['_watched_labels']\nmatching_labels = watched['sets'][metric.name]\nmapping_key = watched['keys'][metric.name]\nlabels_to_get = scraper_config['label_joins'][metric.name]['labels_to_get']\nget_all = '*' in labels_to_get\nmatch_all = mapping_key == '*'\nfor sample in metric.samples:\n# metadata-only metrics that are used for label joins are always equal to 1\n# this is required for metrics where all combinations of a state are sent\n# but only the active one is set to 1 (others are set to 0)\n# example: kube_pod_status_phase in kube-state-metrics\nif sample[self.SAMPLE_VALUE] != 1:\ncontinue\nsample_labels = sample[self.SAMPLE_LABELS]\nsample_labels_keys = sample_labels.keys()\nif match_all or matching_labels.issubset(sample_labels_keys):\nlabel_dict = {}\nif get_all:\nfor label_name, label_value in iteritems(sample_labels):\nif label_name in matching_labels:\ncontinue\nlabel_dict[label_name] = label_value\nelse:\nfor label_name in labels_to_get:\nif label_name in sample_labels:\nlabel_dict[label_name] = sample_labels[label_name]\nif match_all:\nmapping_value = '*'\nelse:\nmapping_value = ','.join([sample_labels[l] for l in matching_labels])\nscraper_config['_label_mapping'].setdefault(mapping_key, {}).setdefault(mapping_value, {}).update(\nlabel_dict\n)\ndef _join_labels(self, metric, scraper_config):\n# Filter metric to see if we can enrich with joined labels\nif not scraper_config['label_joins']:\nreturn\nlabel_mapping = scraper_config['_label_mapping']\nactive_label_mapping = scraper_config['_active_label_mapping']\nwatched = scraper_config['_watched_labels']\nsets = watched['sets']\nkeys = watched['keys']\nsingles = watched['singles']\nfor sample in metric.samples:\nsample_labels = sample[self.SAMPLE_LABELS]\nsample_labels_keys = sample_labels.keys()\n# Match with wildcard label\n# Label names are [a-zA-Z0-9_]*, so no risk of collision\nif '*' in singles:\nactive_label_mapping.setdefault('*', {})['*'] = True\nif '*' in label_mapping and '*' in label_mapping['*']:\nsample_labels.update(label_mapping['*']['*'])\n# Match with single labels\nmatching_single_labels = singles.intersection(sample_labels_keys)\nfor label in matching_single_labels:\nmapping_key = label\nmapping_value = sample_labels[label]\nactive_label_mapping.setdefault(mapping_key, {})[mapping_value] = True\nif mapping_key in label_mapping and mapping_value in label_mapping[mapping_key]:\nsample_labels.update(label_mapping[mapping_key][mapping_value])\n# Match with tuples of labels\nfor key, mapping_key in iteritems(keys):\nif mapping_key in matching_single_labels:\ncontinue\nmatching_labels = sets[key]\nif matching_labels.issubset(sample_labels_keys):\nmatching_values = [sample_labels[l] for l in matching_labels]\nmapping_value = ','.join(matching_values)\nactive_label_mapping.setdefault(mapping_key, {})[mapping_value] = True\nif mapping_key in label_mapping and mapping_value in label_mapping[mapping_key]:\nsample_labels.update(label_mapping[mapping_key][mapping_value])\ndef _ignore_metrics_by_label(self, scraper_config, metric_name, sample):\nignore_metrics_by_label = scraper_config['ignore_metrics_by_labels']\nsample_labels = sample[self.SAMPLE_LABELS]\nfor label_key, label_values in ignore_metrics_by_label.items():\nif not label_values:\nself.log.debug(\n\"Skipping filter label `%s` with an empty values list, did you mean to use '*' wildcard?\", label_key\n)\nelif '*' in label_values:\n# Wildcard '*' means all metrics with label_key will be ignored\nself.log.debug(\"Detected wildcard for label `%s`\", label_key)\nif label_key in sample_labels.keys():\nself.log.debug(\"Skipping metric `%s` due to label key matching: %s\", metric_name, label_key)\nreturn True\nelse:\nfor val in label_values:\nif label_key in sample_labels and sample_labels[label_key] == val:\nself.log.debug(\n\"Skipping metric `%s` due to label `%s` value matching: %s\", metric_name, label_key, val\n)\nreturn True\nreturn False\ndef process_metric(self, metric, scraper_config, metric_transformers=None):\n\"\"\"\n        Handle a Prometheus metric according to the following flow:\n        - search `scraper_config['metrics_mapper']` for a prometheus.metric to datadog.metric mapping\n        - call check method with the same name as the metric\n        - log info if none of the above worked\n        `metric_transformers` is a dict of `&lt;metric name&gt;:&lt;function to run when the metric name is encountered&gt;`\n        \"\"\"\n# If targeted metric, store labels\nself._store_labels(metric, scraper_config)\nif scraper_config['ignore_metrics']:\nif metric.name in scraper_config['_ignored_metrics']:\nself._send_telemetry_counter(\nself.TELEMETRY_COUNTER_METRICS_IGNORE_COUNT, len(metric.samples), scraper_config\n)\nreturn  # Ignore the metric\nif scraper_config['_ignored_re'] and scraper_config['_ignored_re'].search(metric.name):\n# Metric must be ignored\nscraper_config['_ignored_metrics'].add(metric.name)\nself._send_telemetry_counter(\nself.TELEMETRY_COUNTER_METRICS_IGNORE_COUNT, len(metric.samples), scraper_config\n)\nreturn  # Ignore the metric\nself._send_telemetry_counter(self.TELEMETRY_COUNTER_METRICS_PROCESS_COUNT, len(metric.samples), scraper_config)\nif self._filter_metric(metric, scraper_config):\nreturn  # Ignore the metric\n# Filter metric to see if we can enrich with joined labels\nself._join_labels(metric, scraper_config)\nif scraper_config['_dry_run']:\nreturn\ntry:\nself.submit_openmetric(scraper_config['metrics_mapper'][metric.name], metric, scraper_config)\nexcept KeyError:\nif metric_transformers is not None and metric.name in metric_transformers:\ntry:\n# Get the transformer function for this specific metric\ntransformer = metric_transformers[metric.name]\ntransformer(metric, scraper_config)\nexcept Exception as err:\nself.log.warning('Error handling metric: %s - error: %s', metric.name, err)\nreturn\n# check for wildcards in transformers\nfor transformer_name, transformer in iteritems(metric_transformers):\nif transformer_name.endswith('*') and metric.name.startswith(transformer_name[:-1]):\ntransformer(metric, scraper_config, transformer_name)\n# try matching wildcards\nif scraper_config['_wildcards_re'] and scraper_config['_wildcards_re'].search(metric.name):\nself.submit_openmetric(metric.name, metric, scraper_config)\nreturn\nself.log.debug(\n'Skipping metric `%s` as it is not defined in the metrics mapper, '\n'has no transformer function, nor does it match any wildcards.',\nmetric.name,\n)\ndef poll(self, scraper_config, headers=None):\n\"\"\"\n        Returns a valid `requests.Response`, otherwise raise requests.HTTPError if the status code of the\n        response isn't valid - see `response.raise_for_status()`\n        The caller needs to close the requests.Response.\n        Custom headers can be added to the default headers.\n        \"\"\"\nendpoint = scraper_config.get('prometheus_url')\n# Should we send a service check for when we make a request\nhealth_service_check = scraper_config['health_service_check']\nservice_check_name = self._metric_name_with_namespace('prometheus.health', scraper_config)\nservice_check_tags = ['endpoint:{}'.format(endpoint)]\nservice_check_tags.extend(scraper_config['custom_tags'])\ntry:\nresponse = self.send_request(endpoint, scraper_config, headers)\nexcept requests.exceptions.SSLError:\nself.log.error(\"Invalid SSL settings for requesting %s endpoint\", endpoint)\nraise\nexcept IOError:\nif health_service_check:\nself.service_check(service_check_name, AgentCheck.CRITICAL, tags=service_check_tags)\nraise\ntry:\nresponse.raise_for_status()\nif health_service_check:\nself.service_check(service_check_name, AgentCheck.OK, tags=service_check_tags)\nreturn response\nexcept requests.HTTPError:\nresponse.close()\nif health_service_check:\nself.service_check(service_check_name, AgentCheck.CRITICAL, tags=service_check_tags)\nraise\ndef send_request(self, endpoint, scraper_config, headers=None):\nkwargs = {}\nif headers:\nkwargs['headers'] = headers\nhttp_handler = self.get_http_handler(scraper_config)\nreturn http_handler.get(endpoint, stream=True, **kwargs)\ndef get_hostname_for_sample(self, sample, scraper_config):\n\"\"\"\n        Expose the label_to_hostname mapping logic to custom handler methods\n        \"\"\"\nreturn self._get_hostname(None, sample, scraper_config)\ndef submit_openmetric(self, metric_name, metric, scraper_config, hostname=None):\n\"\"\"\n        For each sample in the metric, report it as a gauge with all labels as tags\n        except if a labels `dict` is passed, in which case keys are label names we'll extract\n        and corresponding values are tag names we'll use (eg: {'node': 'node'}).\n        Histograms generate a set of values instead of a unique metric.\n        `send_histograms_buckets` is used to specify if you want to\n        send the buckets as tagged values when dealing with histograms.\n        `custom_tags` is an array of `tag:value` that will be added to the\n        metric when sending the gauge to Datadog.\n        \"\"\"\nif metric.type in [\"gauge\", \"counter\", \"rate\"]:\nmetric_name_with_namespace = self._metric_name_with_namespace(metric_name, scraper_config)\nfor sample in metric.samples:\nif self._ignore_metrics_by_label(scraper_config, metric_name, sample):\ncontinue\nval = sample[self.SAMPLE_VALUE]\nif not self._is_value_valid(val):\nself.log.debug(\"Metric value is not supported for metric %s\", sample[self.SAMPLE_NAME])\ncontinue\ncustom_hostname = self._get_hostname(hostname, sample, scraper_config)\n# Determine the tags to send\ntags = self._metric_tags(metric_name, val, sample, scraper_config, hostname=custom_hostname)\nif metric.type == \"counter\" and scraper_config['send_monotonic_counter']:\nself.monotonic_count(\nmetric_name_with_namespace,\nval,\ntags=tags,\nhostname=custom_hostname,\nflush_first_value=scraper_config['_flush_first_value'],\n)\nelif metric.type == \"rate\":\nself.rate(metric_name_with_namespace, val, tags=tags, hostname=custom_hostname)\nelse:\nself.gauge(metric_name_with_namespace, val, tags=tags, hostname=custom_hostname)\n# Metric is a \"counter\" but legacy behavior has \"send_as_monotonic\" defaulted to False\n# Submit metric as monotonic_count with appended name\nif metric.type == \"counter\" and scraper_config['send_monotonic_with_gauge']:\nself.monotonic_count(\nmetric_name_with_namespace + '.total',\nval,\ntags=tags,\nhostname=custom_hostname,\nflush_first_value=scraper_config['_flush_first_value'],\n)\nelif metric.type == \"histogram\":\nself._submit_gauges_from_histogram(metric_name, metric, scraper_config)\nelif metric.type == \"summary\":\nself._submit_gauges_from_summary(metric_name, metric, scraper_config)\nelse:\nself.log.error(\"Metric type %s unsupported for metric %s.\", metric.type, metric_name)\ndef _get_hostname(self, hostname, sample, scraper_config):\n\"\"\"\n        If hostname is None, look at label_to_hostname setting\n        \"\"\"\nif (\nhostname is None\nand scraper_config['label_to_hostname'] is not None\nand sample[self.SAMPLE_LABELS].get(scraper_config['label_to_hostname'])\n):\nhostname = sample[self.SAMPLE_LABELS][scraper_config['label_to_hostname']]\nsuffix = scraper_config['label_to_hostname_suffix']\nif suffix is not None:\nhostname += suffix\nreturn hostname\ndef _submit_gauges_from_summary(self, metric_name, metric, scraper_config, hostname=None):\n\"\"\"\n        Extracts metrics from a prometheus summary metric and sends them as gauges\n        \"\"\"\nfor sample in metric.samples:\nval = sample[self.SAMPLE_VALUE]\nif not self._is_value_valid(val):\nself.log.debug(\"Metric value is not supported for metric %s\", sample[self.SAMPLE_NAME])\ncontinue\nif self._ignore_metrics_by_label(scraper_config, metric_name, sample):\ncontinue\ncustom_hostname = self._get_hostname(hostname, sample, scraper_config)\nif sample[self.SAMPLE_NAME].endswith(\"_sum\"):\ntags = self._metric_tags(metric_name, val, sample, scraper_config, hostname=custom_hostname)\nself._submit_distribution_count(\nscraper_config['send_distribution_sums_as_monotonic'],\nscraper_config['send_monotonic_with_gauge'],\n\"{}.sum\".format(self._metric_name_with_namespace(metric_name, scraper_config)),\nval,\ntags=tags,\nhostname=custom_hostname,\nflush_first_value=scraper_config['_flush_first_value'],\n)\nelif sample[self.SAMPLE_NAME].endswith(\"_count\"):\ntags = self._metric_tags(metric_name, val, sample, scraper_config, hostname=custom_hostname)\nself._submit_distribution_count(\nscraper_config['send_distribution_counts_as_monotonic'],\nscraper_config['send_monotonic_with_gauge'],\n\"{}.count\".format(self._metric_name_with_namespace(metric_name, scraper_config)),\nval,\ntags=tags,\nhostname=custom_hostname,\nflush_first_value=scraper_config['_flush_first_value'],\n)\nelse:\ntry:\nquantile = sample[self.SAMPLE_LABELS][\"quantile\"]\nexcept KeyError:\n# TODO: In the Prometheus spec the 'quantile' label is optional, but it's not clear yet\n# what we should do in this case. Let's skip for now and submit the rest of metrics.\nmessage = (\n'\"quantile\" label not present in metric %r. '\n'Quantile-less summary metrics are not currently supported. Skipping...'\n)\nself.log.debug(message, metric_name)\ncontinue\nsample[self.SAMPLE_LABELS][\"quantile\"] = str(float(quantile))\ntags = self._metric_tags(metric_name, val, sample, scraper_config, hostname=custom_hostname)\nself.gauge(\n\"{}.quantile\".format(self._metric_name_with_namespace(metric_name, scraper_config)),\nval,\ntags=tags,\nhostname=custom_hostname,\n)\ndef _submit_gauges_from_histogram(self, metric_name, metric, scraper_config, hostname=None):\n\"\"\"\n        Extracts metrics from a prometheus histogram and sends them as gauges\n        \"\"\"\nif scraper_config['non_cumulative_buckets']:\nself._decumulate_histogram_buckets(metric)\nfor sample in metric.samples:\nval = sample[self.SAMPLE_VALUE]\nif not self._is_value_valid(val):\nself.log.debug(\"Metric value is not supported for metric %s\", sample[self.SAMPLE_NAME])\ncontinue\nif self._ignore_metrics_by_label(scraper_config, metric_name, sample):\ncontinue\ncustom_hostname = self._get_hostname(hostname, sample, scraper_config)\nif sample[self.SAMPLE_NAME].endswith(\"_sum\") and not scraper_config['send_distribution_buckets']:\ntags = self._metric_tags(metric_name, val, sample, scraper_config, hostname)\nself._submit_distribution_count(\nscraper_config['send_distribution_sums_as_monotonic'],\nscraper_config['send_monotonic_with_gauge'],\n\"{}.sum\".format(self._metric_name_with_namespace(metric_name, scraper_config)),\nval,\ntags=tags,\nhostname=custom_hostname,\nflush_first_value=scraper_config['_flush_first_value'],\n)\nelif sample[self.SAMPLE_NAME].endswith(\"_count\") and not scraper_config['send_distribution_buckets']:\ntags = self._metric_tags(metric_name, val, sample, scraper_config, hostname)\nif scraper_config['send_histograms_buckets']:\ntags.append(\"upper_bound:none\")\nself._submit_distribution_count(\nscraper_config['send_distribution_counts_as_monotonic'],\nscraper_config['send_monotonic_with_gauge'],\n\"{}.count\".format(self._metric_name_with_namespace(metric_name, scraper_config)),\nval,\ntags=tags,\nhostname=custom_hostname,\nflush_first_value=scraper_config['_flush_first_value'],\n)\nelif scraper_config['send_histograms_buckets'] and sample[self.SAMPLE_NAME].endswith(\"_bucket\"):\nif scraper_config['send_distribution_buckets']:\nself._submit_sample_histogram_buckets(metric_name, sample, scraper_config, hostname)\nelif \"Inf\" not in sample[self.SAMPLE_LABELS][\"le\"] or scraper_config['non_cumulative_buckets']:\nsample[self.SAMPLE_LABELS][\"le\"] = str(float(sample[self.SAMPLE_LABELS][\"le\"]))\ntags = self._metric_tags(metric_name, val, sample, scraper_config, hostname)\nself._submit_distribution_count(\nscraper_config['send_distribution_counts_as_monotonic'],\nscraper_config['send_monotonic_with_gauge'],\n\"{}.count\".format(self._metric_name_with_namespace(metric_name, scraper_config)),\nval,\ntags=tags,\nhostname=custom_hostname,\nflush_first_value=scraper_config['_flush_first_value'],\n)\ndef _compute_bucket_hash(self, tags):\n# we need the unique context for all the buckets\n# hence we remove the \"le\" tag\nreturn hash(frozenset(sorted((k, v) for k, v in iteritems(tags) if k != 'le')))\ndef _decumulate_histogram_buckets(self, metric):\n\"\"\"\n        Decumulate buckets in a given histogram metric and adds the lower_bound label (le being upper_bound)\n        \"\"\"\nbucket_values_by_context_upper_bound = {}\nfor sample in metric.samples:\nif sample[self.SAMPLE_NAME].endswith(\"_bucket\"):\ncontext_key = self._compute_bucket_hash(sample[self.SAMPLE_LABELS])\nif context_key not in bucket_values_by_context_upper_bound:\nbucket_values_by_context_upper_bound[context_key] = {}\nbucket_values_by_context_upper_bound[context_key][float(sample[self.SAMPLE_LABELS][\"le\"])] = sample[\nself.SAMPLE_VALUE\n]\nsorted_buckets_by_context = {}\nfor context in bucket_values_by_context_upper_bound:\nsorted_buckets_by_context[context] = sorted(bucket_values_by_context_upper_bound[context])\n# Tuples (lower_bound, upper_bound, value)\nbucket_tuples_by_context_upper_bound = {}\nfor context in sorted_buckets_by_context:\nfor i, upper_b in enumerate(sorted_buckets_by_context[context]):\nif i == 0:\nif context not in bucket_tuples_by_context_upper_bound:\nbucket_tuples_by_context_upper_bound[context] = {}\nif upper_b &gt; 0:\n# positive buckets start at zero\nbucket_tuples_by_context_upper_bound[context][upper_b] = (\n0,\nupper_b,\nbucket_values_by_context_upper_bound[context][upper_b],\n)\nelse:\n# negative buckets start at -inf\nbucket_tuples_by_context_upper_bound[context][upper_b] = (\nself.MINUS_INF,\nupper_b,\nbucket_values_by_context_upper_bound[context][upper_b],\n)\ncontinue\ntmp = (\nbucket_values_by_context_upper_bound[context][upper_b]\n- bucket_values_by_context_upper_bound[context][sorted_buckets_by_context[context][i - 1]]\n)\nbucket_tuples_by_context_upper_bound[context][upper_b] = (\nsorted_buckets_by_context[context][i - 1],\nupper_b,\ntmp,\n)\n# modify original metric to inject lower_bound &amp; modified value\nfor i, sample in enumerate(metric.samples):\nif not sample[self.SAMPLE_NAME].endswith(\"_bucket\"):\ncontinue\ncontext_key = self._compute_bucket_hash(sample[self.SAMPLE_LABELS])\nmatching_bucket_tuple = bucket_tuples_by_context_upper_bound[context_key][\nfloat(sample[self.SAMPLE_LABELS][\"le\"])\n]\n# Replacing the sample tuple\nsample[self.SAMPLE_LABELS][\"lower_bound\"] = str(matching_bucket_tuple[0])\nmetric.samples[i] = Sample(sample[self.SAMPLE_NAME], sample[self.SAMPLE_LABELS], matching_bucket_tuple[2])\ndef _submit_sample_histogram_buckets(self, metric_name, sample, scraper_config, hostname=None):\nif \"lower_bound\" not in sample[self.SAMPLE_LABELS] or \"le\" not in sample[self.SAMPLE_LABELS]:\nself.log.warning(\n\"Metric: %s was not containing required bucket boundaries labels: %s\",\nmetric_name,\nsample[self.SAMPLE_LABELS],\n)\nreturn\nsample[self.SAMPLE_LABELS][\"le\"] = str(float(sample[self.SAMPLE_LABELS][\"le\"]))\nsample[self.SAMPLE_LABELS][\"lower_bound\"] = str(float(sample[self.SAMPLE_LABELS][\"lower_bound\"]))\nif sample[self.SAMPLE_LABELS][\"le\"] == sample[self.SAMPLE_LABELS][\"lower_bound\"]:\n# this can happen for -inf/-inf bucket that we don't want to send (always 0)\nself.log.warning(\n\"Metric: %s has bucket boundaries equal, skipping: %s\", metric_name, sample[self.SAMPLE_LABELS]\n)\nreturn\ntags = self._metric_tags(metric_name, sample[self.SAMPLE_VALUE], sample, scraper_config, hostname)\nself.submit_histogram_bucket(\nself._metric_name_with_namespace(metric_name, scraper_config),\nsample[self.SAMPLE_VALUE],\nfloat(sample[self.SAMPLE_LABELS][\"lower_bound\"]),\nfloat(sample[self.SAMPLE_LABELS][\"le\"]),\nTrue,\nhostname,\ntags,\nflush_first_value=scraper_config['_flush_first_value'],\n)\ndef _submit_distribution_count(\nself,\nmonotonic,\nsend_monotonic_with_gauge,\nmetric_name,\nvalue,\ntags=None,\nhostname=None,\nflush_first_value=False,\n):\nif monotonic:\nself.monotonic_count(metric_name, value, tags=tags, hostname=hostname, flush_first_value=flush_first_value)\nelse:\nself.gauge(metric_name, value, tags=tags, hostname=hostname)\nif send_monotonic_with_gauge:\nself.monotonic_count(\nmetric_name + \".total\", value, tags=tags, hostname=hostname, flush_first_value=flush_first_value\n)\ndef _metric_tags(self, metric_name, val, sample, scraper_config, hostname=None):\ncustom_tags = scraper_config['custom_tags']\n_tags = list(custom_tags)\n_tags.extend(scraper_config['_metric_tags'])\nfor label_name, label_value in iteritems(sample[self.SAMPLE_LABELS]):\nif label_name not in scraper_config['exclude_labels']:\nif label_name in scraper_config['include_labels'] or len(scraper_config['include_labels']) == 0:\ntag_name = scraper_config['labels_mapper'].get(label_name, label_name)\n_tags.append('{}:{}'.format(to_native_string(tag_name), to_native_string(label_value)))\nreturn self._finalize_tags_to_submit(\n_tags, metric_name, val, sample, custom_tags=custom_tags, hostname=hostname\n)\ndef _is_value_valid(self, val):\nreturn not (isnan(val) or isinf(val))\ndef _get_bearer_token(self, bearer_token_auth, bearer_token_path):\nif bearer_token_auth is False:\nreturn None\npath = None\nif bearer_token_path is not None:\nif isfile(bearer_token_path):\npath = bearer_token_path\nelse:\nself.log.error(\"File not found: %s\", bearer_token_path)\nelif isfile(self.KUBERNETES_TOKEN_PATH):\npath = self.KUBERNETES_TOKEN_PATH\nif path is None:\nself.log.error(\"Cannot get bearer token from bearer_token_path or auto discovery\")\nraise IOError(\"Cannot get bearer token from bearer_token_path or auto discovery\")\ntry:\nwith open(path, 'r') as f:\nreturn f.read().rstrip()\nexcept Exception as err:\nself.log.error(\"Cannot get bearer token from path: %s - error: %s\", path, err)\nraise\ndef _refresh_bearer_token(self, scraper_config):\n\"\"\"\n        Refreshes the bearer token if the refresh interval is elapsed.\n        \"\"\"\nnow = time.time()\nif now - scraper_config['_bearer_token_last_refresh'] &gt; scraper_config['bearer_token_refresh_interval']:\nscraper_config['_bearer_token'] = self._get_bearer_token(\nscraper_config['bearer_token_auth'], scraper_config['bearer_token_path']\n)\nscraper_config['_bearer_token_last_refresh'] = now\ndef _histogram_convert_values(self, metric_name, converter):\ndef _convert(metric, scraper_config=None):\nfor index, sample in enumerate(metric.samples):\nval = sample[self.SAMPLE_VALUE]\nif not self._is_value_valid(val):\nself.log.debug(\"Metric value is not supported for metric %s\", sample[self.SAMPLE_NAME])\ncontinue\nif sample[self.SAMPLE_NAME].endswith(\"_sum\"):\nlst = list(sample)\nlst[self.SAMPLE_VALUE] = converter(val)\nmetric.samples[index] = tuple(lst)\nelif sample[self.SAMPLE_NAME].endswith(\"_bucket\") and \"Inf\" not in sample[self.SAMPLE_LABELS][\"le\"]:\nsample[self.SAMPLE_LABELS][\"le\"] = str(converter(float(sample[self.SAMPLE_LABELS][\"le\"])))\nself.submit_openmetric(metric_name, metric, scraper_config)\nreturn _convert\ndef _histogram_from_microseconds_to_seconds(self, metric_name):\nreturn self._histogram_convert_values(metric_name, lambda v: v / self.MICROS_IN_S)\ndef _histogram_from_seconds_to_microseconds(self, metric_name):\nreturn self._histogram_convert_values(metric_name, lambda v: v * self.MICROS_IN_S)\ndef _summary_convert_values(self, metric_name, converter):\ndef _convert(metric, scraper_config=None):\nfor index, sample in enumerate(metric.samples):\nval = sample[self.SAMPLE_VALUE]\nif not self._is_value_valid(val):\nself.log.debug(\"Metric value is not supported for metric %s\", sample[self.SAMPLE_NAME])\ncontinue\nif sample[self.SAMPLE_NAME].endswith(\"_count\"):\ncontinue\nelse:\nlst = list(sample)\nlst[self.SAMPLE_VALUE] = converter(val)\nmetric.samples[index] = tuple(lst)\nself.submit_openmetric(metric_name, metric, scraper_config)\nreturn _convert\ndef _summary_from_microseconds_to_seconds(self, metric_name):\nreturn self._summary_convert_values(metric_name, lambda v: v / self.MICROS_IN_S)\ndef _summary_from_seconds_to_microseconds(self, metric_name):\nreturn self._summary_convert_values(metric_name, lambda v: v * self.MICROS_IN_S)\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.mixins.OpenMetricsScraperMixin.parse_metric_family","title":"<code>parse_metric_family(response, scraper_config)</code>","text":"<p>Parse the MetricFamily from a valid <code>requests.Response</code> object to provide a MetricFamily object. The text format uses iter_lines() generator.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/mixins.py</code> <pre><code>def parse_metric_family(self, response, scraper_config):\n\"\"\"\n    Parse the MetricFamily from a valid `requests.Response` object to provide a MetricFamily object.\n    The text format uses iter_lines() generator.\n    \"\"\"\nif response.encoding is None:\nresponse.encoding = 'utf-8'\ninput_gen = response.iter_lines(decode_unicode=True)\nif scraper_config['_text_filter_blacklist']:\ninput_gen = self._text_filter_input(input_gen, scraper_config)\nfor metric in text_fd_to_metric_families(input_gen):\nself._send_telemetry_counter(\nself.TELEMETRY_COUNTER_METRICS_INPUT_COUNT, len(metric.samples), scraper_config\n)\ntype_override = scraper_config['type_overrides'].get(metric.name)\nif type_override:\nmetric.type = type_override\nelif scraper_config['_type_override_patterns']:\nfor pattern, new_type in iteritems(scraper_config['_type_override_patterns']):\nif pattern.search(metric.name):\nmetric.type = new_type\nbreak\nif metric.type not in self.METRIC_TYPES:\ncontinue\nmetric.name = self._remove_metric_prefix(metric.name, scraper_config)\nyield metric\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.mixins.OpenMetricsScraperMixin.scrape_metrics","title":"<code>scrape_metrics(scraper_config)</code>","text":"<p>Poll the data from Prometheus and return the metrics as a generator.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/mixins.py</code> <pre><code>def scrape_metrics(self, scraper_config):\n\"\"\"\n    Poll the data from Prometheus and return the metrics as a generator.\n    \"\"\"\nresponse = self.poll(scraper_config)\nif scraper_config['telemetry']:\nif 'content-length' in response.headers:\ncontent_len = int(response.headers['content-length'])\nelse:\ncontent_len = len(response.content)\nself._send_telemetry_gauge(self.TELEMETRY_GAUGE_MESSAGE_SIZE, content_len, scraper_config)\ntry:\n# no dry run if no label joins\nif not scraper_config['label_joins']:\nscraper_config['_dry_run'] = False\nelif not scraper_config['_watched_labels']:\nwatched = scraper_config['_watched_labels']\nwatched['sets'] = {}\nwatched['keys'] = {}\nwatched['singles'] = set()\nfor key, val in iteritems(scraper_config['label_joins']):\nlabels = []\nif 'labels_to_match' in val:\nlabels = val['labels_to_match']\nelif 'label_to_match' in val:\nself.log.warning(\"`label_to_match` is being deprecated, please use `labels_to_match`\")\nif isinstance(val['label_to_match'], list):\nlabels = val['label_to_match']\nelse:\nlabels = [val['label_to_match']]\nif labels:\ns = frozenset(labels)\nwatched['sets'][key] = s\nwatched['keys'][key] = ','.join(s)\nif len(labels) == 1:\nwatched['singles'].add(labels[0])\nfor metric in self.parse_metric_family(response, scraper_config):\nyield metric\n# Set dry run off\nscraper_config['_dry_run'] = False\n# Garbage collect unused mapping and reset active labels\nfor metric, mapping in list(iteritems(scraper_config['_label_mapping'])):\nfor key in list(mapping):\nif (\nmetric in scraper_config['_active_label_mapping']\nand key not in scraper_config['_active_label_mapping'][metric]\n):\ndel scraper_config['_label_mapping'][metric][key]\nscraper_config['_active_label_mapping'] = {}\nfinally:\nresponse.close()\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.mixins.OpenMetricsScraperMixin.process","title":"<code>process(scraper_config, metric_transformers=None)</code>","text":"<p>Polls the data from Prometheus and submits them as Datadog metrics. <code>endpoint</code> is the metrics endpoint to use to poll metrics from Prometheus</p> <p>Note that if the instance has a <code>tags</code> attribute, it will be pushed automatically as additional custom tags and added to the metrics</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/mixins.py</code> <pre><code>def process(self, scraper_config, metric_transformers=None):\n\"\"\"\n    Polls the data from Prometheus and submits them as Datadog metrics.\n    `endpoint` is the metrics endpoint to use to poll metrics from Prometheus\n    Note that if the instance has a `tags` attribute, it will be pushed\n    automatically as additional custom tags and added to the metrics\n    \"\"\"\ntransformers = scraper_config['_default_metric_transformers'].copy()\nif metric_transformers:\ntransformers.update(metric_transformers)\ncounter_buffer = []\nagent_start_time = None\nprocess_start_time = None\nif not scraper_config['_flush_first_value'] and scraper_config['use_process_start_time']:\nagent_start_time = datadog_agent.get_process_start_time()\nif scraper_config['bearer_token_auth']:\nself._refresh_bearer_token(scraper_config)\nfor metric in self.scrape_metrics(scraper_config):\nif agent_start_time is not None:\nif metric.name == 'process_start_time_seconds' and metric.samples:\nmin_metric_value = min(s[self.SAMPLE_VALUE] for s in metric.samples)\nif process_start_time is None or min_metric_value &lt; process_start_time:\nprocess_start_time = min_metric_value\nif metric.type in self.METRICS_WITH_COUNTERS:\ncounter_buffer.append(metric)\ncontinue\nself.process_metric(metric, scraper_config, metric_transformers=transformers)\nif agent_start_time and process_start_time and agent_start_time &lt; process_start_time:\n# If agent was started before the process, we assume counters were started recently from zero,\n# and thus we can compute the rates.\nscraper_config['_flush_first_value'] = True\nfor metric in counter_buffer:\nself.process_metric(metric, scraper_config, metric_transformers=transformers)\nscraper_config['_flush_first_value'] = True\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.mixins.OpenMetricsScraperMixin.poll","title":"<code>poll(scraper_config, headers=None)</code>","text":"<p>Returns a valid <code>requests.Response</code>, otherwise raise requests.HTTPError if the status code of the response isn't valid - see <code>response.raise_for_status()</code></p> <p>The caller needs to close the requests.Response.</p> <p>Custom headers can be added to the default headers.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/mixins.py</code> <pre><code>def poll(self, scraper_config, headers=None):\n\"\"\"\n    Returns a valid `requests.Response`, otherwise raise requests.HTTPError if the status code of the\n    response isn't valid - see `response.raise_for_status()`\n    The caller needs to close the requests.Response.\n    Custom headers can be added to the default headers.\n    \"\"\"\nendpoint = scraper_config.get('prometheus_url')\n# Should we send a service check for when we make a request\nhealth_service_check = scraper_config['health_service_check']\nservice_check_name = self._metric_name_with_namespace('prometheus.health', scraper_config)\nservice_check_tags = ['endpoint:{}'.format(endpoint)]\nservice_check_tags.extend(scraper_config['custom_tags'])\ntry:\nresponse = self.send_request(endpoint, scraper_config, headers)\nexcept requests.exceptions.SSLError:\nself.log.error(\"Invalid SSL settings for requesting %s endpoint\", endpoint)\nraise\nexcept IOError:\nif health_service_check:\nself.service_check(service_check_name, AgentCheck.CRITICAL, tags=service_check_tags)\nraise\ntry:\nresponse.raise_for_status()\nif health_service_check:\nself.service_check(service_check_name, AgentCheck.OK, tags=service_check_tags)\nreturn response\nexcept requests.HTTPError:\nresponse.close()\nif health_service_check:\nself.service_check(service_check_name, AgentCheck.CRITICAL, tags=service_check_tags)\nraise\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.mixins.OpenMetricsScraperMixin.submit_openmetric","title":"<code>submit_openmetric(metric_name, metric, scraper_config, hostname=None)</code>","text":"<p>For each sample in the metric, report it as a gauge with all labels as tags except if a labels <code>dict</code> is passed, in which case keys are label names we'll extract and corresponding values are tag names we'll use (eg: {'node': 'node'}).</p> <p>Histograms generate a set of values instead of a unique metric. <code>send_histograms_buckets</code> is used to specify if you want to send the buckets as tagged values when dealing with histograms.</p> <p><code>custom_tags</code> is an array of <code>tag:value</code> that will be added to the metric when sending the gauge to Datadog.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/mixins.py</code> <pre><code>def submit_openmetric(self, metric_name, metric, scraper_config, hostname=None):\n\"\"\"\n    For each sample in the metric, report it as a gauge with all labels as tags\n    except if a labels `dict` is passed, in which case keys are label names we'll extract\n    and corresponding values are tag names we'll use (eg: {'node': 'node'}).\n    Histograms generate a set of values instead of a unique metric.\n    `send_histograms_buckets` is used to specify if you want to\n    send the buckets as tagged values when dealing with histograms.\n    `custom_tags` is an array of `tag:value` that will be added to the\n    metric when sending the gauge to Datadog.\n    \"\"\"\nif metric.type in [\"gauge\", \"counter\", \"rate\"]:\nmetric_name_with_namespace = self._metric_name_with_namespace(metric_name, scraper_config)\nfor sample in metric.samples:\nif self._ignore_metrics_by_label(scraper_config, metric_name, sample):\ncontinue\nval = sample[self.SAMPLE_VALUE]\nif not self._is_value_valid(val):\nself.log.debug(\"Metric value is not supported for metric %s\", sample[self.SAMPLE_NAME])\ncontinue\ncustom_hostname = self._get_hostname(hostname, sample, scraper_config)\n# Determine the tags to send\ntags = self._metric_tags(metric_name, val, sample, scraper_config, hostname=custom_hostname)\nif metric.type == \"counter\" and scraper_config['send_monotonic_counter']:\nself.monotonic_count(\nmetric_name_with_namespace,\nval,\ntags=tags,\nhostname=custom_hostname,\nflush_first_value=scraper_config['_flush_first_value'],\n)\nelif metric.type == \"rate\":\nself.rate(metric_name_with_namespace, val, tags=tags, hostname=custom_hostname)\nelse:\nself.gauge(metric_name_with_namespace, val, tags=tags, hostname=custom_hostname)\n# Metric is a \"counter\" but legacy behavior has \"send_as_monotonic\" defaulted to False\n# Submit metric as monotonic_count with appended name\nif metric.type == \"counter\" and scraper_config['send_monotonic_with_gauge']:\nself.monotonic_count(\nmetric_name_with_namespace + '.total',\nval,\ntags=tags,\nhostname=custom_hostname,\nflush_first_value=scraper_config['_flush_first_value'],\n)\nelif metric.type == \"histogram\":\nself._submit_gauges_from_histogram(metric_name, metric, scraper_config)\nelif metric.type == \"summary\":\nself._submit_gauges_from_summary(metric_name, metric, scraper_config)\nelse:\nself.log.error(\"Metric type %s unsupported for metric %s.\", metric.type, metric_name)\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.mixins.OpenMetricsScraperMixin.process_metric","title":"<code>process_metric(metric, scraper_config, metric_transformers=None)</code>","text":"<p>Handle a Prometheus metric according to the following flow: - search <code>scraper_config['metrics_mapper']</code> for a prometheus.metric to datadog.metric mapping - call check method with the same name as the metric - log info if none of the above worked</p> <p><code>metric_transformers</code> is a dict of <code>&lt;metric name&gt;:&lt;function to run when the metric name is encountered&gt;</code></p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/mixins.py</code> <pre><code>def process_metric(self, metric, scraper_config, metric_transformers=None):\n\"\"\"\n    Handle a Prometheus metric according to the following flow:\n    - search `scraper_config['metrics_mapper']` for a prometheus.metric to datadog.metric mapping\n    - call check method with the same name as the metric\n    - log info if none of the above worked\n    `metric_transformers` is a dict of `&lt;metric name&gt;:&lt;function to run when the metric name is encountered&gt;`\n    \"\"\"\n# If targeted metric, store labels\nself._store_labels(metric, scraper_config)\nif scraper_config['ignore_metrics']:\nif metric.name in scraper_config['_ignored_metrics']:\nself._send_telemetry_counter(\nself.TELEMETRY_COUNTER_METRICS_IGNORE_COUNT, len(metric.samples), scraper_config\n)\nreturn  # Ignore the metric\nif scraper_config['_ignored_re'] and scraper_config['_ignored_re'].search(metric.name):\n# Metric must be ignored\nscraper_config['_ignored_metrics'].add(metric.name)\nself._send_telemetry_counter(\nself.TELEMETRY_COUNTER_METRICS_IGNORE_COUNT, len(metric.samples), scraper_config\n)\nreturn  # Ignore the metric\nself._send_telemetry_counter(self.TELEMETRY_COUNTER_METRICS_PROCESS_COUNT, len(metric.samples), scraper_config)\nif self._filter_metric(metric, scraper_config):\nreturn  # Ignore the metric\n# Filter metric to see if we can enrich with joined labels\nself._join_labels(metric, scraper_config)\nif scraper_config['_dry_run']:\nreturn\ntry:\nself.submit_openmetric(scraper_config['metrics_mapper'][metric.name], metric, scraper_config)\nexcept KeyError:\nif metric_transformers is not None and metric.name in metric_transformers:\ntry:\n# Get the transformer function for this specific metric\ntransformer = metric_transformers[metric.name]\ntransformer(metric, scraper_config)\nexcept Exception as err:\nself.log.warning('Error handling metric: %s - error: %s', metric.name, err)\nreturn\n# check for wildcards in transformers\nfor transformer_name, transformer in iteritems(metric_transformers):\nif transformer_name.endswith('*') and metric.name.startswith(transformer_name[:-1]):\ntransformer(metric, scraper_config, transformer_name)\n# try matching wildcards\nif scraper_config['_wildcards_re'] and scraper_config['_wildcards_re'].search(metric.name):\nself.submit_openmetric(metric.name, metric, scraper_config)\nreturn\nself.log.debug(\n'Skipping metric `%s` as it is not defined in the metrics mapper, '\n'has no transformer function, nor does it match any wildcards.',\nmetric.name,\n)\n</code></pre>"},{"location":"legacy/prometheus/#datadog_checks.base.checks.openmetrics.mixins.OpenMetricsScraperMixin.create_scraper_configuration","title":"<code>create_scraper_configuration(instance=None)</code>","text":"<p>Creates a scraper configuration.</p> <p>If instance does not specify a value for a configuration option, the value will default to the <code>init_config</code>. Otherwise, the <code>default_instance</code> value will be used.</p> <p>A default mixin configuration will be returned if there is no instance.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/mixins.py</code> <pre><code>def create_scraper_configuration(self, instance=None):\n\"\"\"\n    Creates a scraper configuration.\n    If instance does not specify a value for a configuration option, the value will default to the `init_config`.\n    Otherwise, the `default_instance` value will be used.\n    A default mixin configuration will be returned if there is no instance.\n    \"\"\"\nif 'openmetrics_endpoint' in instance:\nraise CheckException('The setting `openmetrics_endpoint` is only available for Agent version 7 or later')\n# We can choose to create a default mixin configuration for an empty instance\nif instance is None:\ninstance = {}\n# Supports new configuration options\nconfig = copy.deepcopy(instance)\n# Set the endpoint\nendpoint = instance.get('prometheus_url')\nif instance and endpoint is None:\nraise CheckException(\"You have to define a prometheus_url for each prometheus instance\")\n# Set the bearer token authorization to customer value, then get the bearer token\nself.update_prometheus_url(instance, config, endpoint)\n# `NAMESPACE` is the prefix metrics will have. Need to be hardcoded in the\n# child check class.\nnamespace = instance.get('namespace')\n# Check if we have a namespace\nif instance and namespace is None:\nif self.default_namespace is None:\nraise CheckException(\"You have to define a namespace for each prometheus check\")\nnamespace = self.default_namespace\nconfig['namespace'] = namespace\n# Retrieve potential default instance settings for the namespace\ndefault_instance = self.default_instances.get(namespace, {})\ndef _get_setting(name, default):\nreturn instance.get(name, default_instance.get(name, default))\n# `metrics_mapper` is a dictionary where the keys are the metrics to capture\n# and the values are the corresponding metrics names to have in datadog.\n# Note: it is empty in the parent class but will need to be\n# overloaded/hardcoded in the final check not to be counted as custom metric.\n# Metrics are preprocessed if no mapping\nmetrics_mapper = {}\n# We merge list and dictionaries from optional defaults &amp; instance settings\nmetrics = default_instance.get('metrics', []) + instance.get('metrics', [])\nfor metric in metrics:\nif isinstance(metric, string_types):\nmetrics_mapper[metric] = metric\nelse:\nmetrics_mapper.update(metric)\nconfig['metrics_mapper'] = metrics_mapper\n# `_wildcards_re` is a Pattern object used to match metric wildcards\nconfig['_wildcards_re'] = None\nwildcards = set()\nfor metric in config['metrics_mapper']:\nif \"*\" in metric:\nwildcards.add(translate(metric))\nif wildcards:\nconfig['_wildcards_re'] = compile('|'.join(wildcards))\n# `prometheus_metrics_prefix` allows to specify a prefix that all\n# prometheus metrics should have. This can be used when the prometheus\n# endpoint we are scrapping allows to add a custom prefix to it's\n# metrics.\nconfig['prometheus_metrics_prefix'] = instance.get(\n'prometheus_metrics_prefix', default_instance.get('prometheus_metrics_prefix', '')\n)\n# `label_joins` holds the configuration for extracting 1:1 labels from\n# a target metric to all metric matching the label, example:\n# self.label_joins = {\n#     'kube_pod_info': {\n#         'labels_to_match': ['pod'],\n#         'labels_to_get': ['node', 'host_ip']\n#     }\n# }\nconfig['label_joins'] = default_instance.get('label_joins', {})\nconfig['label_joins'].update(instance.get('label_joins', {}))\n# `_label_mapping` holds the additionals label info to add for a specific\n# label value, example:\n# self._label_mapping = {\n#     'pod': {\n#         'dd-agent-9s1l1': {\n#             \"node\": \"yolo\",\n#             \"host_ip\": \"yey\"\n#         }\n#     }\n# }\nconfig['_label_mapping'] = {}\n# `_active_label_mapping` holds a dictionary of label values found during the run\n# to cleanup the label_mapping of unused values, example:\n# self._active_label_mapping = {\n#     'pod': {\n#         'dd-agent-9s1l1': True\n#     }\n# }\nconfig['_active_label_mapping'] = {}\n# `_watched_labels` holds the sets of labels to watch for enrichment\nconfig['_watched_labels'] = {}\nconfig['_dry_run'] = True\n# Some metrics are ignored because they are duplicates or introduce a\n# very high cardinality. Metrics included in this list will be silently\n# skipped without a 'Unable to handle metric' debug line in the logs\nconfig['ignore_metrics'] = instance.get('ignore_metrics', default_instance.get('ignore_metrics', []))\nconfig['_ignored_metrics'] = set()\n# `_ignored_re` is a Pattern object used to match ignored metric patterns\nconfig['_ignored_re'] = None\nignored_patterns = set()\n# Separate ignored metric names and ignored patterns in different sets for faster lookup later\nfor metric in config['ignore_metrics']:\nif '*' in metric:\nignored_patterns.add(translate(metric))\nelse:\nconfig['_ignored_metrics'].add(metric)\nif ignored_patterns:\nconfig['_ignored_re'] = compile('|'.join(ignored_patterns))\n# Ignore metrics based on label keys or specific label values\nconfig['ignore_metrics_by_labels'] = instance.get(\n'ignore_metrics_by_labels', default_instance.get('ignore_metrics_by_labels', {})\n)\n# If you want to send the buckets as tagged values when dealing with histograms,\n# set send_histograms_buckets to True, set to False otherwise.\nconfig['send_histograms_buckets'] = is_affirmative(\ninstance.get('send_histograms_buckets', default_instance.get('send_histograms_buckets', True))\n)\n# If you want the bucket to be non cumulative and to come with upper/lower bound tags\n# set non_cumulative_buckets to True, enabled when distribution metrics are enabled.\nconfig['non_cumulative_buckets'] = is_affirmative(\ninstance.get('non_cumulative_buckets', default_instance.get('non_cumulative_buckets', False))\n)\n# Send histograms as datadog distribution metrics\nconfig['send_distribution_buckets'] = is_affirmative(\ninstance.get('send_distribution_buckets', default_instance.get('send_distribution_buckets', False))\n)\n# Non cumulative buckets are mandatory for distribution metrics\nif config['send_distribution_buckets'] is True:\nconfig['non_cumulative_buckets'] = True\n# If you want to send `counter` metrics as monotonic counts, set this value to True.\n# Set to False if you want to instead send those metrics as `gauge`.\nconfig['send_monotonic_counter'] = is_affirmative(\ninstance.get('send_monotonic_counter', default_instance.get('send_monotonic_counter', True))\n)\n# If you want `counter` metrics to be submitted as both gauges and monotonic counts. Set this value to True.\nconfig['send_monotonic_with_gauge'] = is_affirmative(\ninstance.get('send_monotonic_with_gauge', default_instance.get('send_monotonic_with_gauge', False))\n)\nconfig['send_distribution_counts_as_monotonic'] = is_affirmative(\ninstance.get(\n'send_distribution_counts_as_monotonic',\ndefault_instance.get('send_distribution_counts_as_monotonic', False),\n)\n)\nconfig['send_distribution_sums_as_monotonic'] = is_affirmative(\ninstance.get(\n'send_distribution_sums_as_monotonic',\ndefault_instance.get('send_distribution_sums_as_monotonic', False),\n)\n)\n# If the `labels_mapper` dictionary is provided, the metrics labels names\n# in the `labels_mapper` will use the corresponding value as tag name\n# when sending the gauges.\nconfig['labels_mapper'] = default_instance.get('labels_mapper', {})\nconfig['labels_mapper'].update(instance.get('labels_mapper', {}))\n# Rename bucket \"le\" label to \"upper_bound\"\nconfig['labels_mapper']['le'] = 'upper_bound'\n# `exclude_labels` is an array of label names to exclude. Those labels\n# will just not be added as tags when submitting the metric.\nconfig['exclude_labels'] = default_instance.get('exclude_labels', []) + instance.get('exclude_labels', [])\n# `include_labels` is an array of label names to include. If these labels are not in\n# the `exclude_labels` list, then they are added as tags when submitting the metric.\nconfig['include_labels'] = default_instance.get('include_labels', []) + instance.get('include_labels', [])\n# `type_overrides` is a dictionary where the keys are prometheus metric names\n# and the values are a metric type (name as string) to use instead of the one\n# listed in the payload. It can be used to force a type on untyped metrics.\n# Note: it is empty in the parent class but will need to be\n# overloaded/hardcoded in the final check not to be counted as custom metric.\nconfig['type_overrides'] = default_instance.get('type_overrides', {})\nconfig['type_overrides'].update(instance.get('type_overrides', {}))\n# `_type_override_patterns` is a dictionary where we store Pattern objects\n# that match metric names as keys, and their corresponding metric type overrides as values.\nconfig['_type_override_patterns'] = {}\nwith_wildcards = set()\nfor metric, type in iteritems(config['type_overrides']):\nif '*' in metric:\nconfig['_type_override_patterns'][compile(translate(metric))] = type\nwith_wildcards.add(metric)\n# cleanup metric names with wildcards from the 'type_overrides' dict\nfor metric in with_wildcards:\ndel config['type_overrides'][metric]\n# Some metrics are retrieved from different hosts and often\n# a label can hold this information, this transfers it to the hostname\nconfig['label_to_hostname'] = instance.get('label_to_hostname', default_instance.get('label_to_hostname', None))\n# In combination to label_as_hostname, allows to add a common suffix to the hostnames\n# submitted. This can be used for instance to discriminate hosts between clusters.\nconfig['label_to_hostname_suffix'] = instance.get(\n'label_to_hostname_suffix', default_instance.get('label_to_hostname_suffix', None)\n)\n# Add a 'health' service check for the prometheus endpoint\nconfig['health_service_check'] = is_affirmative(\ninstance.get('health_service_check', default_instance.get('health_service_check', True))\n)\n# Can either be only the path to the certificate and thus you should specify the private key\n# or it can be the path to a file containing both the certificate &amp; the private key\nconfig['ssl_cert'] = instance.get('ssl_cert', default_instance.get('ssl_cert', None))\n# Needed if the certificate does not include the private key\n#\n# /!\\ The private key to your local certificate must be unencrypted.\n# Currently, Requests does not support using encrypted keys.\nconfig['ssl_private_key'] = instance.get('ssl_private_key', default_instance.get('ssl_private_key', None))\n# The path to the trusted CA used for generating custom certificates\nconfig['ssl_ca_cert'] = instance.get('ssl_ca_cert', default_instance.get('ssl_ca_cert', None))\n# Whether or not to validate SSL certificates\nconfig['ssl_verify'] = is_affirmative(instance.get('ssl_verify', default_instance.get('ssl_verify', True)))\n# Extra http headers to be sent when polling endpoint\nconfig['extra_headers'] = default_instance.get('extra_headers', {})\nconfig['extra_headers'].update(instance.get('extra_headers', {}))\n# Timeout used during the network request\nconfig['prometheus_timeout'] = instance.get(\n'prometheus_timeout', default_instance.get('prometheus_timeout', 10)\n)\n# Authentication used when polling endpoint\nconfig['username'] = instance.get('username', default_instance.get('username', None))\nconfig['password'] = instance.get('password', default_instance.get('password', None))\n# Custom tags that will be sent with each metric\nconfig['custom_tags'] = instance.get('tags', [])\n# Some tags can be ignored to reduce the cardinality.\n# This can be useful for cost optimization in containerized environments\n# when the openmetrics check is configured to collect custom metrics.\n# Even when the Agent's Tagger is configured to add low-cardinality tags only,\n# some tags can still generate unwanted metric contexts (e.g pod annotations as tags).\nignore_tags = instance.get('ignore_tags', default_instance.get('ignore_tags', []))\nif ignore_tags:\nignored_tags_re = compile('|'.join(set(ignore_tags)))\nconfig['custom_tags'] = [tag for tag in config['custom_tags'] if not ignored_tags_re.search(tag)]\n# Additional tags to be sent with each metric\nconfig['_metric_tags'] = []\n# List of strings to filter the input text payload on. If any line contains\n# one of these strings, it will be filtered out before being parsed.\n# INTERNAL FEATURE, might be removed in future versions\nconfig['_text_filter_blacklist'] = []\n# Refresh the bearer token every 60 seconds by default.\n# Ref https://github.com/DataDog/datadog-agent/pull/11686\nconfig['bearer_token_refresh_interval'] = instance.get(\n'bearer_token_refresh_interval', default_instance.get('bearer_token_refresh_interval', 60)\n)\nconfig['telemetry'] = is_affirmative(instance.get('telemetry', default_instance.get('telemetry', False)))\n# The metric name services use to indicate build information\nconfig['metadata_metric_name'] = instance.get(\n'metadata_metric_name', default_instance.get('metadata_metric_name')\n)\n# Map of metadata key names to label names\nconfig['metadata_label_map'] = instance.get(\n'metadata_label_map', default_instance.get('metadata_label_map', {})\n)\nconfig['_default_metric_transformers'] = {}\nif config['metadata_metric_name'] and config['metadata_label_map']:\nconfig['_default_metric_transformers'][config['metadata_metric_name']] = self.transform_metadata\n# Whether or not to enable flushing of the first value of monotonic counts\nconfig['_flush_first_value'] = False\n# Whether to use process_start_time_seconds to decide if counter-like values should  be flushed\n# on first scrape.\nconfig['use_process_start_time'] = is_affirmative(_get_setting('use_process_start_time', False))\nreturn config\n</code></pre>"},{"location":"legacy/prometheus/#options","title":"Options","text":"<p>Some options can be set globally in <code>init_config</code> (with <code>instances</code> taking precedence). For complete documentation of every option, see the associated configuration templates for the instances and init_config sections.</p>"},{"location":"legacy/prometheus/#config-changes-between-versions","title":"Config changes between versions","text":"<p>There are config option changes between OpenMetrics V1 and V2, so check if any updated OpenMetrics instances use deprecated options and update accordingly.</p> OpenMetrics V1 OpenMetrics V2 <code>ignore_metrics</code> <code>exclude_metrics</code> <code>prometheus_metrics_prefix</code> <code>raw_metric_prefix</code> <code>health_service_check</code> <code>enable_health_service_check</code> <code>labels_mapper</code> <code>rename_labels</code> <code>label_joins</code> <code>share_labels</code>* <code>send_histograms_buckets</code> <code>collect_histogram_buckets</code> <code>send_distribution_buckets</code> <code>histogram_buckets_as_distributions</code> <p>Note: The <code>type_overrides</code> option is incorporated in the <code>metrics</code> option. This <code>metrics</code> option defines the list of which metrics to collect from the <code>openmetrics_endpoint</code>, and it can be used to remap the names and types of exposed metrics as well as use regular expression to match exposed metrics.</p> <p><code>share_labels</code> are used to join labels with a 1:1 mapping and can take other parameters for sharing. More information can be found in the conf.yaml.exmaple.</p> <p>All HTTP options are also supported.</p> Source code in <code>datadog_checks_base/datadog_checks/base/checks/openmetrics/base_check.py</code> <pre><code>class StandardFields(object):\npass\n</code></pre>"},{"location":"legacy/prometheus/#prometheus-to-datadog-metric-types","title":"Prometheus to Datadog metric types","text":"<p>The Openmetrics Base Check supports various configurations for submitting Prometheus metrics to Datadog. We currently support Prometheus <code>gauge</code>, <code>counter</code>, <code>histogram</code>, and <code>summary</code> metric types.</p>"},{"location":"legacy/prometheus/#gauge","title":"Gauge","text":"<p>A gauge metric represents a single numerical value that can arbitrarily go up or down.</p> <p>Prometheus gauge metrics are submitted as Datadog gauge metrics.</p>"},{"location":"legacy/prometheus/#counter","title":"Counter","text":"<p>A Prometheus counter is a cumulative metric that represents a single monotonically increasing counter whose value can only increase or be reset to zero on restart.</p> Config Option Value Datadog Metric Submitted <code>send_monotonic_counter</code> <code>true</code> (default) <code>monotonic_count</code> <code>false</code> <code>gauge</code>"},{"location":"legacy/prometheus/#histogram","title":"Histogram","text":"<p>A Prometheus histogram samples observations and counts them in configurable buckets along with a sum of all observed values.</p> <p>Histogram metrics ending in:</p> <ul> <li><code>_sum</code> represent the total sum of all observed values. Generally sums  are like counters but it's also possible for a negative observation which would not behave like a typical always increasing counter.</li> <li><code>_count</code> represent the total number of events that have been observed.</li> <li><code>_bucket</code> represent the cumulative counters for the observation buckets. Note that buckets are only submitted if <code>send_histograms_buckets</code> is enabled.</li> </ul> Subtype Config Option Value Datadog Metric Submitted <code>send_distribution_buckets</code> <code>true</code> The entire histogram can be submitted as a single distribution metric. If the option is enabled, none of the subtype metrics will be submitted. <code>_sum</code> <code>send_distribution_sums_as_monotonic</code> <code>false</code> (default) <code>gauge</code> <code>true</code> <code>monotonic_count</code> <code>_count</code> <code>send_distribution_counts_as_monotonic</code> <code>false</code> (default) <code>gauge</code> <code>true</code> <code>monotonic_count</code> <code>_bucket</code> <code>non_cumulative_buckets</code> <code>false</code> (default) <code>gauge</code> <code>true</code> <code>monotonic_count</code> under <code>.count</code> metric name if <code>send_distribution_counts_as_monotonic</code> is enabled. Otherwise, <code>gauge</code>."},{"location":"legacy/prometheus/#summary","title":"Summary","text":"<p>Prometheus summary metrics are similar to histograms but allow configurable quantiles.</p> <p>Summary metrics ending in:</p> <ul> <li><code>_sum</code> represent the total sum of all observed values. Generally sums  are like counters but it's also possible for a negative observation which would not behave like a typical always increasing counter.</li> <li><code>_count</code> represent the total number of events that have been observed.</li> <li>metrics with labels like <code>{quantile=\"&lt;\u03c6&gt;\"}</code> represent the streaming quantiles of observed events.</li> </ul> Subtype Config Option Value Datadog Metric Submitted <code>_sum</code> <code>send_distribution_sums_as_monotonic</code> <code>false</code> (default) <code>gauge</code> <code>true</code> <code>monotonic_count</code> <code>_count</code> <code>send_distribution_counts_as_monotonic</code> <code>false</code> (default) <code>gauge</code> <code>true</code> <code>monotonic_count</code> <code>_quantile</code> <code>gauge</code>"},{"location":"meta/cd/","title":"Continuous delivery","text":""},{"location":"meta/config-models/","title":"Config models","text":"<p>All integrations use pydantic models as the primary way to validate and interface with configuration.</p> <p>As config spec data types are based on OpenAPI 3, we automatically generate the necessary code.</p> <p>The models reside in a package named <code>config_models</code> located at the root of a check's namespaced package. For example, a new integration named <code>foo</code>:</p> <pre><code>foo\n\u2502   ...\n\u251c\u2500\u2500 datadog_checks\n\u2502   \u2514\u2500\u2500 foo\n\u2502       \u2514\u2500\u2500 config_models\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u251c\u2500\u2500 defaults.py\n\u2502           \u251c\u2500\u2500 instance.py\n\u2502           \u251c\u2500\u2500 shared.py\n\u2502           \u2514\u2500\u2500 validators.py\n\u2502       \u2514\u2500\u2500 __init__.py\n\u2502       ...\n...\n</code></pre> <p>There are 2 possible models:</p> <ul> <li><code>InstanceConfig</code> (ID: <code>instance</code>) that corresponds to a check's entry in the <code>instances</code> section</li> <li><code>SharedConfig</code> (ID: <code>shared</code>) that corresponds to the <code>init_config</code> section that is shared by all instances</li> </ul> <p>All models are defined in <code>&lt;ID&gt;.py</code> and are available for import directly under <code>config_models</code>.</p>"},{"location":"meta/config-models/#default-values","title":"Default values","text":"<p>The default values for optional settings are populated in <code>defaults.py</code> and are derived from the value property of config spec options. The precedence is the <code>default</code> key followed by the <code>example</code> key (if it appears to represent a real value rather than an illustrative example and the <code>type</code> is a primitive). In all other cases, the default is <code>None</code>, which means there is no default getter function.</p>"},{"location":"meta/config-models/#validation","title":"Validation","text":"<p>The validation of fields for every model occurs in three high-level stages, as described in this section.</p>"},{"location":"meta/config-models/#initial","title":"Initial","text":"<pre><code>def initialize_&lt;ID&gt;(values: dict[str, Any], **kwargs) -&gt; dict[str, Any]:\n    ...\n</code></pre> <p>If such a validator exists in <code>validators.py</code>, then it is called once with the raw config that was supplied by the user. The returned mapping is used as the input config for the subsequent stages.</p>"},{"location":"meta/config-models/#field","title":"Field","text":"<p>The value of each field goes through the following steps.</p>"},{"location":"meta/config-models/#default-value-population","title":"Default value population","text":"<p>If a field was not supplied by the user nor during the initialization stage, then its default value is taken from <code>defaults.py</code>. This stage is skipped for required fields.</p>"},{"location":"meta/config-models/#custom-field-validators","title":"Custom field validators","text":"<p>The contents of <code>validators.py</code> are entirely custom and contain functions to perform extra validation if necessary.</p> <pre><code>def &lt;ID&gt;_&lt;OPTION_NAME&gt;(value: Any, *, field: pydantic.fields.FieldInfo, **kwargs) -&gt; Any:\n    ...\n</code></pre> <p>Such validators are called for the appropriate field of the proper model. The returned value is used as the new value of the option for the subsequent stages.</p> <p>Note</p> <p>This only occurs if the option was supplied by the user.</p>"},{"location":"meta/config-models/#pre-defined-field-validators","title":"Pre-defined field validators","text":"<p>A <code>validators</code> key under the value property of config spec options is considered. Every entry refers to a relative import path to a field validator under <code>datadog_checks.base.utils.models.validation</code> and is executed in the defined order.</p> <p>Note</p> <p>This only occurs if the option was supplied by the user.</p>"},{"location":"meta/config-models/#conversion-to-immutable-types","title":"Conversion to immutable types","text":"<p>Every <code>list</code> is converted to <code>tuple</code> and every <code>dict</code> is converted to types.MappingProxyType.</p> <p>Note</p> <p>A field or nested field would only be a <code>dict</code> when it is defined as a mapping with arbitrary keys. Otherwise, it would be a model with its own properties as usual.</p>"},{"location":"meta/config-models/#final","title":"Final","text":"<pre><code>def check_&lt;ID&gt;(model: pydantic.BaseModel) -&gt; pydantic.BaseModel:\n    ...\n</code></pre> <p>If such a validator exists in <code>validators.py</code>, then it is called with the final constructed model. At this point, it cannot be mutated, so you can only raise errors.</p>"},{"location":"meta/config-models/#loading","title":"Loading","text":"<p>A check initialization occurs before a check's first run that loads the config models. Validation errors will thus prevent check execution.</p>"},{"location":"meta/config-models/#interface","title":"Interface","text":"<p>The config models package contains a class <code>ConfigMixin</code> from which checks inherit:</p> <pre><code>from datadog_checks.base import AgentCheck\n\nfrom .config_models import ConfigMixin\n\n\nclass Check(AgentCheck, ConfigMixin):\n    ...\n</code></pre> <p>It exposes the instantiated <code>InstanceConfig</code> model as <code>self.config</code> and <code>SharedConfig</code> model as <code>self.shared_config</code>.</p>"},{"location":"meta/config-models/#immutability","title":"Immutability","text":"<p>In addition to each field being converted to an immutable type, all generated models are configured as immutable.</p>"},{"location":"meta/config-models/#deprecation","title":"Deprecation","text":"<p>Every option marked as deprecated in the config spec will log a warning with information about when it will be removed and what to do.</p>"},{"location":"meta/config-models/#enforcement","title":"Enforcement","text":"<p>A validation command <code>validate models</code> runs in our CI. To locally generate the proper files, run <code>ddev validate models [INTEGRATION] --sync</code>.</p>"},{"location":"meta/config-specs/","title":"Configuration specification","text":"<p>Every integration has a specification detailing all the options that influence behavior. These YAML files are located at <code>&lt;INTEGRATION&gt;/assets/configuration/spec.yaml</code>.</p>"},{"location":"meta/config-specs/#producer","title":"Producer","text":"<p>The producer's job is to read a specification and:</p> <ol> <li>Validate for correctness</li> <li>Populate all unset default fields</li> <li>Resolve any defined templates</li> <li>Output the complete specification as JSON for arbitrary consumers</li> </ol>"},{"location":"meta/config-specs/#consumers","title":"Consumers","text":"<p>Consumers may utilize specs in a number of scenarios, such as:</p> <ul> <li>rendering example configuration shipped to end users</li> <li>documenting all options in-app &amp; on the docs site</li> <li>form for creating configuration in multiple formats on Integration tiles</li> <li>automatic configuration loading for Checks</li> <li>Agent based and/or in-app validator for user-supplied configuration</li> </ul>"},{"location":"meta/config-specs/#schema","title":"Schema","text":"<p>The root of every spec is a map with 3 keys:</p> <ul> <li><code>name</code> - The display name of what the spec refers to e.g. <code>Postgres</code>, <code>Datadog Agent</code>, etc.</li> <li><code>version</code> - The released version of what the spec refers to</li> <li><code>files</code> - A list of all files that influence behavior</li> </ul>"},{"location":"meta/config-specs/#files","title":"Files","text":"<p>Every file has 3 possible attributes:</p> <ul> <li><code>name</code> - This is the name of the file the Agent will look for (REQUIRED)</li> <li><code>example_name</code> - This is the name of the example file the Agent will ship. If none is provided, the   default will be <code>conf.yaml.example</code>. The exceptions are as follows:</li> <li>Auto-discovery files, which are named <code>auto_conf.yaml</code></li> <li>Python-based core check default files, which are named <code>conf.yaml.default</code></li> <li><code>options</code> - A list of options (REQUIRED)</li> </ul>"},{"location":"meta/config-specs/#options","title":"Options","text":"<p>Every option has 10 possible attributes:</p> <ul> <li><code>name</code> - This is the name of the option (REQUIRED)</li> <li><code>description</code> - Information about the option. This can be a multi-line string, but each line must contain fewer than 120 characters (REQUIRED).</li> <li><code>required</code> - Whether or not the option is required for basic functionality. It defaults to <code>false</code>.</li> <li><code>hidden</code> - Whether or not the option should not be publicly exposed. It defaults to <code>false</code>.</li> <li><code>display_priority</code> - An integer representing the relative visual rank the option should take on   compared to other options when publicly exposed. It defaults to <code>0</code>, meaning that every option will   be displayed in the order defined in the spec.</li> <li> <p><code>deprecation</code> - If the option is deprecated, a mapping of relevant information. For example:</p> <pre><code>deprecation:\nAgent version: 8.0.0\nMigration: |\ndo this\nand that\n</code></pre> </li> <li> <p><code>multiple</code> - Whether or not options may be selected multiple times like <code>instances</code> or just once   like <code>init_config</code></p> </li> <li><code>multiple_instances_defined</code> - Whether or not we separate the definition into multiple instances or just one</li> <li><code>metadata_tags</code> - A list of tags (like <code>docs:foo</code>) that can be used for unexpected use cases</li> <li><code>options</code> - Nested options, indicating that this is a section like <code>instances</code> or <code>logs</code></li> <li><code>value</code> - The expected type data</li> </ul> <p>There are 2 types of options: those with and without a <code>value</code>. Those with a <code>value</code> attribute are the actual user-controlled settings that influence behavior like <code>username</code>. Those without are expected to be sections and therefore must have an <code>options</code> attribute. An option cannot have both attributes.</p> <p>Options with a <code>value</code> (non-section) also support:</p> <ul> <li><code>secret</code> - Whether or not consumers should treat the option as sensitive information like <code>password</code>.   It defaults to <code>false</code>.</li> </ul> Info <p>The option vs section logic was chosen instead of going fully typed to avoid deeply nested <code>value</code>s.</p>"},{"location":"meta/config-specs/#values","title":"Values","text":"<p>The type system is based on a loose subset of OpenAPI 3 data types.</p> <p>The differences are:</p> <ul> <li>Only the <code>minimum</code> and <code>maximum</code> numeric modifiers are supported</li> <li>Only the <code>pattern</code> string modifier is supported</li> <li>The <code>properties</code> object modifier is not a map, but rather a list of maps with a required <code>name</code>   attribute. This is so consumers will load objects consistently regardless of language guarantees   regarding map key order.</li> </ul> <p>Values also support 1 field of our own:</p> <ul> <li><code>example</code> - An example value, only required if the type is <code>boolean</code>. The default is <code>&lt;OPTION_NAME&gt;</code>.</li> </ul>"},{"location":"meta/config-specs/#templates","title":"Templates","text":"<p>Every option may reference pre-defined templates using a key called <code>template</code>. The template format looks like <code>path/to/template_file</code> where <code>path/to</code> must point an existing directory relative to a template directory and <code>template_file</code> must have the file extension <code>.yaml</code> or <code>.yml</code>.</p> <p>You can use custom templates that will take precedence over the pre-defined templates by using the <code>template_paths</code> parameter of the ConfigSpec class.</p>"},{"location":"meta/config-specs/#override","title":"Override","text":"<p>For occasions when deeply nested default template values need to be overridden, there is the ability to redefine attributes via a . (dot) accessor.</p> <pre><code>options:\n- template: instances/http\noverrides:\ntimeout.value.example: 42\n</code></pre>"},{"location":"meta/config-specs/#example-file-consumer","title":"Example file consumer","text":"<p>The example consumer uses each spec to render the example configuration files that are shipped with every Agent and individual Integration release.</p> <p>It respects a few extra option-level attributes:</p> <ul> <li><code>example</code> - A complete example of an option in lieu of a strictly typed <code>value</code> attribute</li> <li><code>enabled</code> - Whether or not to un-comment the option, overriding the behavior of <code>required</code></li> <li><code>display_priority</code> - This is an integer affecting the order in which options are displayed, with higher values indicating higher priority.   The default is <code>0</code>.</li> </ul> <p>It also respects a few extra fields under the <code>value</code> attribute of each option:</p> <ul> <li><code>display_default</code> - This is the default value that will be shown in the header of each option, useful if it differs from the <code>example</code>.   You may set it to <code>null</code> explicitly to disable showing this part of the header.</li> <li><code>compact_example</code> - Whether or not to display complex types like arrays in their most compact representation. It defaults to <code>false</code>.</li> </ul>"},{"location":"meta/config-specs/#usage","title":"Usage","text":"<p>Use the <code>--sync</code> flag of the config validation command to render the example configuration files.</p>"},{"location":"meta/config-specs/#data-model-consumer","title":"Data model consumer","text":"<p>The model consumer uses each spec to render the pydantic models that checks use to validate and interface with configuration. The models are shipped with every Agent and individual Integration release.</p> <p>It respects an extra field under the <code>value</code> attribute of each option:</p> <ul> <li><code>default</code> - This is the default value that options will be set to, taking precedence over the <code>example</code>.</li> <li><code>validators</code> - This refers to an array of pre-defined field validators to use. Every entry will refer to a relative import path to a   field validator under <code>datadog_checks.base.utils.models.validation</code> and will be executed in the defined order.</li> </ul>"},{"location":"meta/config-specs/#usage_1","title":"Usage","text":"<p>Use the <code>--sync</code> flag of the model validation command to render the data model files.</p>"},{"location":"meta/config-specs/#api","title":"API","text":""},{"location":"meta/config-specs/#datadog_checks.dev.tooling.configuration.ConfigSpec","title":"<code>datadog_checks.dev.tooling.configuration.ConfigSpec</code>","text":"Source code in <code>datadog_checks_dev/datadog_checks/dev/tooling/configuration/core.py</code> <pre><code>class ConfigSpec(object):\ndef __init__(self, contents: str, template_paths: List[str] = None, source: str = None, version: str = None):\n\"\"\"\n        Parameters:\n            contents:\n                the raw text contents of a spec\n            template_paths:\n                a sequence of directories that will take precedence when looking for templates\n            source:\n                a textual representation of what the spec refers to, usually an integration name\n            version:\n                the version of the spec to default to if the spec does not define one\n        \"\"\"\nself.contents = contents\nself.source = source\nself.version = version\nself.templates = ConfigTemplates(template_paths)\nself.data: Union[dict, None] = None\nself.errors = []\ndef load(self) -&gt; None:\n\"\"\"\n        This function de-serializes the specification and:\n        1. fills in default values\n        2. populates any selected templates\n        3. accumulates all error/warning messages\n        If the `errors` attribute is empty after this is called, the `data` attribute\n        will be the fully resolved spec object.\n        \"\"\"\nif self.data is not None and not self.errors:\nreturn\ntry:\nself.data = yaml.safe_load(self.contents)\nexcept Exception as e:\nself.errors.append(f'{self.source}: Unable to parse the configuration specification: {e}')\nreturn\nspec_validator(self.data, self)\n</code></pre>"},{"location":"meta/config-specs/#datadog_checks.dev.tooling.configuration.ConfigSpec.__init__","title":"<code>__init__(contents, template_paths=None, source=None, version=None)</code>","text":"<pre><code>contents:\n    the raw text contents of a spec\ntemplate_paths:\n    a sequence of directories that will take precedence when looking for templates\nsource:\n    a textual representation of what the spec refers to, usually an integration name\nversion:\n    the version of the spec to default to if the spec does not define one\n</code></pre> Source code in <code>datadog_checks_dev/datadog_checks/dev/tooling/configuration/core.py</code> <pre><code>def __init__(self, contents: str, template_paths: List[str] = None, source: str = None, version: str = None):\n\"\"\"\n    Parameters:\n        contents:\n            the raw text contents of a spec\n        template_paths:\n            a sequence of directories that will take precedence when looking for templates\n        source:\n            a textual representation of what the spec refers to, usually an integration name\n        version:\n            the version of the spec to default to if the spec does not define one\n    \"\"\"\nself.contents = contents\nself.source = source\nself.version = version\nself.templates = ConfigTemplates(template_paths)\nself.data: Union[dict, None] = None\nself.errors = []\n</code></pre>"},{"location":"meta/config-specs/#datadog_checks.dev.tooling.configuration.ConfigSpec.load","title":"<code>load()</code>","text":"<p>This function de-serializes the specification and: 1. fills in default values 2. populates any selected templates 3. accumulates all error/warning messages If the <code>errors</code> attribute is empty after this is called, the <code>data</code> attribute will be the fully resolved spec object.</p> Source code in <code>datadog_checks_dev/datadog_checks/dev/tooling/configuration/core.py</code> <pre><code>def load(self) -&gt; None:\n\"\"\"\n    This function de-serializes the specification and:\n    1. fills in default values\n    2. populates any selected templates\n    3. accumulates all error/warning messages\n    If the `errors` attribute is empty after this is called, the `data` attribute\n    will be the fully resolved spec object.\n    \"\"\"\nif self.data is not None and not self.errors:\nreturn\ntry:\nself.data = yaml.safe_load(self.contents)\nexcept Exception as e:\nself.errors.append(f'{self.source}: Unable to parse the configuration specification: {e}')\nreturn\nspec_validator(self.data, self)\n</code></pre>"},{"location":"meta/docs/","title":"Documentation","text":""},{"location":"meta/docs/#generation","title":"Generation","text":"<p>Our docs are configured to be rendered by the static site generator MkDocs with the beautiful Material for MkDocs theme.</p>"},{"location":"meta/docs/#plugins","title":"Plugins","text":"<p>We use a select few MkDocs plugins to achieve the following:</p> <ul> <li>minify HTML ()</li> <li>display the date of the last Git modification of every page ()</li> <li>automatically generate docs based on code and docstrings ()</li> <li>export the site as a PDF ()</li> </ul>"},{"location":"meta/docs/#extensions","title":"Extensions","text":"<p>We also depend on a few Python-Markdown extensions to achieve the following:</p> <ul> <li>support for emojis, collapsible elements, code highlighting, and other advanced features courtesy of the PyMdown extension suite ()</li> <li>ability to inline SVG icons from Material, FontAwesome, and Octicons ()</li> <li>allow arbitrary scripts to modify MkDocs input files ()</li> <li>automatically generate reference docs for Click-based command line interfaces ()</li> </ul>"},{"location":"meta/docs/#references","title":"References","text":"<p>All references are automatically available to all pages.</p>"},{"location":"meta/docs/#abbreviations","title":"Abbreviations","text":"<p>These allow for the expansion of text on hover, useful for acronyms and definitions.</p> <p>For example, if you add the following to the list of abbreviations:</p> <pre><code>*[CERN]: European Organization for Nuclear Research\n</code></pre> <p>then anywhere you type CERN the organization's full name will appear on hover.</p>"},{"location":"meta/docs/#external-links","title":"External links","text":"<p>All links to external resources should be added to the list of external links rather than defined on a per-page basis, for many reasons:</p> <ol> <li>it keeps the Markdown content compact and thus easy to read and modify</li> <li>the ability to re-use a link, even if you forsee no immediate use elsewhere</li> <li>easy automation of stale link detection</li> <li>when links to external resources change, the last date of Git modification displayed on pages will not</li> </ol>"},{"location":"meta/docs/#scripts","title":"Scripts","text":"<p>We use some scripts to dynamically modify pages before being processed by other extensions and MkDocs itself, to achieve the following:</p> <ul> <li>add references to the bottom of every page</li> <li>render the status of various aspects of integrations</li> <li>enumerate all the dependencies that are shipped with the Datadog Agent</li> </ul>"},{"location":"meta/docs/#build","title":"Build","text":"<p>We configure a tox environment called <code>docs</code> that provides all the dependencies necessary to build the documentation.</p> <p>To build and view the documentation in your browser, run the serve command (the first invocation may take a few extra moments):</p> <pre><code>ddev docs serve\n</code></pre> <p>By default, live reloading is enabled so any modification will be reflected in near-real time.</p> <p>Note: In order to export the site as a PDF, you can use the <code>--pdf</code> flag, but you will need some external dependencies.</p>"},{"location":"meta/docs/#deploy","title":"Deploy","text":"<p>Our CI deploys the documentation to GitHub Pages if any changes occur on commits to the <code>master</code> branch.</p> <p>Danger</p> <p>Never make documentation non-deterministic as it will trigger deploys for every single commit.</p> <p>For example, say you want to display the valid values of a CLI option and the enumeration is represented as a <code>set</code>. Formatting the sequence directly will produce inconsistent results because sets do not guarantee order like dictionaries do, so you must sort it first.</p>"},{"location":"meta/status/","title":"Status","text":""},{"location":"meta/status/#dashboards","title":"Dashboards","text":"<p> <p>72.59%</p> </p> Completed 143/197 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airbyte</li> <li> airflow</li> <li> amazon_eks_blueprints</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> avi_vantage</li> <li> azure_active_directory</li> <li> azure_iot_edge</li> <li> boundary</li> <li> btrfs</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> consul_connect</li> <li> container</li> <li> containerd</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> cri</li> <li> crio</li> <li> databricks</li> <li> datadog_cluster_agent</li> <li> datadog_operator</li> <li> dcgm</li> <li> directory</li> <li> disk</li> <li> dotnetclr</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_anywhere</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> external_dns</li> <li> flink</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> go_expvar</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> helm</li> <li> hive</li> <li> hivemq</li> <li> http_check</li> <li> hudi</li> <li> hyperv</li> <li> iam_access_analyzer</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_i</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> jmeter</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_apiserver_metrics</li> <li> kube_controller_manager</li> <li> kube_dns</li> <li> kube_metrics_server</li> <li> kube_proxy</li> <li> kube_scheduler</li> <li> kubelet</li> <li> kubernetes</li> <li> kubernetes_state</li> <li> kubernetes_state_core</li> <li> kyototycoon</li> <li> langchain</li> <li> lighttpd</li> <li> linkerd</li> <li> linux_proc_extras</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> network</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> nvidia_jetson</li> <li> oke</li> <li> oom_kill</li> <li> openai</li> <li> openldap</li> <li> openshift</li> <li> openstack</li> <li> openstack_controller</li> <li> oracle</li> <li> otel</li> <li> pan_firewall</li> <li> pgbouncer</li> <li> php_fpm</li> <li> podman</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> process</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> riakcs</li> <li> sap_hana</li> <li> scylla</li> <li> sidekiq</li> <li> silk</li> <li> singlestore</li> <li> snowflake</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> statsd</li> <li> strimzi</li> <li> system_core</li> <li> systemd</li> <li> tcp_check</li> <li> temporal</li> <li> teradata</li> <li> tls</li> <li> tokumx</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> wincrashdetect</li> <li> windows_performance_counters</li> <li> winkmem</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#logs-support","title":"Logs support","text":"<p> <p>88.24%</p> </p> Completed 120/136 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> azure_iot_edge</li> <li> boundary</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloud_foundry_api</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> crio</li> <li> datadog_cluster_agent</li> <li> dcgm</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> flink</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> hudi</li> <li> hyperv</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openstack</li> <li> openstack_controller</li> <li> pan_firewall</li> <li> pgbouncer</li> <li> php_fpm</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> scylla</li> <li> sidekiq</li> <li> silk</li> <li> singlestore</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> teamcity</li> <li> temporal</li> <li> tenable</li> <li> teradata</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> win32_event_log</li> <li> windows_performance_counters</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#recommended-monitors","title":"Recommended monitors","text":"<p> <p>27.33%</p> </p> Completed 47/172 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> avi_vantage</li> <li> azure_iot_edge</li> <li> boundary</li> <li> btrfs</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloud_foundry_api</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> crio</li> <li> datadog_checks_dependency_provider</li> <li> datadog_cluster_agent</li> <li> dcgm</li> <li> directory</li> <li> dns_check</li> <li> dotnetclr</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> external_dns</li> <li> flink</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> go_expvar</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> http_check</li> <li> hudi</li> <li> hyperv</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_i</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_apiserver_metrics</li> <li> kube_controller_manager</li> <li> kube_dns</li> <li> kube_metrics_server</li> <li> kube_proxy</li> <li> kube_scheduler</li> <li> kubelet</li> <li> kubernetes_state</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> linux_proc_extras</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openmetrics</li> <li> openstack</li> <li> openstack_controller</li> <li> oracle</li> <li> pan_firewall</li> <li> pdh_check</li> <li> pgbouncer</li> <li> php_fpm</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> process</li> <li> prometheus</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> riakcs</li> <li> sap_hana</li> <li> scylla</li> <li> sidekiq</li> <li> silk</li> <li> singlestore</li> <li> snmp</li> <li> snowflake</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> ssh_check</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> system_core</li> <li> system_swap</li> <li> tcp_check</li> <li> teamcity</li> <li> temporal</li> <li> tenable</li> <li> teradata</li> <li> tls</li> <li> tokumx</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> win32_event_log</li> <li> windows_performance_counters</li> <li> windows_service</li> <li> wmi_check</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#config-specs","title":"Config specs","text":"<p> <p>99.42%</p> </p> Completed 171/172 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> avi_vantage</li> <li> azure_iot_edge</li> <li> boundary</li> <li> btrfs</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloud_foundry_api</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> crio</li> <li> datadog_checks_dependency_provider</li> <li> datadog_cluster_agent</li> <li> dcgm</li> <li> directory</li> <li> dns_check</li> <li> dotnetclr</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> external_dns</li> <li> flink</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> go_expvar</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> http_check</li> <li> hudi</li> <li> hyperv</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_i</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_apiserver_metrics</li> <li> kube_controller_manager</li> <li> kube_dns</li> <li> kube_metrics_server</li> <li> kube_proxy</li> <li> kube_scheduler</li> <li> kubelet</li> <li> kubernetes_state</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> linux_proc_extras</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openmetrics</li> <li> openstack</li> <li> openstack_controller</li> <li> oracle</li> <li> pan_firewall</li> <li> pdh_check</li> <li> pgbouncer</li> <li> php_fpm</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> process</li> <li> prometheus</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> riakcs</li> <li> sap_hana</li> <li> scylla</li> <li> sidekiq</li> <li> silk</li> <li> singlestore</li> <li> snmp</li> <li> snowflake</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> ssh_check</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> system_core</li> <li> system_swap</li> <li> tcp_check</li> <li> teamcity</li> <li> temporal</li> <li> tenable</li> <li> teradata</li> <li> tls</li> <li> tokumx</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> win32_event_log</li> <li> windows_performance_counters</li> <li> windows_service</li> <li> wmi_check</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#e2e-tests","title":"E2E tests","text":"<p> <p>91.18%</p> </p> Completed 155/170 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> avi_vantage</li> <li> azure_iot_edge</li> <li> boundary</li> <li> btrfs</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloud_foundry_api</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> crio</li> <li> datadog_checks_dependency_provider</li> <li> datadog_cluster_agent</li> <li> dcgm</li> <li> directory</li> <li> dns_check</li> <li> dotnetclr</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> external_dns</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> go_expvar</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> http_check</li> <li> hudi</li> <li> hyperv</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_i</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_apiserver_metrics</li> <li> kube_controller_manager</li> <li> kube_dns</li> <li> kube_metrics_server</li> <li> kube_proxy</li> <li> kube_scheduler</li> <li> kubelet</li> <li> kubernetes_state</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> linux_proc_extras</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openmetrics</li> <li> openstack</li> <li> openstack_controller</li> <li> oracle</li> <li> pan_firewall</li> <li> pdh_check</li> <li> pgbouncer</li> <li> php_fpm</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> process</li> <li> prometheus</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> riakcs</li> <li> sap_hana</li> <li> scylla</li> <li> silk</li> <li> singlestore</li> <li> snmp</li> <li> snowflake</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> ssh_check</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> system_core</li> <li> system_swap</li> <li> tcp_check</li> <li> teamcity</li> <li> temporal</li> <li> tenable</li> <li> teradata</li> <li> tls</li> <li> tokumx</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> win32_event_log</li> <li> windows_performance_counters</li> <li> windows_service</li> <li> wmi_check</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#new-version-support","title":"New version support","text":"<p> <p>0.00%</p> </p> Completed 0/172 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> avi_vantage</li> <li> azure_iot_edge</li> <li> boundary</li> <li> btrfs</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloud_foundry_api</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> crio</li> <li> datadog_checks_base</li> <li> datadog_checks_dev</li> <li> datadog_checks_downloader</li> <li> datadog_cluster_agent</li> <li> dcgm</li> <li> ddev</li> <li> directory</li> <li> disk</li> <li> dns_check</li> <li> dotnetclr</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> external_dns</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> go_expvar</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> http_check</li> <li> hudi</li> <li> hyperv</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_i</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_apiserver_metrics</li> <li> kube_controller_manager</li> <li> kube_dns</li> <li> kube_metrics_server</li> <li> kube_proxy</li> <li> kube_scheduler</li> <li> kubelet</li> <li> kubernetes_state</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> linux_proc_extras</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> network</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openmetrics</li> <li> openstack</li> <li> openstack_controller</li> <li> oracle</li> <li> pdh_check</li> <li> pgbouncer</li> <li> php_fpm</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> process</li> <li> prometheus</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> riakcs</li> <li> sap_hana</li> <li> scylla</li> <li> silk</li> <li> singlestore</li> <li> snmp</li> <li> snowflake</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> ssh_check</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> system_core</li> <li> system_swap</li> <li> tcp_check</li> <li> teamcity</li> <li> temporal</li> <li> teradata</li> <li> tls</li> <li> tokumx</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> win32_event_log</li> <li> windows_performance_counters</li> <li> windows_service</li> <li> wmi_check</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#config-validation","title":"Config validation","text":"<p> <p>93.64%</p> </p> Completed 162/173 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> avi_vantage</li> <li> azure_iot_edge</li> <li> boundary</li> <li> btrfs</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloud_foundry_api</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> crio</li> <li> datadog_cluster_agent</li> <li> dcgm</li> <li> directory</li> <li> disk</li> <li> dns_check</li> <li> dotnetclr</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> external_dns</li> <li> flink</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> go_expvar</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> http_check</li> <li> hudi</li> <li> hyperv</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_i</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_apiserver_metrics</li> <li> kube_controller_manager</li> <li> kube_dns</li> <li> kube_metrics_server</li> <li> kube_proxy</li> <li> kube_scheduler</li> <li> kubelet</li> <li> kubernetes_state</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> linux_proc_extras</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> network</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openmetrics</li> <li> openstack</li> <li> openstack_controller</li> <li> oracle</li> <li> pan_firewall</li> <li> pdh_check</li> <li> pgbouncer</li> <li> php_fpm</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> process</li> <li> prometheus</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> riakcs</li> <li> sap_hana</li> <li> scylla</li> <li> sidekiq</li> <li> silk</li> <li> singlestore</li> <li> snmp</li> <li> snowflake</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> ssh_check</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> system_core</li> <li> system_swap</li> <li> tcp_check</li> <li> teamcity</li> <li> temporal</li> <li> tenable</li> <li> teradata</li> <li> tls</li> <li> tokumx</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> win32_event_log</li> <li> windows_performance_counters</li> <li> windows_service</li> <li> wmi_check</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#metadata-submission","title":"Metadata submission","text":"<p> <p>24.71%</p> </p> Completed 42/170 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> avi_vantage</li> <li> azure_iot_edge</li> <li> boundary</li> <li> btrfs</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloud_foundry_api</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> crio</li> <li> datadog_checks_dependency_provider</li> <li> datadog_cluster_agent</li> <li> dcgm</li> <li> directory</li> <li> dns_check</li> <li> dotnetclr</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> external_dns</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> go_expvar</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> http_check</li> <li> hudi</li> <li> hyperv</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_i</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_apiserver_metrics</li> <li> kube_controller_manager</li> <li> kube_dns</li> <li> kube_metrics_server</li> <li> kube_proxy</li> <li> kube_scheduler</li> <li> kubelet</li> <li> kubernetes_state</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> linux_proc_extras</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openmetrics</li> <li> openstack</li> <li> openstack_controller</li> <li> oracle</li> <li> pan_firewall</li> <li> pdh_check</li> <li> pgbouncer</li> <li> php_fpm</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> process</li> <li> prometheus</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> riakcs</li> <li> sap_hana</li> <li> scylla</li> <li> silk</li> <li> singlestore</li> <li> snmp</li> <li> snowflake</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> ssh_check</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> system_core</li> <li> system_swap</li> <li> tcp_check</li> <li> teamcity</li> <li> temporal</li> <li> tenable</li> <li> teradata</li> <li> tls</li> <li> tokumx</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> win32_event_log</li> <li> windows_performance_counters</li> <li> windows_service</li> <li> wmi_check</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#process-signatures","title":"Process signatures","text":"<p> <p>39.08%</p> </p> Completed 68/174 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> avi_vantage</li> <li> azure_iot_edge</li> <li> boundary</li> <li> btrfs</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloud_foundry_api</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> crio</li> <li> datadog_checks_dependency_provider</li> <li> datadog_cluster_agent</li> <li> dcgm</li> <li> ddev</li> <li> directory</li> <li> disk</li> <li> dns_check</li> <li> dotnetclr</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> external_dns</li> <li> flink</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> go_expvar</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> http_check</li> <li> hudi</li> <li> hyperv</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_i</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_apiserver_metrics</li> <li> kube_controller_manager</li> <li> kube_dns</li> <li> kube_metrics_server</li> <li> kube_proxy</li> <li> kube_scheduler</li> <li> kubelet</li> <li> kubernetes_state</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> linux_proc_extras</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> network</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openmetrics</li> <li> openstack</li> <li> openstack_controller</li> <li> oracle</li> <li> pan_firewall</li> <li> pdh_check</li> <li> pgbouncer</li> <li> php_fpm</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> process</li> <li> prometheus</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> riakcs</li> <li> sap_hana</li> <li> scylla</li> <li> sidekiq</li> <li> silk</li> <li> singlestore</li> <li> snmp</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> ssh_check</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> system_core</li> <li> system_swap</li> <li> tcp_check</li> <li> teamcity</li> <li> temporal</li> <li> tenable</li> <li> teradata</li> <li> tls</li> <li> tokumx</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> win32_event_log</li> <li> windows_performance_counters</li> <li> windows_service</li> <li> wmi_check</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#agent-8-check-signatures","title":"Agent 8 check signatures","text":"<p> <p>70.29%</p> </p> Completed 123/175 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> amazon_msk</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> avi_vantage</li> <li> azure_iot_edge</li> <li> boundary</li> <li> btrfs</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cert_manager</li> <li> cilium</li> <li> cisco_aci</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cloud_foundry_api</li> <li> cloudera</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> crio</li> <li> datadog_checks_dependency_provider</li> <li> datadog_cluster_agent</li> <li> dcgm</li> <li> ddev</li> <li> directory</li> <li> disk</li> <li> dns_check</li> <li> dotnetclr</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> external_dns</li> <li> flink</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> go_expvar</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> http_check</li> <li> hudi</li> <li> hyperv</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_i</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_apiserver_metrics</li> <li> kube_controller_manager</li> <li> kube_dns</li> <li> kube_metrics_server</li> <li> kube_proxy</li> <li> kube_scheduler</li> <li> kubelet</li> <li> kubernetes_state</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> linux_proc_extras</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> network</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openmetrics</li> <li> openstack</li> <li> openstack_controller</li> <li> oracle</li> <li> pan_firewall</li> <li> pdh_check</li> <li> pgbouncer</li> <li> php_fpm</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> process</li> <li> prometheus</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> ray</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> riakcs</li> <li> sap_hana</li> <li> scylla</li> <li> sidekiq</li> <li> silk</li> <li> singlestore</li> <li> snmp</li> <li> snowflake</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> ssh_check</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> system_core</li> <li> system_swap</li> <li> tcp_check</li> <li> teamcity</li> <li> temporal</li> <li> tenable</li> <li> teradata</li> <li> tls</li> <li> tokumx</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> vsphere</li> <li> weaviate</li> <li> weblogic</li> <li> win32_event_log</li> <li> windows_performance_counters</li> <li> windows_service</li> <li> wmi_check</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/status/#default-saved-views-for-integrations-with-logs","title":"Default saved views (for integrations with logs)","text":"<p> <p>41.32%</p> </p> Completed 50/121 <ul> <li> active_directory</li> <li> activemq</li> <li> activemq_xml</li> <li> aerospike</li> <li> airflow</li> <li> ambari</li> <li> apache</li> <li> arangodb</li> <li> argocd</li> <li> aspdotnet</li> <li> azure_iot_edge</li> <li> boundary</li> <li> cacti</li> <li> calico</li> <li> cassandra</li> <li> cassandra_nodetool</li> <li> ceph</li> <li> cilium</li> <li> citrix_hypervisor</li> <li> clickhouse</li> <li> cockroachdb</li> <li> confluent_platform</li> <li> consul</li> <li> coredns</li> <li> couch</li> <li> couchbase</li> <li> druid</li> <li> ecs_fargate</li> <li> eks_fargate</li> <li> elastic</li> <li> envoy</li> <li> etcd</li> <li> exchange_server</li> <li> flink</li> <li> fluentd</li> <li> foundationdb</li> <li> gearmand</li> <li> gitlab</li> <li> gitlab_runner</li> <li> glusterfs</li> <li> gunicorn</li> <li> haproxy</li> <li> harbor</li> <li> hazelcast</li> <li> hdfs_datanode</li> <li> hdfs_namenode</li> <li> hive</li> <li> hivemq</li> <li> hudi</li> <li> ibm_ace</li> <li> ibm_db2</li> <li> ibm_mq</li> <li> ibm_was</li> <li> ignite</li> <li> iis</li> <li> impala</li> <li> istio</li> <li> jboss_wildfly</li> <li> journald</li> <li> kafka</li> <li> kafka_consumer</li> <li> kong</li> <li> kube_scheduler</li> <li> kyototycoon</li> <li> lighttpd</li> <li> linkerd</li> <li> mapr</li> <li> mapreduce</li> <li> marathon</li> <li> marklogic</li> <li> mcache</li> <li> mesos_master</li> <li> mesos_slave</li> <li> mongo</li> <li> mysql</li> <li> nagios</li> <li> nfsstat</li> <li> nginx</li> <li> nginx_ingress_controller</li> <li> openldap</li> <li> openstack</li> <li> openstack_controller</li> <li> pan_firewall</li> <li> pgbouncer</li> <li> postfix</li> <li> postgres</li> <li> powerdns_recursor</li> <li> presto</li> <li> proxysql</li> <li> pulsar</li> <li> rabbitmq</li> <li> redisdb</li> <li> rethinkdb</li> <li> riak</li> <li> scylla</li> <li> sidekiq</li> <li> singlestore</li> <li> solr</li> <li> sonarqube</li> <li> spark</li> <li> sqlserver</li> <li> squid</li> <li> statsd</li> <li> strimzi</li> <li> supervisord</li> <li> teamcity</li> <li> temporal</li> <li> tenable</li> <li> tomcat</li> <li> torchserve</li> <li> traffic_server</li> <li> twemproxy</li> <li> twistlock</li> <li> varnish</li> <li> vault</li> <li> vertica</li> <li> voltdb</li> <li> weblogic</li> <li> win32_event_log</li> <li> yarn</li> <li> zk</li> </ul>"},{"location":"meta/ci/labels/","title":"Labels","text":"<p>We use official labeler action to automatically add labels to pull requests.</p> <p>The labeler is configured to add the following:</p> Label Condition integration/&lt;NAME&gt; any directory at the root that actually contains an integration documentation any Markdown, config specs, <code>manifest.json</code>, or anything in <code>/docs/</code> dev/testing GitHub Actions or Codecov config dev/tooling GitLab (see CD) or GitHub Actions config, or ddev dependencies any change in shipped dependencies release any base package, dev package, or integration release changelog/no-changelog any release, or if all files don't modify code that is shipped"},{"location":"meta/ci/testing/","title":"Testing","text":""},{"location":"meta/ci/testing/#workflows","title":"Workflows","text":"<ul> <li>Master - Runs tests on Python 3 for every target on merges to the <code>master</code> branch</li> <li>PR - Runs tests on Python 2 &amp; 3 for any modified target in a pull request as long as the base or developer packages were not modified</li> <li>PR All - Runs tests on Python 2 &amp; 3 for every target in a pull request if the base or developer packages were modified</li> <li>Nightly minimum base package test - Runs tests for every target once nightly using the minimum declared required version of the base package</li> <li>Nightly Python 2 tests - Runs tests on Python 2 for every target once nightly</li> <li>Test Agent release - Runs tests for every target when manually scheduled using specific versions of the Agent for E2E tests</li> </ul>"},{"location":"meta/ci/testing/#reusable-workflows","title":"Reusable workflows","text":"<p>These can be used by other repositories.</p>"},{"location":"meta/ci/testing/#pr-test","title":"PR test","text":"<p>This workflow is meant to be used on pull requests.</p> <p>First it computes the job matrix based on what was changed. Since this is time sensitive, rather than fetching the entire history we use GitHub's API to find out the precise depth to fetch in order to reach the merge base. Then it runs the test workflow for every job in the matrix.</p> <p>Note</p> <p>Changes that match any of the following patterns inside a directory will trigger the testing of that target:</p> <ul> <li><code>assets/configuration/**/*</code></li> <li><code>tests/**/*</code></li> <li><code>*.py</code></li> <li><code>hatch.toml</code></li> <li><code>metadata.csv</code></li> <li><code>pyproject.toml</code></li> </ul> <p>Warning</p> <p>A matrix is limited to 256 jobs. Rather than allowing a workflow error, the matrix generator will enforce the cap and emit a warning.</p>"},{"location":"meta/ci/testing/#test-target","title":"Test target","text":"<p>This workflow runs a single job that is the foundation of how all tests are executed. Depending on the input parameters, the order of operations is as follows:</p> <ul> <li>Checkout code (on pull requests this is a merge commit)</li> <li>Set up Python 2.7</li> <li>Set up the Python version the Agent currently ships</li> <li>Restore dependencies from the cache</li> <li>Install &amp; configure ddev</li> <li>Run any setup scripts the target requires</li> <li>Start an HTTP server to capture traces</li> <li>Run unit &amp; integration tests</li> <li>Run E2E tests</li> <li>Run benchmarks</li> <li>Upload captured traces</li> <li>Upload collected test results</li> <li>Submit coverage statistics to Codecov</li> </ul>"},{"location":"meta/ci/testing/#target-setup","title":"Target setup","text":"<p>Some targets require additional set up such as the installation of system dependencies. Therefore, all such logic is put into scripts that live under <code>/.ddev/ci/scripts</code>.</p> <p>As targets may need different set up on different platforms, all scripts live under a directory named after the platform ID. All scripts in the directory are executed in lexicographical order. Files in the scripts directory whose names begin with an underscore are not executed.</p> <p>The step that executes these scripts is the only step that has access to secrets.</p>"},{"location":"meta/ci/testing/#secrets","title":"Secrets","text":"<p>Since environment variables defined in a workflow do not propagate to reusable workflows, secrets must be passed as a JSON string representing a map.</p> <p>Both the PR test and Test target reusable workflows for testing accept a <code>setup-env-vars</code> input parameter that defines the environment variables for the setup step. For example:</p> <pre><code>jobs:\ntest:\nuses: DataDog/integrations-core/.github/workflows/pr-test.yml@master\nwith:\nrepo: \"&lt;NAME&gt;\"\nsetup-env-vars: &gt;-\n${{ format(\n'{{\n\"PYTHONUNBUFFERED\": \"1\",\n\"SECRET_FOO\": \"{0}\",\n\"SECRET_BAR\": \"{1}\"\n}}',\nsecrets.SECRET_FOO,\nsecrets.SECRET_BAR\n)}}\n</code></pre> <p>Note</p> <p>Secrets for integrations-core itself are defined as the default value in the base workflow.</p>"},{"location":"meta/ci/testing/#environment-variable-persistence","title":"Environment variable persistence","text":"<p>If environment variables need to be available for testing, you can add a script that writes to the file defined by the <code>GITHUB_ENV</code> environment variable:</p> <pre><code>#!/bin/bash\nset -euo pipefail\n\nset +x\necho \"LICENSE_KEY=$LICENSE_KEY\" &gt;&gt; \"$GITHUB_ENV\"\nset -x\n</code></pre>"},{"location":"meta/ci/testing/#target-configuration","title":"Target configuration","text":"<p>Configuration for targets lives under the <code>overrides.ci</code> key inside a <code>/.ddev/config.toml</code> file.</p> <p>Note</p> <p>Targets are referenced by the name of their directory.</p>"},{"location":"meta/ci/testing/#platforms","title":"Platforms","text":"Name ID Default runner Linux <code>linux</code> Ubuntu 22.04 Windows <code>windows</code> Windows Server 2022 macOS <code>macos</code> macOS 12 <p>If an integration's <code>manifest.json</code> indicates that the only supported platform is Windows then that will be used to run tests, otherwise they will run on Linux.</p> <p>To override the platform(s) used, one can set the <code>overrides.ci.&lt;TARGET&gt;.platforms</code> array. For example:</p> <pre><code>[overrides.ci.sqlserver]\nplatforms = [\"windows\", \"linux\"]\n</code></pre>"},{"location":"meta/ci/testing/#runners","title":"Runners","text":"<p>To override the runners for each platform, one can set the <code>overrides.ci.&lt;TARGET&gt;.runners</code> mapping of platform IDs to runner labels. For example:</p> <pre><code>[overrides.ci.sqlserver]\nrunners = { windows = [\"windows-2019\"] }\n</code></pre>"},{"location":"meta/ci/testing/#exclusion","title":"Exclusion","text":"<p>To disable testing, one can enable the <code>overrides.ci.&lt;TARGET&gt;.exclude</code> option. For example:</p> <pre><code>[overrides.ci.hyperv]\nexclude = true\n</code></pre>"},{"location":"meta/ci/testing/#target-enumeration","title":"Target enumeration","text":"<p>The list of all jobs is generated as the <code>/.github/workflows/test-all.yml</code> file.</p> <p>This reusable workflow is called by workflows that need to test everything.</p>"},{"location":"meta/ci/testing/#tracing","title":"Tracing","text":"<p>During testing we use ddtrace to submit APM data to the Datadog Agent. To avoid every job pulling the Agent, these HTTP trace requests are captured and saved to a newline-delimited JSON file.</p> <p>A workflow then runs after all jobs are finished and replays the requests to the Agent. At the end the artifact is deleted to avoid needless storage persistence and also so if individual jobs are rerun that only the new traces will be submitted.</p> <p>We maintain a public dashboard for monitoring our CI.</p>"},{"location":"meta/ci/testing/#test-results","title":"Test results","text":"<p>After all test jobs in a workflow complete we publish the results.</p> <p>On pull requests we create a single comment that remains updated:</p> <p></p> <p>On merges to the <code>master</code> branch we generate a badge with stats about all tests:</p> <p></p>"},{"location":"meta/ci/testing/#caching","title":"Caching","text":"<p>A workflow runs on merges to the <code>master</code> branch that, if the files defining the dependencies have not changed, saves the dependencies shared by all targets for the current Python version for each platform.</p> <p>During testing the cache is restored, with a fallback to an older compatible version of the cache.</p>"},{"location":"meta/ci/testing/#python-version","title":"Python version","text":"<p>Tests by default use the Python version the Agent currently ships. This value must be changed in the following locations:</p> <ul> <li><code>PYTHON_VERSION</code> environment variable in /.github/workflows/cache-shared-deps.yml</li> <li><code>PYTHON_VERSION</code> environment variable in /.github/workflows/run-validations.yml</li> <li><code>PYTHON_VERSION</code> environment variable fallback in /.github/workflows/test-target.yml</li> </ul>"},{"location":"meta/ci/testing/#caveats","title":"Caveats","text":""},{"location":"meta/ci/testing/#windows-performance","title":"Windows performance","text":"<p>The first command invocation is extraordinarily slow (see actions/runner-images#6561). Bash appears to be the least affected so we set that as the default shell for all workflows that run commands.</p> <p>Note</p> <p>The official checkout action is affected by a similar issue (see actions/checkout#1246) that has been narrowed down to disk I/O.</p>"},{"location":"meta/ci/validation/","title":"Validation","text":"<p>Various validations are ran to check for correctness. There is a reusable workflow that repositories may call with input parameters defining which validations to use, with each input parameter corresponding to a subcommand under the <code>ddev validate</code> command group.</p>"},{"location":"meta/ci/validation/#agent-requirements","title":"Agent requirements","text":"<pre><code>ddev validate agent-reqs\n</code></pre> <p>This validates that each integration version is in sync with the <code>requirements-agent-release.txt</code> file. It is uncommon for this to fail because the release process is automated.</p>"},{"location":"meta/ci/validation/#ci-configuration","title":"CI configuration","text":"<pre><code>ddev validate ci\n</code></pre> <p>This validates that all CI entries for integrations are valid. This includes checking if the integration has the correct Codecov config, and has a valid CI entry if it is testable.</p> <p>Tip</p> <p>Run <code>ddev validate ci --sync</code> to resolve most errors.</p>"},{"location":"meta/ci/validation/#codeowners","title":"Codeowners","text":"<pre><code>ddev validate codeowners\n</code></pre> <p>This validates that every integration has a codeowner entry. If this validation fails, add an entry in the codewners file corresponding to any newly added integration.</p> <p>Note</p> <p>This validation is only enabled for integrations-extras.</p>"},{"location":"meta/ci/validation/#default-configuration-files","title":"Default configuration files","text":"<pre><code>ddev validate config\n</code></pre> <p>This verifies that the config specs for all integrations are valid by enforcing our configuration spec schema. The most common failure is some version of <code>File &lt;INTEGRATION_SPEC&gt; needs to be synced.</code> To resolve this issue, you can run <code>ddev validate config --sync</code></p> <p>If you see failures regarding formatting or missing parameters, see our config spec documentation for more details on how to construct configuration specs.</p>"},{"location":"meta/ci/validation/#dashboard-definition-files","title":"Dashboard definition files","text":"<pre><code>ddev validate dashboards\n</code></pre> <p>This validates that dashboards are formatted correctly. This means that they need to be proper JSON and generated from Datadog's <code>/dashboard</code> API.</p> <p>Tip</p> <p>If you see a failure regarding use of the screen endpoint, consider using our dashboard utility command to generate your dashboard payload.</p>"},{"location":"meta/ci/validation/#dependencies","title":"Dependencies","text":"<pre><code>ddev validate dep\n</code></pre> <p>This command:</p> <ul> <li>Verifies the uniqueness of dependency versions across all checks.</li> <li>Verifies all the dependencies are pinned.</li> <li>Verifies the embedded Python environment defined in the base check and requirements listed in every integration are compatible.</li> </ul> <p>This validation only applies if your work introduces new external dependencies.</p>"},{"location":"meta/ci/validation/#manifest-files","title":"Manifest files","text":"<pre><code>ddev validate manifest\n</code></pre> <p>This validates that the manifest files contain required fields, are formatted correctly, and don't contain common errors. See the Datadog docs for more detailed constraints.</p>"},{"location":"meta/ci/validation/#metadata","title":"Metadata","text":"<pre><code>ddev validate metadata\n</code></pre> <p>This checks that every <code>metadata.csv</code> file is formatted correctly. See the Datadog docs for more detailed constraints.</p>"},{"location":"meta/ci/validation/#readme-files","title":"README files","text":"<pre><code>ddev validate readmes\n</code></pre> <p>This ensures that every integration's README.md file is formatted correctly. The main purpose of this validation is to ensure that any image linked in the readme exists and that all images are located in an integration's <code>/image</code> directory.</p>"},{"location":"meta/ci/validation/#saved-views-data","title":"Saved views data","text":"<pre><code>ddev validate saved-views\n</code></pre> <p>This validates that saved views for an integration are formatted correctly and contain required fields, such as \"type\".</p> <p>Tip</p> <p>View example saved views for inspiration and guidance.</p>"},{"location":"meta/ci/validation/#service-check-data","title":"Service check data","text":"<pre><code>ddev validate service-checks\n</code></pre> <p>This checks that every service check file is formatted correctly. See the Datadog docs for more specific constraints.</p>"},{"location":"meta/ci/validation/#imports","title":"Imports","text":"<pre><code>ddev validate imports\n</code></pre> <p>This verifies that all integrations import the base package in the correct way, such as:</p> <pre><code>from datadog_checks.base.foo import bar\n</code></pre> <p>Tip</p> <p>See the New Integration Instructions for more examples of how to use the base package.</p>"},{"location":"process/integration-release/","title":"Integration release","text":"<p>Each Agent integration has its own release cycle. Many integrations are actively developed and released often while some are rarely touched (usually indicating feature-completeness).</p>"},{"location":"process/integration-release/#versioning","title":"Versioning","text":"<p>All releases adhere to Semantic Versioning.</p> <p>Tags in the form <code>&lt;INTEGRATION_NAME&gt;-&lt;VERSION&gt;</code> are added to the Git repository. Therefore, it's possible to checkout and build the code for a certain version of a specific check.</p>"},{"location":"process/integration-release/#setup","title":"Setup","text":"<p>Configure your GitHub auth.</p>"},{"location":"process/integration-release/#identify-changes","title":"Identify changes","text":"<p>Note</p> <p>If you already know which integration you'd like to release, skip this section.</p> <p>To see all checks that need to be released, run <code>ddev release show ready</code>.</p> <ol> <li> <p>Checkout and pull the most recent version of the <code>master</code> branch.</p> <pre><code>git checkout master\ngit pull\n</code></pre> <p>Important</p> <p>Not using the latest version of <code>master</code> may cause errors in the build pipeline.</p> </li> <li> <p>Review which PRs were merged in between the latest release and the <code>master</code> branch.</p> <pre><code>ddev release show changes &lt;INTEGRATION&gt;\n</code></pre> <p>You should ensure that PR titles and changelog labels are correct.</p> </li> </ol>"},{"location":"process/integration-release/#creating-the-release","title":"Creating the release","text":"<ol> <li> <p>Create a release branch from master (suggested naming format is <code>&lt;USERNAME&gt;/release-&lt;INTEGRATION_NAME&gt;</code>).    This has the purpose of opening a PR so others can review the changelog.</p> <p>Important</p> <p>It is critical the branch name is not in the form <code>&lt;USERNAME&gt;/&lt;INTEGRATION_NAME&gt;-&lt;NEW_VERSION&gt;</code> because one of our Gitlab jobs is triggered whenever a Git reference matches that pattern, see #3843 &amp; #3980.</p> </li> <li> <p>Make the release (Third party integrations).</p> <ul> <li> <p>Update the version on <code>datadog_checks/&lt;INTEGRATION&gt;/__about__.py</code>.</p> </li> <li> <p>Update the CHANGELOG.md file This file can be automatically updated by <code>ddev</code> using the following command:</p> </li> </ul> <pre><code>ddev release changelog &lt;INTEGRATION_NAME&gt; &lt;VERSION&gt;\n</code></pre> <p>This command will list all merged PRs since the last release and creates a changelog entry based on the pull request labels, this means that the version bump needs to be on a separate PR from the one that included the changes. For changelog types, we adhere to those defined by Keep a Changelog.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |</p> </li> <li> <p>Push your branch to GitHub and create a pull request.</p> <ol> <li>Update the title of the PR to something like <code>[Release] Bumped &lt;INTEGRATION&gt; version to &lt;VERSION&gt;</code>.</li> <li>Ask for a review in Slack.</li> </ol> </li> <li> <p>Merge the pull request after approval or wait for it to be merged.</p> </li> </ol>"},{"location":"process/integration-release/#metadata","title":"Metadata","text":"<p>You need to run certain backend jobs if any changes modified integration metadata or assets such as dashboards. If you are a contributor a datadog employee will handle this.</p>"},{"location":"process/integration-release/#new-integrations-third-party-integrations","title":"New integrations (third party integrations)","text":"<p>For first time releases of third party integrations, simply merge the integration to master and a release will be  triggered with the specified version number in the about file.</p>"},{"location":"tutorials/memory-profiling/","title":"Memory profiling","text":""},{"location":"tutorials/jmx/integration/","title":"JMX integration","text":"<p>Tutorial for starting a JMX integration</p>"},{"location":"tutorials/jmx/integration/#step-1-create-a-jmx-integration-scaffolding","title":"Step 1: Create a JMX integration scaffolding","text":"<pre><code>ddev create --type jmx MyJMXIntegration\n</code></pre> <p>JMX integration contains specific init configs and instance configs:</p> <pre><code>init_config:\nis_jmx: true                   # tells the Agent that the integration is a JMX type of integration\ncollect_default_metrics: true  # if true, metrics declared in `metrics.yaml` are collected\n\ninstances:\n- host: &lt;HOST&gt;                   # JMX hostname\nport: &lt;PORT&gt;                   # JMX port\n...\n</code></pre> <p>Other init and instance configs can be found on JMX integration page</p>"},{"location":"tutorials/jmx/integration/#step-2-define-metrics-you-want-to-collect","title":"Step 2: Define metrics you want to collect","text":"<p>Select what metrics you want to collect from JMX. Available metrics can be usually found on official documentation of the service you want to monitor.</p> <p>You can also use tools like VisualVM, JConsole or jmxterm to explore the available JMX beans and their descriptions.</p>"},{"location":"tutorials/jmx/integration/#step-3-define-metrics-filters","title":"Step 3: Define metrics filters","text":"<p>Edit the <code>metrics.yaml</code> to define the filters for collecting metrics.</p> <p>The metrics filters format details can be found on JMX integration doc</p> <p>JMXFetch test cases also help understanding how metrics filters work and provide many examples.  </p> <p>Example of <code>metrics.yaml</code></p> <pre><code>jmx_metrics:\n- include:\ndomain: org.apache.activemq\ndestinationType: Queue\nattribute:\nAverageEnqueueTime:\nalias: activemq.queue.avg_enqueue_time\nmetric_type: gauge\nConsumerCount:\nalias: activemq.queue.consumer_count\nmetric_type: gauge\n</code></pre>"},{"location":"tutorials/jmx/integration/#testing","title":"Testing","text":"<p>Using <code>ddev</code> tool, you can test against the JMX service by providing a <code>dd_environment</code> in <code>tests/conftest.py</code> like this one:</p> <pre><code>@pytest.fixture(scope=\"session\")\ndef dd_environment():\n    compose_file = os.path.join(HERE, 'compose', 'docker-compose.yaml')\n    with docker_run(\n        compose_file,\n        conditions=[\n            # Kafka Broker\n            CheckDockerLogs('broker', 'Monitored service is now ready'),\n        ],\n    ):\n        yield CHECK_CONFIG, {'use_jmx': True}\n</code></pre> <p>And a <code>e2e</code> test like:</p> <pre><code>@pytest.mark.e2e\ndef test(dd_agent_check):\n    instance = {}\n    aggregator = dd_agent_check(instance)\n\n    for metric in ACTIVEMQ_E2E_METRICS + JVM_E2E_METRICS:\n        aggregator.assert_metric(metric)\n\n    aggregator.assert_all_metrics_covered()\n    aggregator.assert_metrics_using_metadata(get_metadata_metrics(), exclude=JVM_E2E_METRICS)\n</code></pre> <p>Real examples of:</p> <ul> <li>JMX dd_environment</li> <li>JMX e2e test</li> </ul>"},{"location":"tutorials/jmx/tools/","title":"JMX Tools","text":""},{"location":"tutorials/jmx/tools/#list-jmx-beans-using-jmxterm","title":"List JMX beans using JMXTerm","text":"<pre><code>curl -L https://github.com/jiaqi/jmxterm/releases/download/v1.0.1/jmxterm-1.0.1-uber.jar -o /tmp/jmxterm-1.0.1-uber.jar\njava -jar /tmp/jmxterm-1.0.1-uber.jar -l localhost:&lt;JMX_PORT&gt;\ndomains\nbeans\n</code></pre> <p>Example output:</p> <pre><code>$ curl -L https://github.com/jiaqi/jmxterm/releases/download/v1.0.1/jmxterm-1.0.1-uber.jar -o /tmp/jmxterm-1.0.1-uber.jar\n$ java -jar /tmp/jmxterm-1.0.1-uber.jar -l localhost:1616\nWelcome to JMX terminal. Type \"help\" for available commands.\n$&gt;domains\n#following domains are available\nJMImplementation\ncom.sun.management\nio.fabric8.insight\njava.lang\njava.nio\njava.util.logging\njmx4perl\njolokia\norg.apache.activemq\n$&gt;beans\n#domain = JMImplementation:\nJMImplementation:type=MBeanServerDelegate\n#domain = com.sun.management:\ncom.sun.management:type=DiagnosticCommand\ncom.sun.management:type=HotSpotDiagnostic\n#domain = io.fabric8.insight:\nio.fabric8.insight:type=LogQuery\n#domain = java.lang:\njava.lang:name=Code Cache,type=MemoryPool\njava.lang:name=CodeCacheManager,type=MemoryManager\njava.lang:name=Compressed Class Space,type=MemoryPool\njava.lang:name=Metaspace Manager,type=MemoryManager\njava.lang:name=Metaspace,type=MemoryPool\njava.lang:name=PS Eden Space,type=MemoryPool\njava.lang:name=PS MarkSweep,type=GarbageCollector\njava.lang:name=PS Old Gen,type=MemoryPool\njava.lang:name=PS Scavenge,type=GarbageCollector\njava.lang:name=PS Survivor Space,type=MemoryPool\njava.lang:type=ClassLoading\njava.lang:type=Compilation\njava.lang:type=Memory\njava.lang:type=OperatingSystem\njava.lang:type=Runtime\njava.lang:type=Threading\n[...]\n</code></pre>"},{"location":"tutorials/jmx/tools/#list-jmx-beans-using-jmxterm-with-extra-jars","title":"List JMX beans using JMXTerm with extra jars","text":"<p>In the example below, the extra jar is <code>jboss-client.jar</code>.</p> <pre><code>curl -L https://github.com/jiaqi/jmxterm/releases/download/v1.0.1/jmxterm-1.0.1-uber.jar -o /tmp/jmxterm-1.0.1-uber.jar\njava -cp &lt;PATH_WILDFLY&gt;/wildfly-17.0.1.Final/bin/client/jboss-client.jar:/tmp/jmxterm-1.0.1-uber.jar org.cyclopsgroup.jmxterm.boot.CliMain --url service:jmx:remote+http://localhost:9990 -u datadog -p pa$$word\ndomains\nbeans\n</code></pre>"},{"location":"tutorials/snmp/how-to/","title":"SNMP How-To","text":""},{"location":"tutorials/snmp/how-to/#simulate-snmp-devices","title":"Simulate SNMP devices","text":"<p>SNMP is a protocol for gathering metrics from network devices, but automated testing of the integration would not be practical nor reliable if we used actual devices.</p> <p>Our approach is to use a simulated SNMP device that responds to SNMP queries using simulation data.</p> <p>This simulated device is brought up as a Docker container when starting the SNMP test environment using:</p> <pre><code>ddev env start snmp [...]\n</code></pre>"},{"location":"tutorials/snmp/how-to/#test-snmp-profiles-locally","title":"Test SNMP profiles locally","text":"<p>Once the environment is up and running, you can modify the instance configuration to test profiles that support simulated metrics.</p> <p>The following is an example of an instance configured to use the Cisco Nexus profile.</p> <pre><code>init_config:\nprofiles:\ncisco_nexus:\ndefinition_file: cisco-nexus.yaml\n\ninstances:\n- community_string: cisco_nexus  # (1.)\nip_address: &lt;IP_ADDRESS_OF_SNMP_CONTAINER&gt;  # (2.)\nprofile: cisco_nexus\nname: localhost\nport: 1161\n</code></pre> <ol> <li>The <code>community_string</code> must match the corresponding device <code>.snmprec</code> file name. For example, <code>myprofile.snmprec</code> gives <code>community_string: myprofile</code>. This also applies to walk files: <code>myprofile.snmpwalk</code> gives <code>community_string: myprofile</code>.</li> <li>To find the IP address of the SNMP container, run:</li> </ol> <pre><code>docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' dd-snmp\n</code></pre>"},{"location":"tutorials/snmp/how-to/#run-snmp-queries","title":"Run SNMP queries","text":"<p>With the test environment is up and running, we can issue SNMP queries to the simulated device using a command line SNMP client.</p>"},{"location":"tutorials/snmp/how-to/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have the Net-SNMP tools installed on your machine. These should come pre-installed by default on Linux and macOS. If necessary, you can download them on the Net-SNMP website.</p>"},{"location":"tutorials/snmp/how-to/#available-commands","title":"Available commands","text":"<p>The Net-SNMP tools provide a number of commands to interact with SNMP devices.</p> <p>The most commonly used commands are:</p> <ul> <li><code>snmpget</code>: to issue an SNMP GET query.</li> <li><code>snmpgetnext</code>: to issue an SNMP GETNEXT query.</li> <li><code>snmpwalk</code>: to query an entire OID sub-tree at once.</li> <li><code>snmptable</code>: to query rows in an SNMP table.</li> </ul>"},{"location":"tutorials/snmp/how-to/#examples","title":"Examples","text":""},{"location":"tutorials/snmp/how-to/#get-query","title":"GET query","text":"<p>To query a specific OID from a device, we can use the <code>snmpget</code> command.</p> <p>For example, the following command will query <code>sysDescr</code> OID of an SNMP device, which returns its human-readable description:</p> <pre><code>$ snmpget -v 2c -c public -IR 127.0.0.1:1161 system.sysDescr.0\nSNMPv2-MIB::sysDescr.0 = STRING: Linux 41ba948911b9 4.9.87-linuxkit-aufs #1 SMP Wed Mar 14 15:12:16 UTC 2018 x86_64\nSNMPv2-MIB::sysORUpTime.1 = Timeticks: (9) 0:00:00.09\n</code></pre> <p>Let's break this command down:</p> <ul> <li><code>snmpget</code>: this command sends an SNMP GET request, and can be used to query the value of an OID. Here, we are requesting the <code>system.sysDescr.0</code> OID.</li> <li><code>-v 2c</code>: instructs your SNMP client to send the request using SNMP version 2c. See SNMP Versions.</li> <li><code>-c public</code>: instructs the SNMP client to send the community string <code>public</code> along with our request. (This is a form of authentication provided by SNMP v2. See SNMP Versions.)</li> <li><code>127.0.0.1:1161</code>: this is the host and port where the simulated SNMP agent is available at. (Confirm the port used by the ddev environment by inspecting the Docker port mapping via <code>$ docker ps</code>.)</li> <li><code>system.sysDescr.0</code>: this is the OID that the client should request. In practice this can refer to either a fully-resolved OID (e.g. <code>1.3.6.1.4.1[...]</code>), or a label (e.g. <code>sysDescr.0</code>).</li> <li><code>-IR</code>: this option allows us to use labels for OIDs that aren't in the generic <code>1.3.6.1.2.1.*</code> sub-tree (see: The OID tree). TL;DR: always use this option when working with OIDs coming from vendor-specific MIBs.</li> </ul> <p>Tip</p> <p>If the above command fails, try using the explicit OID like so:</p> <pre><code>$ snmpget -v 2c -c public -IR 127.0.0.1:1161 iso.3.6.1.2.1.1.1.0\n</code></pre>"},{"location":"tutorials/snmp/how-to/#table-query","title":"Table query","text":"<p>For tables, use the <code>snmptable</code> command, which will output the rows in the table in a tabular format. Its arguments and options are similar to <code>snmpget</code>.</p> <pre><code>$ snmptable -v 2c -c public -IR -Os 127.0.0.1:1161 hrStorageTable\nSNMP table: hrStorageTable\n\n hrStorageIndex          hrStorageType    hrStorageDescr hrStorageAllocationUnits hrStorageSize hrStorageUsed hrStorageAllocationFailures\n              1           hrStorageRam   Physical memory               1024 Bytes       2046940       1969964                           ?\n              3 hrStorageVirtualMemory    Virtual memory               1024 Bytes       3095512       1969964                           ?\n              6         hrStorageOther    Memory buffers               1024 Bytes       2046940         73580                           ?\n              7         hrStorageOther     Cached memory               1024 Bytes       1577648       1577648                           ?\n              8         hrStorageOther     Shared memory               1024 Bytes          2940          2940                           ?\n             10 hrStorageVirtualMemory        Swap space               1024 Bytes       1048572             0                           ?\n             33     hrStorageFixedDisk              /dev               4096 Bytes         16384             0                           ?\n             36     hrStorageFixedDisk    /sys/fs/cgroup               4096 Bytes        255867             0                           ?\n             52     hrStorageFixedDisk  /etc/resolv.conf               4096 Bytes      16448139       6493059                           ?\n             53     hrStorageFixedDisk     /etc/hostname               4096 Bytes      16448139       6493059                           ?\n             54     hrStorageFixedDisk        /etc/hosts               4096 Bytes      16448139       6493059                           ?\n             55     hrStorageFixedDisk          /dev/shm               4096 Bytes         16384             0                           ?\n             61     hrStorageFixedDisk       /proc/kcore               4096 Bytes         16384             0                           ?\n             62     hrStorageFixedDisk        /proc/keys               4096 Bytes         16384             0                           ?\n             63     hrStorageFixedDisk  /proc/timer_list               4096 Bytes         16384             0                           ?\n             64     hrStorageFixedDisk /proc/sched_debug               4096 Bytes         16384             0                           ?\n             65     hrStorageFixedDisk     /sys/firmware               4096 Bytes        255867             0                           ?\n</code></pre> <p>(In this case, we added the <code>-Os</code> option which prints only the last symbolic element and reduces the output of <code>hrStorageTypes</code>.)</p>"},{"location":"tutorials/snmp/how-to/#walk-query","title":"Walk query","text":"<p>A walk query can be used to query all OIDs in a given sub-tree.</p> <p>The <code>snmpwalk</code> command can be used to perform a walk query.</p> <p>To facilitate usage of walk files for debugging, the following options are recommended: <code>-ObentU</code>. Here's what each option does:</p> <ul> <li><code>b</code>: do not break OID indexes down.</li> <li><code>e</code>: print enums numerically (for example, <code>24</code> instead of <code>softwareLoopback(24)</code>).</li> <li><code>n</code>: print OIDs numerically (for example, <code>.1.3.6.1.2.1.2.2.1.1.1</code> instead of <code>IF-MIB::ifIndex.1</code>).</li> <li><code>t</code>: print timeticks numerically (for example, <code>4226041</code> instead of <code>Timeticks: (4226041) 11:44:20.41</code>).</li> <li><code>U</code>: don't print units.</li> </ul> <p>For example, the following command gets a walk of the <code>1.3.6.1.2.1.1</code> (<code>system</code>) sub-tree:</p> <pre><code>$ snmpwalk -v 2c -c public -ObentU 127.0.0.1:1161 1.3.6.1.2.1.1\n.1.3.6.1.2.1.1.1.0 = STRING: Linux 41ba948911b9 4.9.87-linuxkit-aufs #1 SMP Wed Mar 14 15:12:16 UTC 2018 x86_64\n.1.3.6.1.2.1.1.2.0 = OID: .1.3.6.1.4.1.8072.3.2.10\n.1.3.6.1.2.1.1.3.0 = 4226041\n.1.3.6.1.2.1.1.4.0 = STRING: root@localhost\n.1.3.6.1.2.1.1.5.0 = STRING: 41ba948911b9\n.1.3.6.1.2.1.1.6.0 = STRING: Unknown\n.1.3.6.1.2.1.1.8.0 = 9\n.1.3.6.1.2.1.1.9.1.2.1 = OID: .1.3.6.1.6.3.11.3.1.1\n.1.3.6.1.2.1.1.9.1.2.2 = OID: .1.3.6.1.6.3.15.2.1.1\n.1.3.6.1.2.1.1.9.1.2.3 = OID: .1.3.6.1.6.3.10.3.1.1\n.1.3.6.1.2.1.1.9.1.2.4 = OID: .1.3.6.1.6.3.1\n.1.3.6.1.2.1.1.9.1.2.5 = OID: .1.3.6.1.2.1.49\n.1.3.6.1.2.1.1.9.1.2.6 = OID: .1.3.6.1.2.1.4\n.1.3.6.1.2.1.1.9.1.2.7 = OID: .1.3.6.1.2.1.50\n.1.3.6.1.2.1.1.9.1.2.8 = OID: .1.3.6.1.6.3.16.2.2.1\n.1.3.6.1.2.1.1.9.1.2.9 = OID: .1.3.6.1.6.3.13.3.1.3\n.1.3.6.1.2.1.1.9.1.2.10 = OID: .1.3.6.1.2.1.92\n.1.3.6.1.2.1.1.9.1.3.1 = STRING: The MIB for Message Processing and Dispatching.\n.1.3.6.1.2.1.1.9.1.3.2 = STRING: The management information definitions for the SNMP User-based Security Model.\n.1.3.6.1.2.1.1.9.1.3.3 = STRING: The SNMP Management Architecture MIB.\n.1.3.6.1.2.1.1.9.1.3.4 = STRING: The MIB module for SNMPv2 entities\n.1.3.6.1.2.1.1.9.1.3.5 = STRING: The MIB module for managing TCP implementations\n.1.3.6.1.2.1.1.9.1.3.6 = STRING: The MIB module for managing IP and ICMP implementations\n.1.3.6.1.2.1.1.9.1.3.7 = STRING: The MIB module for managing UDP implementations\n.1.3.6.1.2.1.1.9.1.3.8 = STRING: View-based Access Control Model for SNMP.\n.1.3.6.1.2.1.1.9.1.3.9 = STRING: The MIB modules for managing SNMP Notification, plus filtering.\n.1.3.6.1.2.1.1.9.1.3.10 = STRING: The MIB module for logging SNMP Notifications.\n.1.3.6.1.2.1.1.9.1.4.1 = 9\n.1.3.6.1.2.1.1.9.1.4.2 = 9\n.1.3.6.1.2.1.1.9.1.4.3 = 9\n.1.3.6.1.2.1.1.9.1.4.4 = 9\n.1.3.6.1.2.1.1.9.1.4.5 = 9\n.1.3.6.1.2.1.1.9.1.4.6 = 9\n.1.3.6.1.2.1.1.9.1.4.7 = 9\n.1.3.6.1.2.1.1.9.1.4.8 = 9\n.1.3.6.1.2.1.1.9.1.4.9 = 9\n.1.3.6.1.2.1.1.9.1.4.10 = 9\n</code></pre> <p>As you can see, all OIDs that the device has available in the <code>.1.3.6.1.2.1.1.*</code> sub-tree are returned. In particular, one can recognize:</p> <ul> <li><code>sysObjectID</code> (<code>.1.3.6.1.2.1.1.2.0 = OID: .1.3.6.1.4.1.8072.3.2.10</code>)</li> <li><code>sysUpTime</code> (<code>.1.3.6.1.2.1.1.3.0 = 4226041</code>)</li> <li><code>sysName</code> (<code>.1.3.6.1.2.1.1.5.0 = STRING: 41ba948911b9</code>).</li> </ul> <p>Here is another example that queries the entire contents of <code>ifTable</code> (the table in <code>IF-MIB</code> that contains information about network interfaces):</p> <pre><code>snmpwalk -v 2c -c public -OentU 127.0.0.1:1161 1.3.6.1.2.1.2.2\n.1.3.6.1.2.1.2.2.1.1.1 = INTEGER: 1\n.1.3.6.1.2.1.2.2.1.1.90 = INTEGER: 90\n.1.3.6.1.2.1.2.2.1.2.1 = STRING: lo\n.1.3.6.1.2.1.2.2.1.2.90 = STRING: eth0\n.1.3.6.1.2.1.2.2.1.3.1 = INTEGER: 24\n.1.3.6.1.2.1.2.2.1.3.90 = INTEGER: 6\n.1.3.6.1.2.1.2.2.1.4.1 = INTEGER: 65536\n.1.3.6.1.2.1.2.2.1.4.90 = INTEGER: 1500\n.1.3.6.1.2.1.2.2.1.5.1 = Gauge32: 10000000\n.1.3.6.1.2.1.2.2.1.5.90 = Gauge32: 4294967295\n.1.3.6.1.2.1.2.2.1.6.1 = STRING:\n.1.3.6.1.2.1.2.2.1.6.90 = STRING: 2:42:ac:11:0:2\n.1.3.6.1.2.1.2.2.1.7.1 = INTEGER: 1\n.1.3.6.1.2.1.2.2.1.7.90 = INTEGER: 1\n.1.3.6.1.2.1.2.2.1.8.1 = INTEGER: 1\n.1.3.6.1.2.1.2.2.1.8.90 = INTEGER: 1\n.1.3.6.1.2.1.2.2.1.9.1 = 0\n.1.3.6.1.2.1.2.2.1.9.90 = 0\n.1.3.6.1.2.1.2.2.1.10.1 = Counter32: 5300203\n.1.3.6.1.2.1.2.2.1.10.90 = Counter32: 2928\n.1.3.6.1.2.1.2.2.1.11.1 = Counter32: 63808\n.1.3.6.1.2.1.2.2.1.11.90 = Counter32: 40\n.1.3.6.1.2.1.2.2.1.12.1 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.12.90 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.13.1 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.13.90 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.14.1 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.14.90 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.15.1 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.15.90 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.16.1 = Counter32: 5300203\n.1.3.6.1.2.1.2.2.1.16.90 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.17.1 = Counter32: 63808\n.1.3.6.1.2.1.2.2.1.17.90 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.18.1 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.18.90 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.19.1 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.19.90 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.20.1 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.20.90 = Counter32: 0\n.1.3.6.1.2.1.2.2.1.21.1 = Gauge32: 0\n.1.3.6.1.2.1.2.2.1.21.90 = Gauge32: 0\n.1.3.6.1.2.1.2.2.1.22.1 = OID: .0.0\n.1.3.6.1.2.1.2.2.1.22.90 = OID: .0.0\n</code></pre>"},{"location":"tutorials/snmp/how-to/#generate-table-simulation-data","title":"Generate table simulation data","text":"<p>To generate simulation data for tables automatically, use the <code>mib2dev.py</code> tool shipped with <code>snmpsim</code>. This tool will be renamed as <code>snmpsim-record-mibs</code> in the upcoming 1.0 release of the library.</p> <p>First, install snmpsim:</p> <pre><code>pip install snmpsim\n</code></pre> <p>Then run the tool, specifying the MIB with the start and stop OIDs (which can correspond to .e.g the first and last columns in the table respectively).</p> <p>For example:</p> <pre><code>mib2dev.py --mib-module=&lt;MIB&gt; --start-oid=1.3.6.1.4.1.674.10892.1.400.20 --stop-oid=1.3.6.1.4.1.674.10892.1.600.12 &gt; /path/to/mytable.snmprec\n</code></pre> <p>The following command generates 4 rows for the <code>IF-MIB:ifTable (1.3.6.1.2.1.2.2)</code>:</p> <pre><code>mib2dev.py --mib-module=IF-MIB --start-oid=1.3.6.1.2.1.2.2 --stop-oid=1.3.6.1.2.1.2.3 --table-size=4 &gt; /path/to/mytable.snmprec\n</code></pre>"},{"location":"tutorials/snmp/how-to/#known-issues","title":"Known issues","text":"<p><code>mib2dev</code> has a known issue with <code>IF-MIB::ifPhysAddress</code>, that is expected to contain an hexadecimal string, but <code>mib2dev</code> fills it with a string. To fix this, provide a valid hextring when prompted on the command line:</p> <pre><code># Synthesizing row #1 of table 1.3.6.1.2.1.2.2.1\n*** Inconsistent value: Display format eval failure: b'driving kept zombies quaintly forward zombies': invalid literal for int() with base 16: 'driving kept zombies quaintly forward zombies'caused by &lt;class 'ValueError'&gt;: invalid literal for int() with base 16: 'driving kept zombies quaintly forward zombies'\n*** See constraints and suggest a better one for:\n# Table IF-MIB::ifTable\n# Row IF-MIB::ifEntry\n# Index IF-MIB::ifIndex (type InterfaceIndex)\n# Column IF-MIB::ifPhysAddress (type PhysAddress)\n# Value ['driving kept zombies quaintly forward zombies'] ? 001122334455\n</code></pre>"},{"location":"tutorials/snmp/how-to/#generate-simulation-data-from-a-walk","title":"Generate simulation data from a walk","text":"<p>As an alternative to <code>.snmprec</code> files, it is possible to use a walk as simulation data. This is especially useful when debugging live devices, since you can export the device walk and use this real data locally.</p> <p>To do so, paste the output of a walk query into a <code>.snmpwalk</code> file, and add this file to the test data directory. Then, pass the name of the walk file as the <code>community_string</code>. For more information, see Test SNMP profiles locally.</p>"},{"location":"tutorials/snmp/how-to/#find-where-mibs-are-installed-on-your-machine","title":"Find where MIBs are installed on your machine","text":"<p>See the Using and loading MIBs Net-SNMP tutorial.</p>"},{"location":"tutorials/snmp/how-to/#browse-locally-installed-mibs","title":"Browse locally installed MIBs","text":"<p>Since community resources that list MIBs and OIDs are best effort, the MIB you are investigating may not be present or may not be available in its the latest version.</p> <p>In that case, you can use the <code>snmptranslate</code> CLI tool to output similar information for MIBs installed on your system. This tool is part of Net-SNMP - see SNMP queries prerequisites.</p> <p>Steps</p> <ol> <li>Run <code>$ snmptranslate -m &lt;MIBNAME&gt; -Tz -On</code> to get a complete list of OIDs in the <code>&lt;MIBNAME&gt;</code> MIB along with their labels.</li> <li>Redirect to a file for nicer formatting as needed.</li> </ol> <p>Example:</p> <pre><code>$ snmptranslate -m IF-MIB -Tz -On &gt; out.log\n$ cat out.log\n\"org\"                   \"1.3\"\n\"dod\"                   \"1.3.6\"\n\"internet\"                      \"1.3.6.1\"\n\"directory\"                     \"1.3.6.1.1\"\n\"mgmt\"                  \"1.3.6.1.2\"\n\"mib-2\"                 \"1.3.6.1.2.1\"\n\"system\"                        \"1.3.6.1.2.1.1\"\n\"sysDescr\"                      \"1.3.6.1.2.1.1.1\"\n\"sysObjectID\"                   \"1.3.6.1.2.1.1.2\"\n\"sysUpTime\"                     \"1.3.6.1.2.1.1.3\"\n\"sysContact\"                    \"1.3.6.1.2.1.1.4\"\n\"sysName\"                       \"1.3.6.1.2.1.1.5\"\n\"sysLocation\"                   \"1.3.6.1.2.1.1.6\"\n[...]\n</code></pre> <p>Tip</p> <p>Use the <code>-M &lt;DIR&gt;</code> option to specify the directory where <code>snmptranslate</code> should look for MIBs. Useful if you want to inspect a MIB you've just downloaded but not moved to the default MIB directory.</p> <p>Tip</p> <p>Use <code>-Tp</code> for an alternative tree-like formatting.</p>"},{"location":"tutorials/snmp/introduction/","title":"Introduction to SNMP","text":"<p>In this introduction, we'll cover general information about the SNMP protocol, including key concepts such as OIDs and MIBs.</p> <p>If you're already familiar with the SNMP protocol, feel free to skip to the next page.</p>"},{"location":"tutorials/snmp/introduction/#what-is-snmp","title":"What is SNMP?","text":""},{"location":"tutorials/snmp/introduction/#overview","title":"Overview","text":"<p>SNMP (Simple Network Management Protocol) is a protocol for monitoring network devices. It uses UDP and supports both a request/response model (commands and queries) and a notification model (traps, informs).</p> <p>In the request/response model, the SNMP manager (eg. the Datadog Agent) issues an SNMP command (<code>GET</code>, <code>GETNEXT</code>, <code>BULK</code>) to an SNMP agent (eg. a network device).</p> <p>SNMP was born in the 1980s, so it has been around for a long time. While more modern alternatives like NETCONF and OpenConfig have been gaining attention, a large amount of network devices still use SNMP as their primary monitoring interface.</p>"},{"location":"tutorials/snmp/introduction/#snmp-versions","title":"SNMP versions","text":"<p>The SNMP protocol exists in 3 versions: <code>v1</code> (legacy), <code>v2c</code>, and <code>v3</code>.</p> <p>The main differences between v1/v2c and v3 are the authentication mechanism and transport layer, as summarized below.</p> Version Authentication Transport layer v1/v2c Password (the community string) Plain text only v3 Username/password Support for packet signing and encryption"},{"location":"tutorials/snmp/introduction/#oids","title":"OIDs","text":""},{"location":"tutorials/snmp/introduction/#what-is-an-oid","title":"What is an OID?","text":"<p>Identifiers for queryable quantities</p> <p>An OID, also known as an Object Identifier, is an identifier for a quantity (\"object\") that can be retrieved from an SNMP device. Such quantities may include uptime, temperature, network traffic, etc (quantities available will vary across devices).</p> <p>To make them processable by machines, OIDs are represented as dot-separated sequences of numbers, e.g. <code>1.3.6.1.2.1.1.1</code>.</p> <p>Global definition</p> <p>OIDs are globally defined, which means they have the same meaning regardless of the device that processes the SNMP query. For example, querying the <code>1.3.6.1.2.1.1.1</code> OID (also known as <code>sysDescr</code>) on any SNMP agent will make it return the system description. (More on the OID/label mapping can be found in the MIBs section below.)</p> <p>Not all OIDs contain metrics data</p> <p>OIDs can refer to various types of objects, such as strings, numbers, tables, etc.</p> <p>In particular, this means that only a fraction of OIDs refer to numerical quantities that can actually be sent as metrics to Datadog. However, non-numerical OIDs can also be useful, especially for tagging.</p>"},{"location":"tutorials/snmp/introduction/#the-oid-tree","title":"The OID tree","text":"<p>OIDs are structured in a tree-like fashion. Each number in the OID represents a node in the tree.</p> <p>The wildcard notation is often used to refer to a sub-tree of OIDs, e.g. <code>1.3.6.1.2.*</code>.</p> <p>It so happens that there are two main OID sub-trees: a sub-tree for general-purpose OIDs, and a sub-tree for vendor-specific OIDs.</p>"},{"location":"tutorials/snmp/introduction/#generic-oids","title":"Generic OIDs","text":"<p>Located under the sub-tree: <code>1.3.6.1.2.1.*</code> (a.k.a.<code>SNMPv2-MIB</code> or <code>mib-2</code>).</p> <p>These OIDs are applicable to all kinds of network devices (although all devices may not expose all OIDs in this sub-tree).</p> <p>For example, <code>1.3.6.1.2.1.1.1</code> corresponds to <code>sysDescr</code>, which contains a free-form, human-readable description of the device.</p>"},{"location":"tutorials/snmp/introduction/#vendor-specific-oids","title":"Vendor-specific OIDs","text":"<p>Located under the sub-tree: <code>1.3.6.1.4.1.*</code> (a.k.a. <code>enterprises</code>).</p> <p>These OIDs are defined and managed by network device vendors themselves.</p> <p>Each vendor is assigned its own enterprise sub-tree in the form of <code>1.3.6.1.4.1.&lt;N&gt;.*</code>.</p> <p>For example:</p> <ul> <li><code>1.3.6.1.4.1.2.*</code> is the sub-tree for IBM-specific OIDs.</li> <li><code>1.3.6.1.4.1.9.*</code> is the sub-tree for Cisco-specific OIDs.</li> </ul> <p>The full list of vendor sub-trees can be found here: SNMP OID 1.3.6.1.4.1.</p>"},{"location":"tutorials/snmp/introduction/#notable-oids","title":"Notable OIDs","text":"OID Label Description <code>1.3.6.1.2.1.2</code> <code>sysObjectId</code> An OID whose value is an OID that represents the device make and model (yes, it's a bit meta). <code>1.3.6.1.2.1.1.1</code> <code>sysDescr</code> A human-readable, free-form description of the device. <code>1.3.6.1.2.1.1.3</code> <code>sysUpTimeInstance</code> The device uptime."},{"location":"tutorials/snmp/introduction/#mibs","title":"MIBs","text":""},{"location":"tutorials/snmp/introduction/#what-is-an-mib","title":"What is an MIB?","text":"<p>OIDs are grouped in modules called MIBs (Management Information Base). An MIB describes the hierarchy of a given set of OIDs. (This is somewhat analogous to a dictionary that contains the definitions for each word in a spoken language.)</p> <p>For example, the <code>IF-MIB</code> describes the hierarchy of OIDs within the sub-tree <code>1.3.6.1.2.1.2.*</code>. These OIDs contain metrics about the network interfaces available on the device. (Note how its location under the <code>1.3.6.1.2.*</code> sub-tree indicates that it is a generic MIB, available on most network devices.)</p> <p>As part of the description of OIDs, an MIB defines a human-readable label for each OID. For example, <code>IF-MIB</code> describes the OID <code>1.3.6.1.2.1.1</code> and assigns it the label <code>sysDescr</code>. The operation that consists in finding the OID from a label is called OID resolution.</p>"},{"location":"tutorials/snmp/introduction/#tools-and-resources","title":"Tools and resources","text":"<p>The following resources can be useful when working with MIBs:</p> <ul> <li>MIB Discovery: a search engine for OIDs. Use it to find what an OID corresponds to, which MIB it comes from, what label it is known as, etc.</li> <li>Circitor MIB files repository: a repository and search engine where one can download actual <code>.mib</code> files.</li> <li>SNMP Labs MIB repository: alternate repo of many common MIBs. Note: this site hosts the underlying MIBs which the <code>pysnmp-mibs</code> library (used by the SNMP Python check) actually validates against. Double check any MIB you get from an alternate source with what is in this repo.</li> </ul>"},{"location":"tutorials/snmp/introduction/#learn-more","title":"Learn more","text":"<p>For other high-level overviews of SNMP, see:</p> <ul> <li>How SNMP Works (Youtube)</li> <li>SNMP (Wikipedia)</li> <li>Tutorials: Internet Management and SNMP (YouTube) (In-depth videos about SNMP architecture, MIBs, protocol data structures, security models, monitoring code examples, etc.)</li> </ul>"},{"location":"tutorials/snmp/profile-format/","title":"Profile Format Reference","text":""},{"location":"tutorials/snmp/profile-format/#overview","title":"Overview","text":"<p>SNMP profiles are our way of providing out-of-the-box monitoring for certain makes and models of network devices.</p> <p>An SNMP profile is materialised as a YAML file with the following structure:</p> <pre><code>sysobjectid: &lt;x.y.z...&gt;\n\n# extends:\n#   &lt;Optional list of base profiles to extend from...&gt;\n\nmetrics:\n# &lt;List of metrics to collect...&gt;\n\n# metric_tags:\n#   &lt;List of tags to apply to collected metrics. Required for table metrics, optional otherwise&gt;\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#fields","title":"Fields","text":""},{"location":"tutorials/snmp/profile-format/#sysobjectid","title":"<code>sysobjectid</code>","text":"<p>(Required)</p> <p>The <code>sysobjectid</code> field is used to match profiles against devices during device autodiscovery.</p> <p>It can refer to a fully-defined OID for a specific device make and model:</p> <pre><code>sysobjectid: 1.3.6.1.4.1.232.9.4.10\n</code></pre> <p>or a wildcard pattern to address multiple device models:</p> <pre><code>sysobjectid: 1.3.6.1.131.12.4.*\n</code></pre> <p>or a list of fully-defined OID / wildcard patterns:</p> <pre><code>sysobjectid:\n- 1.3.6.1.131.12.4.*\n- 1.3.6.1.4.1.232.9.4.10\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#extends","title":"<code>extends</code>","text":"<p>(Optional)</p> <p>This field can be used to include metrics and metric tags from other so-called base profiles. Base profiles can derive from other base profiles to build a hierarchy of reusable profile mixins.</p> <p>Important</p> <p>All device profiles should extend from the <code>_base.yaml</code> profile, which defines items that should be collected for all devices.</p> <p>Example:</p> <pre><code>extends:\n- _base.yaml\n- _generic-if.yaml  # Include basic metrics from IF-MIB.\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#metrics","title":"<code>metrics</code>","text":"<p>(Required)</p> <p>Entries in the <code>metrics</code> field define which metrics will be collected by the profile. They can reference either a single OID (a.k.a symbol), or an SNMP table.</p>"},{"location":"tutorials/snmp/profile-format/#symbol-metrics","title":"Symbol metrics","text":"<p>An SNMP symbol is an object with a scalar type (i.e. <code>Counter32</code>, <code>Integer32</code>, <code>OctetString</code>, etc).</p> <p>In a MIB file, a symbol can be recognized as an <code>OBJECT-TYPE</code> node with a scalar <code>SYNTAX</code>, placed under an <code>OBJECT IDENTIFIER</code> node (which is often the root OID of the MIB):</p> <pre><code>EXAMPLE-MIB DEFINITIONS ::= BEGIN\n-- ...\nexample OBJECT IDENTIFIER ::= { mib-2 7 }\n\nexampleSymbol OBJECT-TYPE\n    SYNTAX Counter32\n    -- ...\n    ::= { example 1 }\n</code></pre> <p>In profiles, symbol metrics can be specified as entries that specify the <code>MIB</code> and <code>symbol</code> fields:</p> <pre><code>metrics:\n# Example for the above dummy MIB and symbol:\n- MIB: EXAMPLE-MIB\nsymbol:\nOID: 1.3.5.1.2.1.7.1\nname: exampleSymbol\n# More realistic examples:\n- MIB: ISILON-MIB\nsymbol:\nOID: 1.3.6.1.4.1.12124.1.1.2\nname: clusterHealth\n- MIB: ISILON-MIB\nsymbol:\nOID: 1.3.6.1.4.1.12124.1.2.1.1\nname: clusterIfsInBytes\n- MIB: ISILON-MIB\nsymbol:\nOID: 1.3.6.1.4.1.12124.1.2.1.3\nname: clusterIfsOutBytes\n</code></pre> <p>Warning</p> <p>Symbol metrics from the same <code>MIB</code> must still be listed as separate <code>metrics</code> entries, as shown above.</p> <p>For example, this is not valid syntax:</p> <pre><code>metrics:\n- MIB: ISILON-MIB\nsymbol:\n- OID: 1.3.6.1.4.1.12124.1.2.1.1\nname: clusterIfsInBytes\n- OID: 1.3.6.1.4.1.12124.1.2.1.3\nname: clusterIfsOutBytes\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#table-metrics","title":"Table metrics","text":"<p>An SNMP table is an object that is composed of multiple entries (\"rows\"), where each entry contains values a set of symbols (\"columns\").</p> <p>In a MIB file, tables be recognized by the presence of <code>SEQUENCE OF</code>:</p> <pre><code>exampleTable OBJECT-TYPE\n    SYNTAX   SEQUENCE OF exampleEntry\n    -- ...\n    ::= { example 10 }\n\nexampleEntry OBJECT-TYPE\n   -- ...\n   ::= { exampleTable 1 }\n\nexampleColumn1 OBJECT-TYPE\n   -- ...\n   ::= { exampleEntry 1 }\n\nexampleColumn2 OBJECT-TYPE\n   -- ...\n   ::= { exampleEntry 2 }\n\n-- ...\n</code></pre> <p>In profiles, tables can be specified as entries containing the <code>MIB</code>, <code>table</code> and <code>symbols</code> fields. The syntax for the value contained in each row is typically <code>&lt;TABLE_OID&gt;.1.&lt;COLUMN_ID&gt;.&lt;INDEX&gt;</code>:</p> <pre><code>metrics:\n# Example for the dummy table above:\n- MIB: EXAMPLE-MIB\ntable:\n# Identification of the table which metrics come from.\nOID: 1.3.6.1.4.1.10\nname: exampleTable\nsymbols:\n# List of symbols ('columns') to retrieve.\n# Same format as for a single OID.\n# The value from each row (index) in the table will be collected `&lt;TABLE_OID&gt;.1.&lt;COLUMN_ID&gt;.&lt;INDEX&gt;`\n- OID: 1.3.6.1.4.1.10.1.1\nname: exampleColumn1\n- OID: 1.3.6.1.4.1.10.1.2\nname: exampleColumn2\n# ...\n\n# More realistic example:\n- MIB: CISCO-PROCESS-MIB\ntable:\n# Each row in this table contains information about a CPU unit of the device.\nOID: 1.3.6.1.4.1.9.9.109.1.1.1\nname: cpmCPUTotalTable\nsymbols:\n- OID: 1.3.6.1.4.1.9.9.109.1.1.1.1.12\nname: cpmCPUMemoryUsed\n# ...\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#table-metrics-tagging","title":"Table metrics tagging","text":"<p>Table metrics require <code>metric_tags</code> to identify each row's metric.  It is possible to add tags to metrics retrieved from a table in three ways:</p>"},{"location":"tutorials/snmp/profile-format/#using-a-column-within-the-same-table","title":"Using a column within the same table","text":"<pre><code>metrics:\n- MIB: IF-MIB\ntable:\nOID: 1.3.6.1.2.1.2.2\nname: ifTable\nsymbols:\n- OID: 1.3.6.1.2.1.2.2.1.14\nname: ifInErrors\n# ...\nmetric_tags:\n# Add an 'interface' tag to each metric of each row,\n# whose value is obtained from the 'ifDescr' column of the row.\n# This allows querying metrics by interface, e.g. 'interface:eth0'.\n- tag: interface\ncolumn:\nOID: 1.3.6.1.2.1.2.2.1.2\nname: ifDescr\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#using-a-column-from-a-different-table-with-identical-indexes","title":"Using a column from a different table with identical indexes","text":"<pre><code>metrics:\n- MIB: CISCO-IF-EXTENSION-MIB\nmetric_type: monotonic_count\ntable:\nOID: 1.3.6.1.4.1.9.9.276.1.1.2\nname: cieIfInterfaceTable\nsymbols:\n- OID: 1.3.6.1.4.1.9.9.276.1.1.2.1.1\nname: cieIfResetCount\nmetric_tags:\n- MIB: IF-MIB\ncolumn:\nOID: 1.3.6.1.2.1.31.1.1.1.1\nname: ifName\ntable: ifXTable\ntag: interface\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#using-a-column-from-a-different-table-with-different-indexes","title":"Using a column from a different table with different indexes","text":"<pre><code>metrics:\n- MIB: CPI-UNITY-MIB\ntable:\nOID: 1.3.6.1.4.1.30932.1.10.1.3.110\nname: cpiPduBranchTable\nsymbols:\n- OID: 1.3.6.1.4.1.30932.1.10.1.3.110.1.3\nname: cpiPduBranchCurrent\nmetric_tags:\n- column:\nOID: 1.3.6.1.4.1.30932.1.10.1.2.10.1.3\nname: cpiPduName\ntable: cpiPduTable\nindex_transform:\n- start: 1\nend: 7\ntag: pdu_name\n</code></pre> <p>If the external table has different indexes, use <code>index_transform</code> to select a subset of the full index. <code>index_transform</code> is a list of <code>start</code>/<code>end</code> ranges to extract from the current table index to match the external table index. <code>start</code> and <code>end</code> are inclusive.</p> <p>External table indexes must be a subset of the indexes of the current table, or same indexes in a different order.</p> <p>Example</p> <p>In the example above, the index of <code>cpiPduBranchTable</code> looks like <code>1.6.0.36.155.53.3.246</code>, the first digit is the <code>cpiPduBranchId</code> index and the rest is the <code>cpiPduBranchMac</code> index. The index of <code>cpiPduTable</code> looks like <code>6.0.36.155.53.3.246</code> and represents <code>cpiPduMac</code> (equivalent to <code>cpiPduBranchMac</code>).</p> <p>By using the <code>index_transform</code> with start 1 and end 7, we extract <code>6.0.36.155.53.3.246</code> from <code>1.6.0.36.155.53.3.246</code> (<code>cpiPduBranchTable</code> full index), and then use it to match <code>6.0.36.155.53.3.246</code> (<code>cpiPduTable</code> full index).</p> <p><code>index_transform</code> can be more complex, the following definition will extract <code>2.3.5.6.7</code> from <code>1.2.3.4.5.6.7</code>.</p> <pre><code>        index_transform:\n- start: 1\nend: 2\n- start: 4\nend: 6\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#mapping-column-to-tag-string-value","title":"Mapping column to tag string value","text":"<p>You can use the following syntax to map OID values to tag string values. In the example below, the submitted metrics will be <code>snmp.ifInOctets</code> with tags like <code>if_type:regular1822</code>. Available in Agent 7.45+.</p> <pre><code>metrics:\n- MIB: IP-MIB\ntable:\nOID: 1.3.6.1.2.1.2.2\nname: ifTable\nsymbols:\n- OID: 1.3.6.1.2.1.2.2.1.10\nname: ifInOctets\nmetric_tags:\n- tag: if_type\ncolumn:\nOID: 1.3.6.1.2.1.2.2.1.3\nname: ifType\nmapping:\n1: other\n2: regular1822\n3: hdh1822\n4: ddn-x25\n29: ultra\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#using-an-index","title":"Using an index","text":"<p>Important: \"index\" refers to one digit of the index part of the row OID. For example, if the column OID is <code>1.2.3.1.2</code> and the row OID is <code>1.2.3.1.2.7.8.9</code>, the full index is <code>7.8.9</code>. In this example, <code>index: 1</code> refers to <code>7</code> and <code>index: 2</code> refers to <code>8</code>, and so on.  </p> <p>Here is specific example of an OID with multiple positions in the index (OID ref):</p> <pre><code>cfwConnectionStatEntry OBJECT-TYPE\n    SYNTAX CfwConnectionStatEntry\n    ACCESS not-accessible\n    STATUS mandatory\n    DESCRIPTION\n        \"An entry in the table, containing information about a\n        firewall statistic.\"\n    INDEX { cfwConnectionStatService, cfwConnectionStatType }\n    ::= { cfwConnectionStatTable 1 }\n</code></pre> <p>The index in the case is a combination of <code>cfwConnectionStatService</code> and <code>cfwConnectionStatType</code>. Inspecting the <code>OBJECT-TYPE</code> of <code>cfwConnectionStatService</code> reveals the <code>SYNTAX</code> as <code>Services</code> (OID ref):</p> <p><pre><code>cfwConnectionStatService OBJECT-TYPE\n        SYNTAX     Services\n        MAX-ACCESS not-accessible\n        STATUS     current\n        DESCRIPTION\n            \"The identification of the type of connection providing\n            statistics.\"\n    ::= { cfwConnectionStatEntry 1 }\n</code></pre> For example, when we fetch the value of <code>cfwConnectionStatValue</code>, the OID with the index is like <code>1.3.6.1.4.1.9.9.147.1.2.2.2.1.5.20.2</code> = <code>4087850099</code>, here the indexes are 20.2 (<code>1.3.6.1.4.1.9.9.147.1.2.2.2.1.5.&lt;service type&gt;.&lt;stat type&gt;</code>).  Here is how we would specify this configuration in the yaml (as seen in the corresponding profile packaged with the agent): </p> <pre><code>metrics:\n- MIB: CISCO-FIREWALL-MIB\ntable:\nOID: 1.3.6.1.4.1.9.9.147.1.2.2.2\nname: cfwConnectionStatTable\nsymbols:\n- OID: 1.3.6.1.4.1.9.9.147.1.2.2.2.1.5\nname: cfwConnectionStatValue\nmetric_tags:\n- index: 1 // capture first index digit\ntag: service_type\n- index: 2 // capture second index digit\ntag: stat_type\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#mapping-index-to-tag-string-value","title":"Mapping index to tag string value","text":"<p>You can use the following syntax to map indexes to tag string values. In the example below, the submitted metrics will be <code>snmp.ipSystemStatsHCInReceives</code> with tags like <code>ipversion:ipv6</code>.</p> <pre><code>metrics:\n- MIB: IP-MIB\ntable:\nOID: 1.3.6.1.2.1.4.31.1\nname: ipSystemStatsTable\nmetric_type: monotonic_count\nsymbols:\n- OID: 1.3.6.1.2.1.4.31.1.1.4\nname: ipSystemStatsHCInReceives\nmetric_tags:\n- index: 1\ntag: ipversion\nmapping:\n0: unknown\n1: ipv4\n2: ipv6\n3: ipv4z\n4: ipv6z\n16: dns\n</code></pre> <p>See meaning of index as used here in Using an index section.</p>"},{"location":"tutorials/snmp/profile-format/#tagging-tips","title":"Tagging tips","text":"<p>Note</p> <p>General guidelines on Datadog tagging also apply to table metric tags.</p> <p>In particular, be mindful of the kind of value contained in the columns used a tag sources. E.g. avoid using a <code>DisplayString</code> (an arbitrarily long human-readable text description) or unbounded sources (timestamps, IDs...) as tag values.</p> <p>Good candidates for tag values include short strings, enums, or integer indexes.</p>"},{"location":"tutorials/snmp/profile-format/#metric-type-inference","title":"Metric type inference","text":"<p>By default, the Datadog metric type of a symbol will be inferred from the SNMP type (i.e. the MIB <code>SYNTAX</code>):</p> SNMP type Inferred metric type <code>Counter32</code> <code>rate</code> <code>Counter64</code> <code>rate</code> <code>Gauge32</code> <code>gauge</code> <code>Integer</code> <code>gauge</code> <code>Integer32</code> <code>gauge</code> <code>CounterBasedGauge64</code> <code>gauge</code> <code>Opaque</code> <code>gauge</code> <p>SNMP types not listed in this table are submitted as <code>gauge</code> by default.</p>"},{"location":"tutorials/snmp/profile-format/#forced-metric-types","title":"Forced metric types","text":"<p>Sometimes the inferred type may not be what you want. Typically, OIDs that represent \"total number of X\" are defined as <code>Counter32</code> in MIBs, but you probably want to submit them <code>monotonic_count</code> instead of a <code>rate</code>.</p> <p>For such cases, you can define a <code>metric_type</code>. Possible values and their effect are listed below.</p> Forced type Description <code>gauge</code> Submit as a gauge. <code>rate</code> Submit as a rate. <code>percent</code> Multiply by 100 and submit as a rate. <code>monotonic_count</code> Submit as a monotonic count. <code>monotonic_count_and_rate</code> Submit 2 copies of the metric: one as a monotonic count, and one as a rate (suffixed with <code>.rate</code>). <code>flag_stream</code> Submit each flag of a flag stream as individual metric with value <code>0</code> or <code>1</code>. See Flag Stream section. <p>This works on both symbol and table metrics:</p> <pre><code>metrics:\n# On a symbol:\n- MIB: TCP-MIB\nsymbol:\nOID: 1.3.6.1.2.1.6.5\nname: tcpActiveOpens\nmetric_type: monotonic_count\n# On a table, apply same metric_type to all metrics:\n- MIB: IP-MIB\ntable:\nOID: 1.3.6.1.2.1.4.31.1\nname: ipSystemStatsTable\nmetric_type: monotonic_count\nsymbols:\n- OID: 1.3.6.1.2.1.4.31.1.1.4\nname: ipSystemStatsHCInReceives\n- OID: 1.3.6.1.2.1.4.31.1.1.6\nname: ipSystemStatsHCInOctets\n# On a table, apply different metric_type per metric:\n- MIB: IP-MIB\ntable:\nOID: 1.3.6.1.2.1.4.31.1\nname: ipSystemStatsTable\nsymbols:\n- OID: 1.3.6.1.2.1.4.31.1.1.4\nname: ipSystemStatsHCInReceives\nmetric_type: monotonic_count\n- OID: 1.3.6.1.2.1.4.31.1.1.6\nname: ipSystemStatsHCInOctets\nmetric_type: gauge\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#flag-stream","title":"Flag stream","text":"<p>When the value is a flag stream like <code>010101</code>, you can use <code>metric_type: flag_stream</code> to submit each flag as individual metric with value <code>0</code> or <code>1</code>. Two options are required when using <code>flag_stream</code>:</p> <ul> <li><code>options.placement</code>: position of the flag in the flag stream (1-based indexing, first element is placement 1).</li> <li><code>options.metric_suffix</code>: suffix appended to the metric name for a specific flag, usually matching the name of the flag. </li> </ul> <p>Example:</p> <pre><code>metrics:\n- MIB: PowerNet-MIB\nsymbol:\nOID: 1.3.6.1.4.1.318.1.1.1.11.1.1.0\nname: upsBasicStateOutputState\nmetric_type: flag_stream\noptions:\nplacement: 4\nmetric_suffix: OnLine\n- MIB: PowerNet-MIB\nsymbol:\nOID: 1.3.6.1.4.1.318.1.1.1.11.1.1.0\nname: upsBasicStateOutputState\nmetric_type: flag_stream\noptions:\nplacement: 5\nmetric_suffix: ReplaceBattery\n</code></pre> <p>This example will submit two metrics <code>snmp.upsBasicStateOutputState.OnLine</code> and <code>snmp.upsBasicStateOutputState.ReplaceBattery</code> with value <code>0</code> or <code>1</code>.</p> <p>Example of flag_stream usage in a profile.</p>"},{"location":"tutorials/snmp/profile-format/#report-string-oids","title":"Report string OIDs","text":"<p>To report statuses from your network devices, you can use the constant metrics feature available in Agent 7.45+.</p> <p><code>constant_value_one</code> sends a constant metric, equal to one, that can be tagged with string properties.</p> <p>Example use case:</p> <pre><code>metrics:\n- MIB: MY-MIB\nsymbols:\n- name: myDevice\nconstant_value_one: true\nmetric_tags:\n- tag: status\ncolumn:\nOID: 1.2.3.4\nname: myStatus\nmapping:\n1: up\n2: down\n# ...\n</code></pre> <p>An <code>snmp.myDevice</code> metric is sent, with a value of 1 and tagged by statuses. This allows you to monitor status changes, number of devices per state, etc., in Datadog.</p>"},{"location":"tutorials/snmp/profile-format/#metric_tags","title":"<code>metric_tags</code>","text":"<p>(Optional)</p> <p>This field is used to apply tags to all metrics collected by the profile. It has the same meaning than the instance-level config option (see <code>conf.yaml.example</code>).</p> <p>Several collection methods are supported, as illustrated below:</p> <pre><code>metric_tags:\n- OID: 1.3.6.1.2.1.1.5.0\nsymbol: sysName\ntag: snmp_host\n- # With regular expression matching\nOID: 1.3.6.1.2.1.1.5.0\nsymbol: sysName\nmatch: (.*)-(.*)\ntags:\ndevice_type: \\1\nhost: \\2\n- # With value mapping\nOID: 1.3.6.1.2.1.1.7\nsymbol: sysServices\nmapping:\n4: routing\n72: application\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#metadata","title":"<code>metadata</code>","text":"<p>(Optional)</p> <p>This <code>metadata</code> section is used to declare where and how metadata should be collected.</p> <p>General structure:</p> <pre><code>metadata:\n&lt;RESOURCCE&gt;:  # example: device, interface\nfields:\n&lt;FIELD_NAME&gt;: # example: vendor, model, serial_number, etc\nvalue: \"dell\"\n</code></pre> <p>Supported resources and fields can be found here: payload.go</p>"},{"location":"tutorials/snmp/profile-format/#value-from-a-static-value","title":"Value from a static value","text":"<pre><code>metadata:\ndevice:\nfields:\nvendor:\nvalue: \"dell\"\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#value-from-an-oid-symbol-value","title":"Value from an OID (symbol) value","text":"<pre><code>metadata:\ndevice:\nfields:\nvendor:\nvalue: \"dell\"\nserial_number:\nsymbol:\nOID: 1.3.6.1.4.1.12124.2.51.1.3.1\nname: chassisSerialNumber\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#value-from-multiple-oids-symbols","title":"Value from multiple OIDs (symbols)","text":"<p>When the value might be from multiple symbols, we try to get the value from first symbol, if the value can't be fetched (e.g. OID not available from the device), we try to get the value from the second symbol, and so on.</p> <pre><code>metadata:\ndevice:\nfields:\nvendor:\nvalue: \"dell\"\nmodel:\nsymbols:\n- OID: 1.3.6.100.0\nname: someSymbolName\n- OID: 1.3.6.101.0\nname: someSymbolName\n</code></pre> <p>All OID values are fetched, even if they might not be used in the end. In the example above, both <code>1.3.6.100.0</code> and <code>1.3.6.101.0</code> are retrieved.</p>"},{"location":"tutorials/snmp/profile-format/#symbol-modifiers","title":"Symbol modifiers","text":""},{"location":"tutorials/snmp/profile-format/#extract_value","title":"<code>extract_value</code>","text":"<p>If the metric value to be submitted is from a OID with string value and needs to be extracted from it, you can use extract value feature.</p> <p><code>extract_value</code> is a regex pattern with one capture group like <code>(\\d+)C</code>, where the capture group is <code>(\\d+)</code>.</p> <p>Example use cases respective regex patterns:</p> <ul> <li>stripping the C unit from a temperature value: <code>(\\d+)C</code></li> <li>stripping the USD unit from a currency value: <code>USD(\\d+)</code></li> <li>stripping the F unit from a temperature value with spaces between the metric and the unit: <code>(\\d+) *F</code></li> </ul> <p>Example:</p> <p>Scalar Metric Example:</p> <pre><code>metrics:\n- MIB: MY-MIB\nsymbol:\nOID: 1.2.3.4.5.6.7\nname: temperature\nextract_value: '(\\d+)C'\n</code></pre> <p>Table Column Metric Example:</p> <pre><code>metrics:\n- MIB: MY-MIB\ntable:\nOID: 1.2.3.4.5.6\nname: myTable\nsymbols:\n- OID: 1.2.3.4.5.6.7\nname: temperature\nextract_value: '(\\d+)C'\n# ...\n</code></pre> <p>In the examples above, the OID value is a snmp OctetString value <code>22C</code> and we want <code>22</code> to be submitted as value for <code>snmp.temperature</code>.</p>"},{"location":"tutorials/snmp/profile-format/#extract_value-can-be-used-to-trim-surrounding-non-printable-characters","title":"<code>extract_value</code> can be used to trim surrounding non-printable characters","text":"<p>If the raw SNMP OctetString value contains leading or trailing non-printable characters, you can use <code>extract_value</code> regex like <code>([a-zA-Z0-9_]+)</code> to ignore them.</p> <pre><code>metrics:\n- MIB: IF-MIB\ntable:\nOID: 1.3.6.1.2.1.2.2\nname: ifTable\nsymbols:\n- OID: 1.3.6.1.2.1.2.2.1.14\nname: ifInErrors\nmetric_tags:\n- tag: interface\ncolumn:\nOID: 1.3.6.1.2.1.2.2.1.2\nname: ifDescr\nextract_value: '([a-zA-Z0-9_]+)' # will ignore surrounding non-printable characters\n</code></pre>"},{"location":"tutorials/snmp/profile-format/#match_pattern-and-match_value","title":"<code>match_pattern</code> and <code>match_value</code>","text":"<pre><code>metadata:\ndevice:\nfields:\nvendor:\nvalue: \"dell\"\nversion:\nsymbol:\nOID: 1.3.6.1.2.1.1.1.0\nname: sysDescr\nmatch_pattern: 'Isilon OneFS v(\\S+)'\nmatch_value: '$1'\n# Will match `8.2.0.0` in `device-name-3 263829375 Isilon OneFS v8.2.0.0`\n</code></pre> <p>Regex groups captured in <code>match_pattern</code> can be used in <code>match_value</code>. <code>$1</code> is the first captured group, <code>$2</code> is the second captured group, and so on.</p>"},{"location":"tutorials/snmp/profile-format/#format-mac_address","title":"<code>format: mac_address</code>","text":"<p>If you see MAC Address in tags being encoded as <code>0x000000000000</code> instead of <code>00:00:00:00:00:00</code>, then you can use <code>format: mac_address</code> to format the MAC Address to <code>00:00:00:00:00:00</code> format.</p> <p>Example:</p> <pre><code>metrics:\n- MIB: MERAKI-CLOUD-CONTROLLER-MIB\ntable:\nOID: 1.3.6.1.4.1.29671.1.1.4\nname: devTable\nsymbols:\n- OID: 1.3.6.1.4.1.29671.1.1.4.1.5\nname: devClientCount\nmetric_tags:\n- column:\nOID: 1.3.6.1.4.1.29671.1.1.4.1.1\nname: devMac\nformat: mac_address\ntag: mac_address\n</code></pre> <p>In this case, the metrics will be tagged with <code>mac_address:00:00:00:00:00:00</code>.</p>"},{"location":"tutorials/snmp/profile-format/#format-ip_address","title":"<code>format: ip_address</code>","text":"<p>If you see IP Address in tags being encoded as <code>0x0a430007</code> instead of <code>10.67.0.7</code>, then you can use <code>format: ip_address</code> to format the IP Address to <code>10.67.0.7</code> format.</p> <p>Example:</p> <pre><code>metrics:\n- MIB: MY-MIB\nsymbols:\n- OID: 1.2.3.4.6.7.1.2\nname: myOidSymbol\nmetric_tags:\n- column:\nOID: 1.2.3.4.6.7.1.3\nname: oidValueWithIpAsBytes\nformat: ip_address\ntag: connected_device\n</code></pre> <p>In this case, the metrics <code>snmp.myOidSymbol</code> will be tagged like this: <code>connected_device:10.67.0.7</code>.</p> <p>This <code>format: ip_address</code> formatter also works for IPv6 when the input bytes represent IPv6. </p>"},{"location":"tutorials/snmp/profile-format/#scale_factor","title":"<code>scale_factor</code>","text":"<p>In a value is in kilobytes and you would like to convert it to bytes, <code>scale_factor</code> can be used for that.  </p> <p>Example:</p> <pre><code>metrics:\n- MIB: AIRESPACE-SWITCHING-MIB\nsymbol:\nOID: 1.3.6.1.4.1.14179.1.1.5.3 # agentFreeMemory (in Kb)\nscale_factor: 1000 # convert to bytes\nname: memory.free\n</code></pre> <p>To scale down by 1000x: <code>scale_factor: 0.001</code>.</p>"},{"location":"tutorials/snmp/profiles/","title":"Build an SNMP Profile","text":"<p>SNMP profiles are our way of providing out-of-the-box monitoring for certain makes and models of network devices.</p> <p>This tutorial will walk you through the steps of building a basic SNMP profile that collects OID metrics from HP iLO4 devices.</p> <p>Feel free to read the Introduction to SNMP if you need a refresher on SNMP concepts such as OIDs and MIBs.</p> <p>Ready? Let's get started!</p>"},{"location":"tutorials/snmp/profiles/#research","title":"Research","text":"<p>The first step to building an SNMP profile is doing some basic research about the device, and which metrics we want to collect.</p>"},{"location":"tutorials/snmp/profiles/#general-device-information","title":"General device information","text":"<p>Generally, you'll want to search the web and find out about the following:</p> <ul> <li> <p>Device name, manufacturer, and device <code>sysobjectid</code>.</p> </li> <li> <p>Understand what the device does, and what it is used for. (Which metrics are relevant varies between routers, switches, bridges, etc. See Networking hardware.)</p> <p>E.g. from the HP iLO Wikipedia page, we can see that iLO4 devices are used by system administrators for remote management of embedded servers.</p> </li> <li> <p>Available versions of the device, and which ones we target.</p> <p>E.g. HP iLO devices exist in multiple versions (version 3, version 4...). Here, we are specifically targeting HP iLO4.</p> </li> <li> <p>Supported MIBs and OIDs (often available in official documentation), and associated MIB files.</p> <p>E.g. we can see that HP provides a MIB package for iLO devices here.</p> </li> </ul>"},{"location":"tutorials/snmp/profiles/#metrics-selection","title":"Metrics selection","text":"<p>Now that we have gathered some basic information about the device and its SNMP interfaces, we should decide which metrics we want to collect. (Devices often expose thousands of metrics through SNMP. We certainly don't want to collect them all.)</p> <p>Devices typically expose thousands of OIDs that can span dozens of MIB, so this can feel daunting at first. Remember, never give up!</p> <p>Some guidelines to help you in this process:</p> <ul> <li>10-40 metrics is a good amount already.</li> <li>Explore base profiles to see which ones could be applicable to the device.</li> <li>Explore manufacturer-specific MIB files looking for metrics such as:<ul> <li>General health: status gauges...</li> <li>Network traffic: bytes in/out, errors in/out, ...</li> <li>CPU and memory usage.</li> <li>Temperature: temperature sensors, thermal condition, ...</li> <li>Power supply.</li> <li>Storage.</li> <li>Field-replaceable units (FRU).</li> <li>...</li> </ul> </li> </ul>"},{"location":"tutorials/snmp/profiles/#implementation","title":"Implementation","text":"<p>It might be tempting to gather as many metrics as possible, and only then start building the profile and writing tests.</p> <p>But we recommend you start small. This will allow you to quickly gain confidence on the various components of the SNMP development workflow:</p> <ul> <li>Editing profile files.</li> <li>Writing tests.</li> <li>Building and using simulation data.</li> </ul>"},{"location":"tutorials/snmp/profiles/#add-a-profile-file","title":"Add a profile file","text":"<p>Add a <code>.yaml</code> file for the profile with the <code>sysobjectid</code> and a metric (you'll be able to add more later).</p> <p>For example:</p> <pre><code>sysobjectid: 1.3.6.1.4.1.232.9.4.10\n\nmetrics:\n- MIB: CPQHLTH-MIB\nsymbol:\nOID: 1.3.6.1.4.1.232.6.2.8.1.0\nname: cpqHeSysUtilLifeTime\n</code></pre> <p>Tip</p> <p><code>sysobjectid</code> can also be a wildcard pattern to match a sub-tree of devices, eg <code>1.3.6.1.131.12.4.*</code>.</p>"},{"location":"tutorials/snmp/profiles/#generate-a-profile-file-from-a-collection-of-mibs","title":"Generate a profile file from a collection of MIBs","text":"<p>You can use <code>ddev</code> to create a profile from a list of mibs.</p> <pre><code>$  ddev meta snmp generate-profile-from-mibs --help\n</code></pre> <p>This script requires a list of ASN1 MIB files as input argument, and copies to the clipboard a list of metrics that can be used to create a profile.</p>"},{"location":"tutorials/snmp/profiles/#options","title":"Options","text":"<p><code>-f, --filters</code> is an option to provide the path to a YAML file containing a collection of MIB names and their list of node names to be included.</p> <p>For example:</p> <pre><code>RFC1213-MIB:\n- system\n- interfaces\n- ip\nCISCO-SYSLOG-MIB: []\nSNMP-FRAMEWORK-MIB:\n- snmpEngine\n</code></pre> <p>Will include <code>system</code>, <code>interfaces</code> and <code>ip</code> nodes from <code>RFC1213-MIB</code>, no node from <code>CISCO-SYSLOG-MIB</code>, and node <code>snmpEngine</code> from <code>SNMP-FRAMEWORK-MIB</code>.</p> <p>Note that each <code>MIB:node_name</code> correspond to exactly one and only one OID. However, some MIBs report legacy nodes that are overwritten.</p> <p>To resolve, edit the MIB by removing legacy values manually before loading them with this profile generator. If a MIB is fully supported, it can be omitted from the filter as MIBs not found in a filter will be fully loaded. If a MIB is not fully supported, it can be listed with an empty node list, as <code>CISCO-SYSLOG-MIB</code> in the example.</p> <p><code>-a, --aliases</code> is an option to provide the path to a YAML file containing a list of aliases to be used as metric tags for tables, in the following format:</p> <pre><code>aliases:\n- from:\nMIB: ENTITY-MIB\nname: entPhysicalIndex\nto:\nMIB: ENTITY-MIB\nname: entPhysicalName\n</code></pre> <p>MIBs tables most of the time define one or more indexes, as columns within the same table, or columns from a different table and even a different MIB. The index value can be used to tag table's metrics. This is defined in the <code>INDEX</code> field in <code>row</code> nodes.</p> <p>As an example, <code>entPhysicalContainsTable</code> in <code>ENTITY-MIB</code> is as follows:</p> <pre><code>entPhysicalContainsEntry OBJECT-TYPE\nSYNTAX      EntPhysicalContainsEntry\nMAX-ACCESS  not-accessible\nSTATUS      current\nDESCRIPTION\n        \"A single container/'containee' relationship.\"\nINDEX       { entPhysicalIndex, entPhysicalChildIndex }  &lt;== this is the index definition\n::= { entPhysicalContainsTable 1 }\n</code></pre> <p>or its JSON dump, where <code>INDEX</code> is replaced by <code>indices</code>:</p> <pre><code>\"entPhysicalContainsEntry\": {\n\"name\": \"entPhysicalContainsEntry\",\n\"oid\": \"1.3.6.1.2.1.47.1.3.3.1\",\n\"nodetype\": \"row\",\n\"class\": \"objecttype\",\n\"maxaccess\": \"not-accessible\",\n\"indices\": [\n{\n\"module\": \"ENTITY-MIB\",\n\"object\": \"entPhysicalIndex\",\n\"implied\": 0\n},\n{\n\"module\": \"ENTITY-MIB\",\n\"object\": \"entPhysicalChildIndex\",\n\"implied\": 0\n}\n],\n\"status\": \"current\",\n\"description\": \"A single container/'containee' relationship.\"\n},\n</code></pre> <p>Indexes can be replaced by another MIB symbol that is more human friendly. You might prefer to see the interface name versus its numerical table index. This can be achieved using <code>metric_tag_aliases</code>.</p>"},{"location":"tutorials/snmp/profiles/#add-unit-tests","title":"Add unit tests","text":"<p>Add a unit test in <code>test_profiles.py</code> to verify that the metric is successfully collected by the integration when the profile is enabled. (These unit tests are mostly used to prevent regressions and will help with maintenance.)</p> <p>For example:</p> <pre><code>def test_hp_ilo4(aggregator):\n    run_profile_check('hp_ilo4')\n\n    common_tags = common.CHECK_TAGS + ['snmp_profile:hp-ilo4']\n\n    aggregator.assert_metric('snmp.cpqHeSysUtilLifeTime', metric_type=aggregator.MONOTONIC_COUNT, tags=common_tags, count=1)\n    aggregator.assert_all_metrics_covered()\n</code></pre> <p>We don't have simulation data yet, so the test should fail. Let's make sure it does:</p> <pre><code>$ ddev test -k test_hp_ilo4 snmp:py38\n[...]\n======================================= FAILURES ========================================\n_____________________________________ test_hp_ilo4 ______________________________________\ntests/test_profiles.py:1464: in test_hp_ilo4\n    aggregator.assert_metric('snmp.cpqHeSysUtilLifeTime', metric_type=aggregator.GAUGE, tags=common.CHECK_TAGS, count=1)\n../datadog_checks_base/datadog_checks/base/stubs/aggregator.py:253: in assert_metric\n    self._assert(condition, msg=msg, expected_stub=expected_metric, submitted_elements=self._metrics)\n../datadog_checks_base/datadog_checks/base/stubs/aggregator.py:295: in _assert\n    assert condition, new_msg\nE   AssertionError: Needed exactly 1 candidates for 'snmp.cpqHeSysUtilLifeTime', got 0\n[...]\n</code></pre> <p>Good. Now, onto adding simulation data.</p>"},{"location":"tutorials/snmp/profiles/#add-simulation-data","title":"Add simulation data","text":"<p>Add a <code>.snmprec</code> file named after the <code>community_string</code>, which is the value we gave to <code>run_profile_check()</code>:</p> <pre><code>$ touch snmp/tests/compose/data/hp_ilo4.snmprec\n</code></pre> <p>Add lines to the <code>.snmprec</code> file to specify the <code>sysobjectid</code> and the OID listed in the profile:</p> <pre><code>1.3.6.1.2.1.1.2.0|6|1.3.6.1.4.1.232.9.4.10\n1.3.6.1.4.1.232.6.2.8.1.0|2|1051200\n</code></pre> <p>Run the test again, and make sure it passes this time:</p> <pre><code>$ ddev test -k test_hp_ilo4 snmp:py38\n[...]\n\ntests/test_profiles.py::test_hp_ilo4 PASSED                                                                                        [100%]\n\n=================================================== 1 passed, 107 deselected in 9.87s ====================================================\n________________________________________________________________ summary _________________________________________________________________\n  py38: commands succeeded\n  congratulations :)\n</code></pre>"},{"location":"tutorials/snmp/profiles/#rinse-and-repeat","title":"Rinse and repeat","text":"<p>We have now covered the basic workflow \u2014 add metrics, expand tests, add simulation data. You can now go ahead and add more metrics to the profile!</p>"},{"location":"tutorials/snmp/profiles/#next-steps","title":"Next steps","text":"<p>Congratulations! You should now be able to write a basic SNMP profile.</p> <p>We kept this tutorial as simple as possible, but profiles offer many more options to collect metrics from SNMP devices.</p> <ul> <li>To learn more about what can be done in profiles, read the Profile format reference.</li> <li>To learn more about <code>.snmprec</code> files, see the Simulation data format reference.</li> </ul>"},{"location":"tutorials/snmp/sim-format/","title":"Simulation Data Format Reference","text":""},{"location":"tutorials/snmp/sim-format/#conventions","title":"Conventions","text":"<ul> <li>Simulation data for profiles is contained in <code>.snmprec</code> files located in the tests directory.</li> <li>Simulation files must be named after the SNMP community string used in the profile unit tests. For example: <code>cisco-nexus.snmprec</code>.</li> </ul>"},{"location":"tutorials/snmp/sim-format/#file-contents","title":"File contents","text":"<p>Each line in a <code>.snmprec</code> file corresponds to a value for an OID.</p> <p>Lines must be formatted as follows:</p> <pre><code>&lt;OID&gt;|&lt;type&gt;|&lt;value&gt;\n</code></pre> <p>For the list of supported types, see the <code>snmpsim</code> simulation data file format documentation.</p> <p>Warning</p> <p>Due to a limitation of <code>snmpsim</code>, contents of <code>.snmprec</code> files must be sorted in lexicographic order.</p> <p>Use <code>$ sort -V /path/to/profile.snmprec</code> to sort lines from the terminal.</p>"},{"location":"tutorials/snmp/sim-format/#symbols","title":"Symbols","text":"<p>For symbol metrics, add a single line corresponding to the symbol OID. For example:</p> <pre><code>1.3.6.1.4.1.232.6.2.8.1.0|2|1051200\n</code></pre>"},{"location":"tutorials/snmp/sim-format/#tables","title":"Tables","text":"<p>Tip</p> <p>Adding simulation data for tables can be particularly tedious. This section documents the manual process, but automatic generation is possible \u2014 see How to generate table simulation data.</p> <p>For table metrics, add one copy of the metric per row, appending the index to the OID.</p> <p>For example, to simulate 3 rows in the table <code>1.3.6.1.4.1.6.13</code> that has OIDs <code>1.3.6.1.4.1.6.13.1.6</code> and <code>1.3.6.1.4.1.6.13.1.8</code>, you could write:</p> <pre><code>1.3.6.1.4.1.6.13.1.6.0|2|1051200\n1.3.6.1.4.1.6.13.1.6.1|2|1446\n1.3.6.1.4.1.6.13.1.6.2|2|23\n1.3.6.1.4.1.6.13.1.8.0|2|165\n1.3.6.1.4.1.6.13.1.8.1|2|976\n1.3.6.1.4.1.6.13.1.8.2|2|0\n</code></pre> <p>Note</p> <p>If the table uses table metric tags, you may need to add additional OID simulation data for those tags.</p>"},{"location":"tutorials/snmp/tools/","title":"Tools","text":""},{"location":"tutorials/snmp/tools/#using-tcpdump-with-snmp","title":"Using <code>tcpdump</code> with SNMP","text":"<p>The <code>tcpdump</code> command shows the exact request and response content of SNMP <code>GET</code>, <code>GETNEXT</code> and other SNMP calls.</p> <p>In a shell run <code>tcpdump</code>:</p> <pre><code>tcpdump -vv -nni lo0 -T snmp host localhost and port 161\n</code></pre> <ul> <li><code>-nn</code>:  turn off host and protocol name resolution (to avoid generating DNS packets)</li> <li><code>-i INTERFACE</code>: listen on INTERFACE (default: lowest numbered interface)</li> <li><code>-T snmp</code>: type/protocol, snmp in our case</li> </ul> <p>In another separate shell run <code>snmpwalk</code> or <code>snmpget</code>:</p> <pre><code>snmpwalk -O n -v2c -c &lt;COMMUNITY_STRING&gt; localhost:1161 1.3.6\n</code></pre> <p>After you've run <code>snmpwalk</code>, you'll see results like this from <code>tcpdump</code>:</p> <pre><code>tcpdump -vv -nni lo0 -T snmp host localhost and port 161\ntcpdump: listening on lo0, link-type NULL (BSD loopback), capture size 262144 bytes\n17:25:43.639639 IP (tos 0x0, ttl 64, id 29570, offset 0, flags [none], proto UDP (17), length 76, bad cksum 0 (-&gt;91d)!)\n    127.0.0.1.59540 &gt; 127.0.0.1.1161:  { SNMPv2c C=\"cisco-nexus\" { GetRequest(28) R=1921760388  .1.3.6.1.2.1.1.2.0 } }\n17:25:43.645088 IP (tos 0x0, ttl 64, id 26543, offset 0, flags [none], proto UDP (17), length 88, bad cksum 0 (-&gt;14e4)!)\n    127.0.0.1.1161 &gt; 127.0.0.1.59540:  { SNMPv2c C=\"cisco-nexus\" { GetResponse(40) R=1921760388  .1.3.6.1.2.1.1.2.0=.1.3.6.1.4.1.9.12.3.1.3.1.2 } }\n</code></pre>"},{"location":"tutorials/snmp/tools/#from-the-docker-agent-container","title":"From the Docker Agent container","text":"<p>If you want to run <code>snmpget</code>, <code>snmpwalk</code>, and <code>tcpdump</code> from the Docker Agent container you can install them by running the following commands (in the container):</p> <pre><code>apt update\napt install -y snmp tcpdump\n</code></pre>"}]}